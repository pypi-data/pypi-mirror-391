#!/usr/bin/env python3
"""Stateless/Serverless Mode for Session Management MCP Server.

Enables request-scoped sessions with external storage backends (Redis, S3, DynamoDB).
Allows the session management server to operate in cloud/serverless environments.
"""

import asyncio
import gzip
import hashlib
import json
import logging
from abc import ABC, abstractmethod
from datetime import UTC, datetime, timedelta
from pathlib import Path
from typing import Any

from pydantic import BaseModel, Field, field_validator

CONFIG_LOGGER = logging.getLogger("serverless.config")


class SessionState(BaseModel):
    """Represents complete session state for serialization."""

    session_id: str = Field(
        min_length=1, description="Unique identifier for the session"
    )
    user_id: str = Field(min_length=1, description="Identifier for the user")
    project_id: str = Field(min_length=1, description="Identifier for the project")
    created_at: str = Field(description="ISO timestamp when session was created")
    last_activity: str = Field(description="ISO timestamp of last activity")
    permissions: list[str] = Field(
        default_factory=list, description="List of permissions granted to the session"
    )
    conversation_history: list[dict[str, Any]] = Field(
        default_factory=list, description="History of conversation entries"
    )
    reflection_data: dict[str, Any] = Field(
        default_factory=dict, description="Stored reflection and memory data"
    )
    app_monitoring_state: dict[str, Any] = Field(
        default_factory=dict, description="Application monitoring state"
    )
    llm_provider_configs: dict[str, Any] = Field(
        default_factory=dict, description="LLM provider configurations"
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional session metadata"
    )

    @field_validator("created_at", "last_activity")
    @classmethod
    def validate_iso_timestamp(cls, v: str) -> str:
        """Validate that timestamps are in ISO format."""
        try:
            datetime.fromisoformat(v)
            return v
        except ValueError as e:
            msg = f"Invalid ISO timestamp format: {v}"
            raise ValueError(msg) from e

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return self.model_dump()

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "SessionState":
        """Create from dictionary."""
        return cls.model_validate(data)

    def get_compressed_size(self) -> int:
        """Get compressed size of session state."""
        serialized = self.model_dump_json()
        compressed = gzip.compress(serialized.encode("utf-8"))
        return len(compressed)


class SessionStorage(ABC):
    """Abstract base class for session storage backends."""

    def __init__(self, config: dict[str, Any]) -> None:
        self.config = config
        self.logger = logging.getLogger(f"serverless.{self.__class__.__name__.lower()}")

    @abstractmethod
    async def store_session(
        self,
        session_state: SessionState,
        ttl_seconds: int | None = None,
    ) -> bool:
        """Store session state with optional TTL."""

    @abstractmethod
    async def retrieve_session(self, session_id: str) -> SessionState | None:
        """Retrieve session state by ID."""

    @abstractmethod
    async def delete_session(self, session_id: str) -> bool:
        """Delete session state."""

    @abstractmethod
    async def list_sessions(
        self,
        user_id: str | None = None,
        project_id: str | None = None,
    ) -> list[str]:
        """List session IDs matching criteria."""

    @abstractmethod
    async def cleanup_expired_sessions(self) -> int:
        """Clean up expired sessions, return count removed."""

    @abstractmethod
    async def is_available(self) -> bool:
        """Check if storage backend is available."""


class RedisStorage(SessionStorage):
    """Redis-based session storage."""

    def __init__(self, config: dict[str, Any]) -> None:
        super().__init__(config)
        self.host = config.get("host", "localhost")
        self.port = config.get("port", 6379)
        self.db = config.get("db", 0)
        self.password = config.get("password")
        self.key_prefix = config.get("key_prefix", "session_mgmt:")
        self._redis = None

    async def _get_redis(self) -> Any:
        """Get or create Redis connection."""
        if self._redis is None:
            try:
                import redis.asyncio as redis

                self._redis = redis.Redis(  # type: ignore[assignment]
                    host=self.host,
                    port=self.port,
                    db=self.db,
                    password=self.password,
                    decode_responses=False,  # We handle encoding ourselves
                )
            except ImportError:
                msg = "Redis package not installed. Install with: pip install redis"
                raise ImportError(
                    msg,
                )
        return self._redis

    def _get_key(self, session_id: str) -> str:
        """Get Redis key for session."""
        return f"{self.key_prefix}session:{session_id}"

    def _get_index_key(self, index_type: str) -> str:
        """Get Redis key for index."""
        return f"{self.key_prefix}index:{index_type}"

    async def store_session(
        self,
        session_state: SessionState,
        ttl_seconds: int | None = None,
    ) -> bool:
        """Store session in Redis with optional TTL."""
        try:
            redis_client = await self._get_redis()

            # Serialize and compress session state
            serialized = json.dumps(session_state.to_dict())
            compressed = gzip.compress(serialized.encode("utf-8"))

            # Store session data
            key = self._get_key(session_state.session_id)
            await redis_client.set(key, compressed, ex=ttl_seconds)

            # Update indexes
            user_index_key = self._get_index_key(f"user:{session_state.user_id}")
            project_index_key = self._get_index_key(
                f"project:{session_state.project_id}",
            )

            await redis_client.sadd(user_index_key, session_state.session_id)
            await redis_client.sadd(project_index_key, session_state.session_id)

            # Set TTL on indexes if specified
            if ttl_seconds:
                await redis_client.expire(user_index_key, ttl_seconds)
                await redis_client.expire(project_index_key, ttl_seconds)

            return True

        except Exception as e:
            self.logger.exception(
                f"Failed to store session {session_state.session_id}: {e}",
            )
            return False

    async def retrieve_session(self, session_id: str) -> SessionState | None:
        """Retrieve session from Redis."""
        try:
            redis_client = await self._get_redis()
            key = self._get_key(session_id)

            compressed_data = await redis_client.get(key)
            if not compressed_data:
                return None

            # Decompress and deserialize
            serialized = gzip.decompress(compressed_data).decode("utf-8")
            session_data = json.loads(serialized)

            return SessionState.from_dict(session_data)

        except Exception as e:
            self.logger.exception(f"Failed to retrieve session {session_id}: {e}")
            return None

    async def delete_session(self, session_id: str) -> bool:
        """Delete session from Redis."""
        try:
            redis_client = await self._get_redis()

            # Get session to find user/project for index cleanup
            session_state = await self.retrieve_session(session_id)

            # Delete session data
            key = self._get_key(session_id)
            deleted_result = await redis_client.delete(key)
            deleted = int(deleted_result) if deleted_result is not None else 0

            # Clean up indexes
            if session_state:
                user_index_key = self._get_index_key(f"user:{session_state.user_id}")
                project_index_key = self._get_index_key(
                    f"project:{session_state.project_id}",
                )

                await redis_client.srem(user_index_key, session_id)
                await redis_client.srem(project_index_key, session_id)

            return deleted > 0

        except Exception as e:
            self.logger.exception(f"Failed to delete session {session_id}: {e}")
            return False

    async def list_sessions(
        self,
        user_id: str | None = None,
        project_id: str | None = None,
    ) -> list[str]:
        """List sessions by user or project."""
        try:
            redis_client = await self._get_redis()

            if user_id:
                index_key = self._get_index_key(f"user:{user_id}")
                session_ids = await redis_client.smembers(index_key)
                return [
                    sid.decode("utf-8") if isinstance(sid, bytes) else sid
                    for sid in session_ids
                ]

            if project_id:
                index_key = self._get_index_key(f"project:{project_id}")
                session_ids = await redis_client.smembers(index_key)
                return [
                    sid.decode("utf-8") if isinstance(sid, bytes) else sid
                    for sid in session_ids
                ]

            # List all sessions (expensive operation)
            pattern = self._get_key("*")
            keys = await redis_client.keys(pattern)
            return [
                key.decode("utf-8").split(":")[-1]
                if isinstance(key, bytes)
                else key.split(":")[-1]
                for key in keys
            ]

        except Exception as e:
            self.logger.exception(f"Failed to list sessions: {e}")
            return []

    async def cleanup_expired_sessions(self) -> int:
        """Clean up expired sessions."""
        # Redis automatically handles TTL expiration
        # This method could scan for orphaned index entries
        try:
            redis_client = await self._get_redis()
            index_keys = await self._get_index_keys(redis_client)

            cleaned = 0
            for index_key in index_keys:
                cleaned += await self._cleanup_index_key(redis_client, index_key)

            return cleaned

        except Exception as e:
            self.logger.exception(f"Failed to cleanup expired sessions: {e}")
            return 0

    async def _get_index_keys(self, redis_client: Any) -> list[str]:
        """Get all index keys for cleanup."""
        index_pattern = self._get_index_key("*")
        raw_keys = await redis_client.keys(index_pattern)

        return [
            key.decode("utf-8") if isinstance(key, bytes) else key for key in raw_keys
        ]

    async def _cleanup_index_key(self, redis_client: Any, index_key: str) -> int:
        """Clean up orphaned sessions from a single index key."""
        session_ids = await redis_client.smembers(index_key)
        cleaned = 0

        for session_id in session_ids:
            if await self._is_orphaned_session(redis_client, session_id):
                await redis_client.srem(index_key, session_id)
                cleaned += 1

        return cleaned

    async def _is_orphaned_session(self, redis_client: Any, session_id: Any) -> bool:
        """Check if a session ID refers to an orphaned session."""
        if isinstance(session_id, bytes):
            session_id = session_id.decode("utf-8")

        session_key = self._get_key(session_id)
        return not await redis_client.exists(session_key)

    async def is_available(self) -> bool:
        """Check if Redis is available."""
        try:
            redis_client = await self._get_redis()
            await redis_client.ping()
            return True
        except Exception:
            return False


class S3Storage(SessionStorage):
    """S3-based session storage."""

    def __init__(self, config: dict[str, Any]) -> None:
        super().__init__(config)
        self.bucket_name = config.get("bucket_name", "session-mgmt-mcp")
        self.region = config.get("region", "us-east-1")
        self.key_prefix = config.get("key_prefix", "sessions/")
        self.access_key_id = config.get("access_key_id")
        self.secret_access_key = config.get("secret_access_key")
        self._s3_client = None

    async def _get_s3_client(self) -> Any:
        """Get or create S3 client."""
        if self._s3_client is None:
            try:
                import boto3
                from botocore.client import Config

                session = boto3.Session(
                    aws_access_key_id=self.access_key_id,
                    aws_secret_access_key=self.secret_access_key,
                    region_name=self.region,
                )

                self._s3_client = session.client(
                    "s3",
                    config=Config(retries={"max_attempts": 3}, max_pool_connections=50),
                )
            except ImportError:
                msg = "Boto3 package not installed. Install with: pip install boto3"
                raise ImportError(
                    msg,
                )

        return self._s3_client

    def _get_key(self, session_id: str) -> str:
        """Get S3 key for session."""
        return f"{self.key_prefix}{session_id}.json.gz"

    async def store_session(
        self,
        session_state: SessionState,
        ttl_seconds: int | None = None,
    ) -> bool:
        """Store session in S3."""
        try:
            s3_client = await self._get_s3_client()

            # Serialize and compress session state
            serialized = json.dumps(session_state.to_dict())
            compressed = gzip.compress(serialized.encode("utf-8"))

            # Prepare S3 object metadata
            metadata = {
                "user_id": session_state.user_id,
                "project_id": session_state.project_id,
                "created_at": session_state.created_at,
                "last_activity": session_state.last_activity,
            }

            # Set expiration if TTL specified
            expires = None
            if ttl_seconds:
                expires = datetime.now(UTC) + timedelta(seconds=ttl_seconds)

            # Upload to S3
            key = self._get_key(session_state.session_id)

            put_args = {
                "Bucket": self.bucket_name,
                "Key": key,
                "Body": compressed,
                "ContentType": "application/json",
                "ContentEncoding": "gzip",
                "Metadata": metadata,
            }

            if expires:
                put_args["Expires"] = expires

            # Execute in thread pool since boto3 is synchronous
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, lambda: s3_client.put_object(**put_args))

            return True

        except Exception as e:
            self.logger.exception(
                f"Failed to store session {session_state.session_id}: {e}",
            )
            return False

    async def retrieve_session(self, session_id: str) -> SessionState | None:
        """Retrieve session from S3."""
        try:
            s3_client = await self._get_s3_client()
            key = self._get_key(session_id)

            # Download from S3
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: s3_client.get_object(Bucket=self.bucket_name, Key=key),
            )

            # Decompress and deserialize
            compressed_data = response["Body"].read()
            serialized = gzip.decompress(compressed_data).decode("utf-8")
            session_data = json.loads(serialized)

            return SessionState.from_dict(session_data)

        except Exception as e:
            self.logger.exception(f"Failed to retrieve session {session_id}: {e}")
            return None

    async def delete_session(self, session_id: str) -> bool:
        """Delete session from S3."""
        try:
            s3_client = await self._get_s3_client()
            key = self._get_key(session_id)

            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: s3_client.delete_object(Bucket=self.bucket_name, Key=key),
            )

            return True

        except Exception as e:
            self.logger.exception(f"Failed to delete session {session_id}: {e}")
            return False

    async def list_sessions(
        self,
        user_id: str | None = None,
        project_id: str | None = None,
    ) -> list[str]:
        """List sessions in S3."""
        try:
            s3_client = await self._get_s3_client()
            s3_objects = await self._get_s3_objects(s3_client)

            session_ids = []
            for obj in s3_objects:
                key = obj["Key"]
                session_id = self._extract_session_id_from_key(key)

                if await self._should_include_s3_session(
                    s3_client, key, user_id, project_id
                ):
                    session_ids.append(session_id)

            return session_ids

        except Exception as e:
            self.logger.exception(f"Failed to list sessions: {e}")
            return []

    async def _get_s3_objects(self, s3_client: Any) -> list[dict[str, Any]]:
        """Get S3 objects with the configured prefix."""
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=self.key_prefix,
            ),
        )
        contents = response.get("Contents", [])
        return list(contents) if contents else []

    def _extract_session_id_from_key(self, key: str) -> str:
        """Extract session ID from S3 object key."""
        return key.replace(self.key_prefix, "").replace(".json.gz", "")

    async def _should_include_s3_session(
        self,
        s3_client: Any,
        key: str,
        user_id: str | None,
        project_id: str | None,
    ) -> bool:
        """Check if S3 session should be included based on filters."""
        if not user_id and not project_id:
            return True

        metadata = await self._get_s3_object_metadata(s3_client, key)

        if user_id and metadata.get("user_id") != user_id:
            return False
        return not (project_id and metadata.get("project_id") != project_id)

    async def _get_s3_object_metadata(self, s3_client: Any, key: str) -> dict[str, Any]:
        """Get metadata for an S3 object."""
        loop = asyncio.get_event_loop()
        head_response = await loop.run_in_executor(
            None,
            lambda: s3_client.head_object(Bucket=self.bucket_name, Key=key),
        )
        metadata = head_response.get("Metadata", {})
        return dict(metadata) if metadata else {}

    async def cleanup_expired_sessions(self) -> int:
        """Clean up expired sessions from S3."""
        try:
            s3_client = await self._get_s3_client()

            # S3 lifecycle policies handle expiration automatically
            # This could implement custom logic for old sessions

            now = datetime.now(UTC)
            cleaned = 0

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: s3_client.list_objects_v2(
                    Bucket=self.bucket_name,
                    Prefix=self.key_prefix,
                ),
            )

            for obj in response.get("Contents", []):
                # Check if object is expired (custom logic)
                last_modified = obj["LastModified"].replace(tzinfo=None)
                age_days = (now - last_modified).days

                if age_days > 30:  # Cleanup sessions older than 30 days
                    await loop.run_in_executor(
                        None,
                        lambda: s3_client.delete_object(
                            Bucket=self.bucket_name,
                            Key=obj["Key"],
                        ),
                    )
                    cleaned += 1

            return cleaned

        except Exception as e:
            self.logger.exception(f"Failed to cleanup expired sessions: {e}")
            return 0

    async def is_available(self) -> bool:
        """Check if S3 is available."""
        try:
            s3_client = await self._get_s3_client()

            # Test bucket access
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: s3_client.head_bucket(Bucket=self.bucket_name),
            )

            return True
        except Exception:
            return False


class LocalFileStorage(SessionStorage):
    """Local file-based session storage (for development/testing)."""

    def __init__(self, config: dict[str, Any]) -> None:
        super().__init__(config)
        self.storage_dir = Path(
            config.get("storage_dir", Path.home() / ".claude" / "data" / "sessions"),
        )
        self.storage_dir.mkdir(parents=True, exist_ok=True)

    def _get_session_file(self, session_id: str) -> Path:
        """Get file path for session."""
        return self.storage_dir / f"{session_id}.json.gz"

    async def store_session(
        self,
        session_state: SessionState,
        ttl_seconds: int | None = None,
    ) -> bool:
        """Store session in local file."""
        try:
            # Serialize and compress session state
            serialized = json.dumps(session_state.to_dict())
            compressed = gzip.compress(serialized.encode("utf-8"))

            # Write to file
            session_file = self._get_session_file(session_state.session_id)
            with session_file.open("wb") as f:
                f.write(compressed)

            return True

        except Exception as e:
            self.logger.exception(
                f"Failed to store session {session_state.session_id}: {e}",
            )
            return False

    async def retrieve_session(self, session_id: str) -> SessionState | None:
        """Retrieve session from local file."""
        try:
            session_file = self._get_session_file(session_id)

            if not session_file.exists():
                return None

            # Read and decompress
            with session_file.open("rb") as f:
                compressed_data = f.read()

            serialized = gzip.decompress(compressed_data).decode("utf-8")
            session_data = json.loads(serialized)

            return SessionState.from_dict(session_data)

        except Exception as e:
            self.logger.exception(f"Failed to retrieve session {session_id}: {e}")
            return None

    async def delete_session(self, session_id: str) -> bool:
        """Delete session file."""
        try:
            session_file = self._get_session_file(session_id)

            if session_file.exists():
                session_file.unlink()
                return True

            return False

        except Exception as e:
            self.logger.exception(f"Failed to delete session {session_id}: {e}")
            return False

    async def list_sessions(
        self,
        user_id: str | None = None,
        project_id: str | None = None,
    ) -> list[str]:
        """List session files."""
        try:
            session_ids = []
            for session_file in self.storage_dir.glob("*.json.gz"):
                session_id = self._extract_session_id(session_file)
                if await self._should_include_session(session_id, user_id, project_id):
                    session_ids.append(session_id)
            return session_ids
        except Exception as e:
            self.logger.exception(f"Failed to list sessions: {e}")
            return []

    def _extract_session_id(self, session_file: Path) -> str:
        """Extract session ID from file path."""
        return session_file.stem.replace(".json", "")

    async def _should_include_session(
        self, session_id: str, user_id: str | None, project_id: str | None
    ) -> bool:
        """Check if session should be included based on filters."""
        if not user_id and not project_id:
            return True

        session_state = await self.retrieve_session(session_id)
        if not session_state:
            return False

        return self._matches_filters(session_state, user_id, project_id)

    def _matches_filters(
        self, session_state: SessionState, user_id: str | None, project_id: str | None
    ) -> bool:
        """Check if session matches the given filters."""
        if user_id and session_state.user_id != user_id:
            return False
        return not (project_id and session_state.project_id != project_id)

    async def cleanup_expired_sessions(self) -> int:
        """Clean up old session files."""
        try:
            now = datetime.now()
            cleaned = 0

            for session_file in self.storage_dir.glob("*.json.gz"):
                # Check file age
                file_age = now - datetime.fromtimestamp(session_file.stat().st_mtime)

                if file_age.days > 7:  # Cleanup sessions older than 7 days
                    session_file.unlink()
                    cleaned += 1

            return cleaned

        except Exception as e:
            self.logger.exception(f"Failed to cleanup expired sessions: {e}")
            return 0

    async def is_available(self) -> bool:
        """Check if local storage is available."""
        return self.storage_dir.exists() and self.storage_dir.is_dir()


class ACBCacheStorage(SessionStorage):
    """ACB cache adapter for session storage.

    Wraps ACB cache adapters (Redis or Memory) to implement SessionStorage interface.
    Provides connection pooling, SSL/TLS support, and automatic compression via ACB.

    Benefits over custom backends:
    - Battle-tested ACB cache infrastructure
    - MsgPack + Brotli compression for efficient storage
    - Connection pooling and health checks built-in
    - SSL/TLS support via ACB configuration
    - Automatic reconnection handling
    """

    def __init__(
        self,
        cache: Any,
        namespace: str = "session",
        config: dict[str, Any] | None = None,
    ) -> None:
        """Initialize ACB cache storage.

        Args:
            cache: ACB cache adapter instance (from acb.adapters.cache)
            namespace: Namespace for multi-tenant isolation (default: "session")
            config: Optional configuration dict for compatibility

        """
        super().__init__(config or {})
        self.cache = cache
        self.namespace = namespace
        # Track session IDs for list_sessions() functionality
        self._index_key = f"{namespace}:index"

    def _get_key(self, session_id: str) -> str:
        """Get namespaced key for session."""
        return f"{self.namespace}:{session_id}"

    async def store_session(
        self,
        session_state: SessionState,
        ttl_seconds: int | None = None,
    ) -> bool:
        """Store session using ACB cache adapter.

        Args:
            session_state: Session data to store
            ttl_seconds: Time-to-live in seconds (default: 86400 = 24 hours)

        Returns:
            True if stored successfully, False otherwise

        """
        try:
            key = self._get_key(session_state.session_id)

            # Serialize SessionState using Pydantic's model_dump
            # ACB cache will handle msgpack + brotli compression automatically
            data = session_state.model_dump(mode="json")

            # Store with TTL
            ttl = ttl_seconds if ttl_seconds is not None else 86400  # 24 hours default
            await self.cache.set(key, data, ttl=ttl)

            # Add session ID to index for list_sessions()
            await self._add_to_index(
                session_state.session_id,
                session_state.user_id,
                session_state.project_id,
                ttl,
            )

            self.logger.debug(
                f"Stored session {session_state.session_id} "
                f"with TTL {ttl}s (compressed via ACB)"
            )
            return True

        except Exception as e:
            self.logger.exception(
                f"Failed to store session {session_state.session_id}: {e}"
            )
            return False

    async def retrieve_session(self, session_id: str) -> SessionState | None:
        """Retrieve session using ACB cache adapter.

        Args:
            session_id: Unique session identifier

        Returns:
            SessionState if found, None otherwise

        """
        try:
            key = self._get_key(session_id)
            data = await self.cache.get(key)

            if data is None:
                self.logger.debug(f"Session {session_id} not found")
                return None

            # Deserialize from dict to SessionState
            session_state = SessionState.model_validate(data)
            self.logger.debug(f"Retrieved session {session_id}")
            return session_state

        except Exception as e:
            self.logger.exception(f"Failed to retrieve session {session_id}: {e}")
            return None

    async def delete_session(self, session_id: str) -> bool:
        """Delete session using ACB cache adapter.

        Args:
            session_id: Unique session identifier

        Returns:
            True if deleted, False otherwise

        """
        try:
            key = self._get_key(session_id)
            result = await self.cache.delete(key)

            # Remove from index
            await self._remove_from_index(session_id)

            self.logger.debug(f"Deleted session {session_id}")
            return bool(result)

        except Exception as e:
            self.logger.exception(f"Failed to delete session {session_id}: {e}")
            return False

    async def list_sessions(
        self,
        user_id: str | None = None,
        project_id: str | None = None,
    ) -> list[str]:
        """List session IDs matching criteria.

        Note: This implementation uses a separate index key to track sessions.
        For production use with many sessions, consider using Redis SCAN or
        dedicated index storage.

        Args:
            user_id: Filter by user ID (optional)
            project_id: Filter by project ID (optional)

        Returns:
            List of session IDs matching criteria

        """
        try:
            # Get index data
            index_data = await self.cache.get(self._index_key)
            if index_data is None:
                return []

            # Filter by criteria
            session_ids = []
            for session_id, metadata in index_data.items():
                if user_id and metadata.get("user_id") != user_id:
                    continue
                if project_id and metadata.get("project_id") != project_id:
                    continue
                session_ids.append(session_id)

            return session_ids

        except Exception as e:
            self.logger.exception(f"Failed to list sessions: {e}")
            return []

    async def cleanup_expired_sessions(self) -> int:
        """Clean up expired sessions.

        Note: With ACB cache + TTL, sessions expire automatically.
        This method cleans up stale index entries.

        Returns:
            Count of sessions removed from index

        """
        try:
            index_data = await self.cache.get(self._index_key)
            if index_data is None:
                return 0

            # Check which sessions still exist
            cleaned = 0
            for session_id in list(index_data.keys()):
                key = self._get_key(session_id)
                exists = await self.cache.exists(key)
                if not exists:
                    # Session expired, remove from index
                    del index_data[session_id]
                    cleaned += 1

            # Update index if we cleaned anything
            if cleaned > 0:
                await self.cache.set(self._index_key, index_data, ttl=None)
                self.logger.info(f"Cleaned up {cleaned} expired session(s) from index")

            return cleaned

        except Exception as e:
            self.logger.exception(f"Failed to cleanup expired sessions: {e}")
            return 0

    async def is_available(self) -> bool:
        """Check if ACB cache backend is available.

        Returns:
            True if cache is reachable, False otherwise

        """
        try:
            # Try a simple operation to test connectivity
            test_key = f"{self.namespace}:health_check"
            await self.cache.set(test_key, {"status": "ok"}, ttl=10)
            result = await self.cache.get(test_key)
            if result is not None:
                await self.cache.delete(test_key)
                return True
            return False

        except Exception as e:
            self.logger.debug(f"ACB cache health check failed: {e}")
            # For memory cache without proper config, assume available
            return True

    # Helper methods for index management

    async def _add_to_index(
        self,
        session_id: str,
        user_id: str,
        project_id: str,
        ttl: int,
    ) -> None:
        """Add session to index for list_sessions()."""
        try:
            result = await self.cache.get(self._index_key)
            index_data: dict[str, Any] = result if isinstance(result, dict) else {}
            index_data[session_id] = {
                "user_id": user_id,
                "project_id": project_id,
                "expires_at": (datetime.now(UTC) + timedelta(seconds=ttl)).isoformat(),
            }
            # Index doesn't expire (we clean it up manually)
            await self.cache.set(self._index_key, index_data, ttl=None)
        except Exception as e:
            self.logger.warning(f"Failed to update index: {e}")

    async def _remove_from_index(self, session_id: str) -> None:
        """Remove session from index."""
        try:
            index_data = await self.cache.get(self._index_key)
            if index_data and session_id in index_data:
                del index_data[session_id]
                await self.cache.set(self._index_key, index_data, ttl=None)
        except Exception as e:
            self.logger.warning(f"Failed to update index: {e}")


class ServerlessSessionManager:
    """Main session manager for serverless/stateless operation."""

    def __init__(self, storage_backend: SessionStorage) -> None:
        self.storage = storage_backend
        self.logger = logging.getLogger("serverless.session_manager")
        self.session_cache: dict[
            str, SessionState
        ] = {}  # In-memory cache for current request

    async def create_session(
        self,
        user_id: str,
        project_id: str,
        session_data: dict[str, Any] | None = None,
        ttl_hours: int = 24,
    ) -> str:
        """Create new session."""
        session_id = self._generate_session_id(user_id, project_id)

        session_state = SessionState(
            session_id=session_id,
            user_id=user_id,
            project_id=project_id,
            created_at=datetime.now().isoformat(),
            last_activity=datetime.now().isoformat(),
            permissions=[],
            conversation_history=[],
            reflection_data={},
            app_monitoring_state={},
            llm_provider_configs={},
            metadata=session_data or {},
        )

        # Store with TTL
        ttl_seconds = ttl_hours * 3600
        success = await self.storage.store_session(session_state, ttl_seconds)

        if success:
            self.session_cache[session_id] = session_state
            return session_id
        msg = "Failed to create session"
        raise RuntimeError(msg)

    async def get_session(self, session_id: str) -> SessionState | None:
        """Get session state."""
        # Check cache first
        if session_id in self.session_cache:
            return self.session_cache[session_id]

        # Load from storage
        session_state = await self.storage.retrieve_session(session_id)
        if session_state:
            self.session_cache[session_id] = session_state

        return session_state

    async def update_session(
        self,
        session_id: str,
        updates: dict[str, Any],
        ttl_hours: int | None = None,
    ) -> bool:
        """Update session state."""
        session_state = await self.get_session(session_id)
        if not session_state:
            return False

        # Apply updates
        for key, value in updates.items():
            if hasattr(session_state, key):
                setattr(session_state, key, value)

        # Update last activity
        session_state.last_activity = datetime.now().isoformat()

        # Store updated state
        ttl_seconds = ttl_hours * 3600 if ttl_hours else None
        success = await self.storage.store_session(session_state, ttl_seconds)

        if success:
            self.session_cache[session_id] = session_state

        return success

    async def delete_session(self, session_id: str) -> bool:
        """Delete session."""
        # Remove from cache
        self.session_cache.pop(session_id, None)

        # Delete from storage
        return await self.storage.delete_session(session_id)

    async def list_user_sessions(self, user_id: str) -> list[str]:
        """List sessions for user."""
        return await self.storage.list_sessions(user_id=user_id)

    async def list_project_sessions(self, project_id: str) -> list[str]:
        """List sessions for project."""
        return await self.storage.list_sessions(project_id=project_id)

    async def cleanup_sessions(self) -> int:
        """Clean up expired sessions."""
        return await self.storage.cleanup_expired_sessions()

    def _generate_session_id(self, user_id: str, project_id: str) -> str:
        """Generate unique session ID."""
        timestamp = datetime.now().isoformat()
        data = f"{user_id}:{project_id}:{timestamp}"
        return hashlib.sha256(data.encode()).hexdigest()[:16]

    def get_session_stats(self) -> dict[str, Any]:
        """Get session statistics."""
        return {
            "cached_sessions": len(self.session_cache),
            "storage_backend": self.storage.__class__.__name__,
            "storage_config": {
                k: v for k, v in self.storage.config.items() if "key" not in k.lower()
            },
        }


class ServerlessConfigManager:
    """Manages configuration for serverless mode."""

    @staticmethod
    def load_config(config_path: str | None = None) -> dict[str, Any]:
        """Load serverless configuration."""
        default_config = {
            "storage_backend": "local",
            "session_ttl_hours": 24,
            "cleanup_interval_hours": 6,
            "backends": {
                "redis": {
                    "host": "localhost",
                    "port": 6379,
                    "db": 0,
                    "key_prefix": "session_mgmt:",
                },
                "s3": {
                    "bucket_name": "session-mgmt-mcp",
                    "region": "us-east-1",
                    "key_prefix": "sessions/",
                },
                "local": {
                    "storage_dir": str(Path.home() / ".claude" / "data" / "sessions"),
                },
            },
        }

        if config_path and Path(config_path).exists():
            try:
                with open(config_path) as f:
                    file_config = json.load(f)
                    default_config.update(file_config)
            except (OSError, json.JSONDecodeError):
                pass

        return default_config

    @staticmethod
    def create_storage_backend(config: dict[str, Any]) -> SessionStorage:
        """Create storage backend from config.

        Supports legacy backends (redis, s3, local) and new ACB cache backend.
        ACB is the recommended default for new deployments.
        """
        backend_type = config.get("storage_backend", "acb")  # Default to ACB
        backend_config = config.get("backends", {}).get(backend_type, {})

        # ACB-style cache backend (using aiocache directly)
        if backend_type == "acb":
            try:
                # Use aiocache directly (same as ACB uses internally)
                # This avoids ACB DI complexity while still getting benefits
                from aiocache import Cache as AIOCache
                from aiocache.serializers import PickleSerializer

                cache_type = backend_config.get("cache_type", "memory")

                # Configure aiocache backend
                if cache_type == "redis":
                    try:
                        from aiocache.backends.redis import RedisBackend

                        cache = AIOCache(
                            cache_class=RedisBackend,
                            serializer=PickleSerializer(),
                            endpoint=backend_config.get("host", "localhost"),
                            port=backend_config.get("port", 6379),
                            password=backend_config.get("password"),
                            db=backend_config.get("db", 0),
                        )
                    except ImportError:
                        CONFIG_LOGGER.warning("redis not available, using memory cache")
                        cache = AIOCache(serializer=PickleSerializer())
                else:
                    # Memory cache (default)
                    cache = AIOCache(serializer=PickleSerializer())

                namespace = backend_config.get("namespace", "session")
                return ACBCacheStorage(cache, namespace, backend_config)

            except ImportError as e:
                CONFIG_LOGGER.warning(
                    f"aiocache not available ({e}), falling back to local file storage. "
                    "Install with: pip install aiocache[redis]"
                )
                # Fallback to local storage if aiocache not available
                return LocalFileStorage(backend_config)

        # Legacy backends (deprecated)
        if backend_type == "redis":
            CONFIG_LOGGER.warning(
                "RedisStorage is deprecated. Use 'acb' backend for better "
                "connection pooling, SSL/TLS support, and automatic compression."
            )
            return RedisStorage(backend_config)

        if backend_type == "s3":
            CONFIG_LOGGER.warning(
                "S3Storage is deprecated. Consider using ACB cache backends "
                "with Redis for production deployments."
            )
            return S3Storage(backend_config)

        if backend_type == "local":
            return LocalFileStorage(backend_config)

        msg = f"Unsupported storage backend: {backend_type}"
        raise ValueError(msg)

    @staticmethod
    async def test_storage_backends(config: dict[str, Any]) -> dict[str, bool]:
        """Test all configured storage backends."""
        results: dict[str, bool] = {}

        for backend_name, backend_config in config.get("backends", {}).items():
            try:
                storage: SessionStorage
                match backend_name:
                    case "acb":
                        # Test ACB-style cache backend
                        try:
                            from aiocache import Cache as AIOCache
                            from aiocache.serializers import PickleSerializer

                            backend_config.get("cache_type", "memory")
                            cache = AIOCache(serializer=PickleSerializer())
                            namespace = backend_config.get("namespace", "session")
                            storage = ACBCacheStorage(cache, namespace, backend_config)
                        except ImportError:
                            results[backend_name] = False
                            continue

                    case "redis":
                        storage = RedisStorage(backend_config)
                    case "s3":
                        storage = S3Storage(backend_config)
                    case "local":
                        storage = LocalFileStorage(backend_config)
                    case _:
                        results[backend_name] = False
                        continue

                results[backend_name] = await storage.is_available()

            except Exception:
                results[backend_name] = False

        return results
