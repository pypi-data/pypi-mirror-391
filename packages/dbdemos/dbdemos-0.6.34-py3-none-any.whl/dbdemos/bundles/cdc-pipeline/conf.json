{"name": "cdc-pipeline", "category": "data-engineering", "serverless_supported": true, "custom_schema_supported": true, "default_catalog": "main", "default_schema": "dbdemos_cdc_pipeline", "title": "CDC Pipeline with Delta", "description": "Process CDC data to build an entire pipeline and materialize your operational tables in your lakehouse.", "fullDescription": "This demo highlight how to implement a CDC flow (Change Data Capture) with Spark API and Delta Lake.<br/> CDC is typically done ingesting changes from external system (ERP, SQL databases) with tools like fivetran, debezium etc. <br/> In this demo, we'll show you how to re-create your table consuming CDC information. <br/><br/>Ultimately, we'll show you how to programatically scan multiple incoming folder and trigger N stream (1 for each CDC table).<br/>Note that CDC is made easier with Spark Declarative Pipelines (CDC). We recommend you to try the SDP CDC demo!", "usecase": "Data Engineering", "products": ["Delta Lake", "Spark", "CDC"], "related_links": [{"title": "View all Product demos", "url": "<TBD: LINK TO A FILTER WITH ALL DBDEMOS CONTENT>"}, {"title": "Databricks Delta Lake CDC", "url": "https://www.databricks.com/blog/2021/06/09/how-to-simplify-cdc-with-delta-lakes-change-data-feed.html"}], "recommended_items": ["sdp-cdc", "sdp-loans", "delta-lake"], "demo_assets": [], "bundle": true, "tags": [{"delta": "Delta Lake"}], "notebooks": [{"path": "_resources/00-setup", "pre_run": false, "publish_on_website": false, "add_cluster_setup_cell": false, "title": "Setup", "description": "Setup.", "object_type": "NOTEBOOK"}, {"path": "_resources/01-load-data", "pre_run": false, "publish_on_website": false, "add_cluster_setup_cell": false, "title": "Data initialization", "description": "Data initialization", "object_type": "NOTEBOOK"}, {"path": "01-CDC-CDF-simple-pipeline", "pre_run": true, "publish_on_website": true, "add_cluster_setup_cell": true, "title": "Implement CDC flow with Delta Lake", "description": "Ingest CDC data and materialize your tables and propagate changes downstream."}, {"path": "02-CDC-CDF-full-multi-tables", "pre_run": true, "publish_on_website": true, "add_cluster_setup_cell": true, "title": "Delta Lake Performance & operation", "description": "Programatically ingest multiple CDC flows to synch all your database."}, {"path": "_resources/00-global-setup-v2", "title": "Global init", "description": "Global init", "pre_run": false, "publish_on_website": false, "add_cluster_setup_cell": false, "parameters": {}, "depends_on_previous": true, "libraries": [], "warehouse_id": null, "object_type": null}], "cluster": {"spark_version": "16.4.x-cpu-ml-scala2.12", "spark_conf": {"spark.master": "local[*]", "spark.databricks.cluster.profile": "singleNode"}, "custom_tags": {"ResourceClass": "SingleNode"}, "single_user_name": "{{CURRENT_USER}}", "data_security_mode": "SINGLE_USER", "num_workers": 0}}