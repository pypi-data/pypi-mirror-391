[project]
name = "NL2SQLEvaluator"
version = "1.3.4"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12,<3.13"
dependencies = [
    "bm25s>=0.2.14",
    "datasets>=4.0.0",
    "fastparquet>=2024.11.0",
    "func-timeout>=4.3.5",
    "loguru>=0.7.3",
    "pandas>=2.3.2",
    "pyarrow>=21.0.0",
    "pydantic>=2.11.7",
    "python-dotenv>=1.1.1",
    "sqlglot>=27.11.0",
    "tqdm>=4.67.1",
    "wandb>=0.21.3",
    "typer>=0.20.0",
    "typer-config[all]>=1.4.2",
]


[dependency-groups]
dev = [
    "pytest>=8.4.1",
]

models = [
    "vllm==0.10.2",
    "flashinfer-python",
    "flashinfer-cubin",
    "flash-attn==2.8.3",
    "litellm>=1.77.7",
    "transformers>=4.56.0",
]

[project.scripts]
nl2sql_eval = "NL2SQLEvaluator.cli:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.uv.sources]
flashinfer-jit-cache = { index = "flashinfer" }
# https://github.com/Dao-AILab/flash-attention/releases/ if you change python version or cuda version or torch version you have to change also the whell
#flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiFALSE-cp312-cp312-linux_x86_64.whl" }

[[tool.uv.index]]
name = "flashinfer"
url = "https://flashinfer.ai/whl/cu124"

