Metadata-Version: 2.3
Name: cntext
Version: 2.2.0
Summary: Chinese text analysis library, which can perform word frequency statistics, dictionary expansion, sentiment analysis, similarity, readability, co-occurrence analysis, social computing (attitude, prejudice, culture) on texts; Now support Ollama(LLM) analysis
License: MIT
Author: DaDeng
Requires-Python: >=3.8,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: PyMuPDF (>=1.25.4,<2.0.0)
Requires-Dist: PyYAML (>=6.0.2,<7.0.0)
Requires-Dist: aiolimiter (==1.2.1)
Requires-Dist: chardet (>=5.0.0,<6.0.0)
Requires-Dist: contractions (>=0.1.73,<0.2.0)
Requires-Dist: distinctiveness (>=0.14.6,<0.15.0)
Requires-Dist: ftfy (>=6.2.0,<7.0.0)
Requires-Dist: gensim (>=4.2.0,<5.0.0)
Requires-Dist: h5py (>=3.9.0,<4.0.0)
Requires-Dist: instructor (>=1.11.2,<2.0.0)
Requires-Dist: jieba (>=0.42,<0.43)
Requires-Dist: matplotlib (>=3.6.0,<4.0.0)
Requires-Dist: nest-asyncio (>=1.6.0,<2.0.0)
Requires-Dist: networkx (>=3.2,<4.0)
Requires-Dist: nltk (>=3.8,<4.0)
Requires-Dist: numpy (>=1.26.4,<2.0.0)
Requires-Dist: ollama (>=0.2.1,<0.3.0)
Requires-Dist: openai (>=1.107.0,<2.0.0)
Requires-Dist: opencc-python-reimplemented (>=0.1.6,<0.2.0)
Requires-Dist: pandas (>=2.2.0,<3.0.0)
Requires-Dist: pydantic (>=2.11.7,<3.0.0)
Requires-Dist: pyecharts (==2.0.0)
Requires-Dist: python-docx (>=1.1.2,<2.0.0)
Requires-Dist: scikit-learn (>=1.5.0,<2.0.0)
Requires-Dist: scipy (>=1.12.0,<2.0.0)
Requires-Dist: tqdm (>=4.66.4,<5.0.0)
Description-Content-Type: text/markdown

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**

- [cntextï¼šé¢å‘ç¤¾ä¼šç§‘å­¦ç ”ç©¶çš„ä¸­æ–‡æ–‡æœ¬åˆ†æå·¥å…·åº“](#cntexté¢å‘ç¤¾ä¼šç§‘å­¦ç ”ç©¶çš„ä¸­æ–‡æ–‡æœ¬åˆ†æå·¥å…·åº“)
- [å®‰è£… cntext](#å®‰è£…-cntext)
- [åŠŸèƒ½æ¨¡å—](#åŠŸèƒ½æ¨¡å—)
- [QuickStart](#quickstart)
- [ä¸€ã€IO æ¨¡å—](#ä¸€io-æ¨¡å—)
  - [1.1 get\_dict\_list()](#11-get_dict_list)
  - [1.2 å†…ç½® yaml è¯å…¸](#12-å†…ç½®-yaml-è¯å…¸)
  - [1.3 read\_dict\_yaml()](#13-read_dict_yaml)
  - [1.4 detect\_encoding()](#14-detect_encoding)
  - [1.5 get\_files(fformat)](#15-get_filesfformat)
  - [1.6 read\_pdf](#16-read_pdf)
  - [1.7 read\_docx](#17-read_docx)
  - [1.8 read\_file()](#18-read_file)
  - [1.9 read\_files()](#19-read_files)
  - [1.10 extract\_mda](#110-extract_mda)
  - [1.11 traditional2simple()](#111-traditional2simple)
  - [1.12 fix\_text()](#112-fix_text)
  - [1.13 fix\_contractions(text)](#113-fix_contractionstext)
  - [1.14 clean\_text(text)](#114-clean_texttext)
- [äºŒã€Stats æ¨¡å—](#äºŒstats-æ¨¡å—)
  - [2.1 word\_count()](#21-word_count)
  - [2.2 readability()](#22-readability)
  - [2.3 sentiment(text, diction, lang)](#23-sentimenttext-diction-lang)
  - [2.4 sentiment\_by\_valence()](#24-sentiment_by_valence)
  - [2.5 word\_in\_context()](#25-word_in_context)
  - [2.6 epu()](#26-epu)
  - [2.7 fepu()](#27-fepu)
  - [2.8 semantic\_brand\_score()](#28-semantic_brand_score)
  - [2.9 æ–‡æœ¬ç›¸ä¼¼åº¦](#29-æ–‡æœ¬ç›¸ä¼¼åº¦)
  - [2.10 word\_hhi](#210-word_hhi)
- [ä¸‰ã€Plot æ¨¡å—](#ä¸‰plot-æ¨¡å—)
  - [3.1 matplotlib\_chinese()](#31-matplotlib_chinese)
  - [3.2 lexical\_dispersion\_plot1()](#32-lexical_dispersion_plot1)
  - [3.3 lexical\_dispersion\_plot2()](#33-lexical_dispersion_plot2)
- [å››ã€Model æ¨¡å—](#å››model-æ¨¡å—)
  - [4.1 Word2Vec()](#41-word2vec)
  - [4.2 GloVe()](#42-glove)
  - [4.3 evaluate\_similarity()](#43-evaluate_similarity)
  - [4.4 evaluate\_analogy()](#44-evaluate_analogy)
  - [4.5 SoPmi()](#45-sopmi)
  - [4.6 load\_w2v()](#46-load_w2v)
  - [4.7 glove2word2vec()](#47-glove2word2vec)
  - [æ³¨æ„](#æ³¨æ„)
  - [4.8 expand\_dictionary()](#48-expand_dictionary)
- [äº”ã€Mind æ¨¡å—](#äº”mind-æ¨¡å—)
  - [5.1 semantic\_centroid(wv, words)](#51-semantic_centroidwv-words)
  - [5.2 generate\_concept\_axis(wv, poswords, negwords)](#52-generate_concept_axiswv-poswords-negwords)
  - [5.3 sematic\_distance()](#53-sematic_distance)
  - [5.4 sematic\_projection()](#54-sematic_projection)
  - [5.5 project\_word](#55-project_word)
  - [5.6 project\_text()](#56-project_text)
  - [5.7 divergent\_association\_task()](#57-divergent_association_task)
  - [5.8 discursive\_diversity\_score()](#58-discursive_diversity_score)
  - [5.8 procrustes\_align()](#58-procrustes_align)
- [å…­ã€LLM æ¨¡å—](#å…­llm-æ¨¡å—)
  - [6.1 ct.llm()](#61-ctllm)
  - [6.2 å†…ç½®prompt](#62-å†…ç½®prompt)
- [ä½¿ç”¨å£°æ˜](#ä½¿ç”¨å£°æ˜)
  - [apalike](#apalike)
  - [bibtex](#bibtex)
  - [endnote](#endnote)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->



## cntextï¼šé¢å‘ç¤¾ä¼šç§‘å­¦ç ”ç©¶çš„ä¸­æ–‡æ–‡æœ¬åˆ†æå·¥å…·åº“
cntext æ˜¯ä¸“ä¸º**ç¤¾ä¼šç§‘å­¦å®è¯ç ”ç©¶è€…**è®¾è®¡çš„ä¸­æ–‡æ–‡æœ¬åˆ†æ Python åº“ã€‚å®ƒä¸æ­¢äºè¯é¢‘ç»Ÿè®¡å¼çš„ä¼ ç»Ÿæƒ…æ„Ÿåˆ†æï¼Œè¿˜æ‹¥æœ‰è¯åµŒå…¥è®­ç»ƒã€è¯­ä¹‰æŠ•å½±è®¡ç®—ï¼Œ**å¯ä»å¤§è§„æ¨¡éç»“æ„åŒ–æ–‡æœ¬ä¸­æµ‹é‡æŠ½è±¡æ„å¿µ**â€”â€”å¦‚æ€åº¦ã€è®¤çŸ¥ã€æ–‡åŒ–è§‚å¿µä¸å¿ƒç†çŠ¶æ€ã€‚

ğŸ¯ **ä½ èƒ½ç”¨å®ƒåšä»€ä¹ˆ**
1. æ„å»ºç»“æ„åŒ–ç ”ç©¶æ•°æ®é›†
   - æ±‡æ€»å¤šä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ˆtxt/pdf/docx/csvï¼‰ä¸º DataFrameï¼š``ct.read_files()``
   - æå–ä¸Šå¸‚å…¬å¸å¹´æŠ¥ä¸­çš„â€œç®¡ç†å±‚è®¨è®ºä¸åˆ†æâ€ï¼ˆMD&Aï¼‰ï¼š``ct.extract_mda()``
   - è®¡ç®—æ–‡æœ¬å¯è¯»æ€§æŒ‡æ ‡ï¼ˆå¦‚FleschæŒ‡æ•°ï¼‰ï¼š``ct.readability()``

2. **åŸºç¡€æ–‡æœ¬åˆ†æ(ä¼ ç»Ÿæ–¹æ³•)**
   - è¯é¢‘ç»Ÿè®¡ä¸å…³é”®è¯æå–ï¼š``ct.word_count()``
   - æƒ…æ„Ÿåˆ†æï¼ˆå¯é€‰hownetã€dutirç­‰å†…ç½®è¯å…¸ï¼‰ï¼š``ct.sentiment()``
   - æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆä½™å¼¦è·ç¦»ï¼‰ï¼š``ct.cosine_sim()``

3. **æµ‹é‡å†…éšæ€åº¦ä¸æ–‡åŒ–å˜è¿**
   - ä¸¤è¡Œä»£ç è®­ç»ƒé¢†åŸŸä¸“ç”¨è¯å‘é‡ï¼ˆWord2Vec/GloVeï¼‰ï¼š``ct.Word2Vec()``
   - æ„å»ºæ¦‚å¿µè¯­ä¹‰è½´ï¼ˆå¦‚â€œåˆ›æ–° vs å®ˆæ—§â€ï¼‰ï¼š``ct.generate_concept_axis()``
   - é€šè¿‡è¯­ä¹‰æŠ•å½±é‡åŒ–åˆ»æ¿å°è±¡ã€ç»„ç»‡æ–‡åŒ–åç§»ï¼š``ct.project_text()``
4. **èåˆå¤§æ¨¡å‹è¿›è¡Œç»“æ„åŒ–åˆ†æ**
   - è°ƒç”¨ LLM å¯¹æ–‡æœ¬è¿›è¡Œè¯­ä¹‰è§£æï¼Œè¿”å›ç»“æ„åŒ–ç»“æœï¼ˆå¦‚æƒ…ç»ªç»´åº¦ã€æ„å›¾åˆ†ç±»ï¼‰ï¼š``ct.llm()``


cntext ä¸è¿½æ±‚é»‘ç®±é¢„æµ‹ï¼Œè€Œè‡´åŠ›äºè®©æ–‡æœ¬æˆä¸ºç†è®ºé©±åŠ¨çš„ç§‘å­¦æµ‹é‡å·¥å…·ã€‚ å¼€æºå…è´¹ï¼Œæ¬¢è¿å­¦ç•ŒåŒä»ä½¿ç”¨ã€éªŒè¯ä¸å…±å»ºã€‚

<br><br>

## å®‰è£… cntext

```
pip3 install cntext --upgrade
```

<br>

éœ€è¦æ³¨æ„ï¼Œ **cntext ä½¿ç”¨ç¯å¢ƒä¸º Python3.9 ~ 3.12**,å¦‚å®‰è£…å¤±è´¥ï¼Œé—®é¢˜å¯èƒ½å‡ºåœ¨ python ç‰ˆæœ¬é—®é¢˜ï¼› 

<br><br>

## åŠŸèƒ½æ¨¡å—
```python
import cntext as ct
ct.hello()
```
![](img/01-hello.jpg)

cntext å« ioã€modelã€statsã€mind äº”ä¸ªæ¨¡å—

1. å¯¼å…¥æ•°æ®ç”¨ io
2. è®­ç»ƒæ¨¡å‹æ‰©å±•è¯å…¸ç”¨ model
3. ç»Ÿè®¡è¯é¢‘ã€æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼åº¦ç­‰ç”¨ stats
4. å¯è§†åŒ–æ¨¡å— plot
5. æ€åº¦è®¤çŸ¥æ–‡åŒ–å˜è¿ç”¨ mind
6. å¤§æ¨¡å‹ LLM

å‡½æ•°éƒ¨åˆ†åŠ ç²—çš„ä¸ºå¸¸ç”¨å‡½æ•°ã€‚

| æ¨¡å—        | å‡½æ•°                                                                                           | åŠŸèƒ½                                                                           |
| ----------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| **io**    | **_ct.get_cntext_path()_**                                                                     | æŸ¥çœ‹ cntext å®‰è£…è·¯å¾„                                                           |
| **io**    | **_ct.get_dict_list()_**                                                                       | æŸ¥çœ‹ cntext å†…ç½®è¯å…¸                                                           |
| **io**    | `ct.get_files(fformat)`                                                                        | æŸ¥çœ‹ç¬¦åˆ fformat è·¯å¾„è§„åˆ™çš„æ‰€æœ‰çš„æ–‡ä»¶                                          |
| **io**    | `ct.detect_encoding(file, num_lines=100)`                                                      | è¯Šæ–­ txtã€csv ç¼–ç æ ¼å¼                                                         |
| **io**    | **_ct.read_yaml_dict(yfile)_**                                                                 | è¯»å–å†…ç½® yaml è¯å…¸                                                             |
| **io**    | **_ct.read_pdf(file)_**                                                                        | è¯»å– PDF æ–‡ä»¶                                                                  |
| **io**    | **_ct.read_docx(file)_**                                                                       | è¯»å– docx æ–‡ä»¶                                                                 |
| **io**    | **_ct.read_file(file, encodings)_**                                                            | è¯»å–æ–‡ä»¶                                                                       |
| **io**    | **_ct.read_files(fformat, encoding)_**                                                         | è¯»å–ç¬¦åˆ fformat è·¯å¾„è§„åˆ™çš„æ‰€æœ‰çš„æ–‡ä»¶ï¼Œè¿”å› df                                 |
| **io**    | **_ct.extract_mda(text, kws_pattern)_**                                                        | æå– A è‚¡å¹´æŠ¥ä¸­çš„ MD&A æ–‡æœ¬å†…å®¹ã€‚å¦‚æœè¿”å›'',åˆ™æå–å¤±è´¥ã€‚                       |
| **io**    | **_ct.traditional2simple(text)_**                                                              | ç¹ä½“è½¬ç®€ä½“                                                                     |
| **io**    | **_ct.fix_text(text)_**                                                                        | å°†ä¸æ­£å¸¸çš„ã€æ··ä¹±ç¼–ç çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ­£å¸¸çš„æ–‡æœ¬ã€‚ä¾‹å¦‚å…¨è§’è½¬åŠè§’                     |
| **io**    | `ct.fix_contractions(text)`                                                                    | è‹±æ–‡ç¼©å†™(å«ä¿šè¯­è¡¨è¾¾)å¤„ç†ï¼Œ å¦‚ you're -> you are                                |
| **io** | `ct.clean_text(text, lang='chinese')`               | ä¸­æ–‡ã€è‹±æ–‡æ–‡æœ¬æ¸…æ´—         |
| **model** | **_ct.Word2Vec(corpus_file, encoding, lang='chinese', ...)_**                                  | è®­ç»ƒ Word2Vec                                                                  |
| **model** | **_ct.GloVe(corpus_file, encoding, lang='chinese', ...)_**                                     | GloVe, åº•å±‚ä½¿ç”¨çš„ [Standfordnlp/GloVe](https://github.com/standfordnlp/GloVe)  |
| **model** | **_ct.evaluate_similarity(wv, file=None)_**                                                    | ä½¿ç”¨è¿‘ä¹‰æ³•è¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®çš„æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚                           |
| **model** | **_ct.evaluate_analogy(wv, file=None)_**                                                       | ä½¿ç”¨ç±»æ¯”æ³•è¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®çš„æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚                           |
| **model** | **_ct.glove2word2vec(glove_file, word2vec_file)_**                                             | å°† GLoVe æ¨¡å‹.txt æ–‡ä»¶è½¬åŒ–ä¸º Word2Vec æ¨¡å‹.txt æ–‡ä»¶ï¼› ä¸€èˆ¬å¾ˆå°‘ç”¨åˆ°             |
| **model** | **_ct.load_w2v(wv_path)_**                                                                     | è¯»å– cntext2.x è®­ç»ƒå‡ºçš„ Word2Vec/GloVe æ¨¡å‹æ–‡ä»¶                                |
| **model** | **_ct.expand_dictionary(wv, seeddict, topn=100)_**                                             | æ‰©å±•è¯å…¸, ç»“æœä¿å­˜åˆ°è·¯å¾„[output/Word2Vec]ä¸­                                    |
| **model** | `ct.SoPmi(corpus_file, seed_file, lang='chinese')`                                             | å…±ç°æ³•æ‰©å±•è¯å…¸                                                                 |
| **stats** | `ct.word_count(text, lang='chinese')`                                                          | è¯é¢‘ç»Ÿè®¡                                                                       |
| **stats** | `readability(text, lang='chinese', syllables=3)`                                               | æ–‡æœ¬å¯è¯»æ€§                                                                     |
| **stats** | **_ct.sentiment(text, diction, lang='chinese')_**                                              | æ— (ç­‰)æƒé‡è¯å…¸çš„æƒ…æ„Ÿåˆ†æ                                                       |
| **stats** | `ct.sentiment_by_valence(text, diction, lang='chinese')`                                       | å¸¦æƒé‡çš„è¯å…¸çš„æƒ…æ„Ÿåˆ†æ                                                         |
| **stats** | **_ct.word_in_context(text, keywords, window=3, lang='chinese')_**                             | åœ¨ text ä¸­æŸ¥æ‰¾ keywords å‡ºç°çš„ä¸Šä¸‹æ–‡å†…å®¹(çª—å£ window)ï¼Œè¿”å› df                 |
| **stats** | **_ct.epu()_**                                                                                 | ä½¿ç”¨æ–°é—»æ–‡æœ¬æ•°æ®è®¡ç®—ç»æµæ”¿ç­–ä¸ç¡®å®šæ€§ EPUï¼Œè¿”å› df                              |
| **stats** | **_ct.fepu(text, ep_pattern='', u_pattern='')_**                                               | ä½¿ç”¨ md&a æ–‡æœ¬æ•°æ®è®¡ç®—ä¼ä¸šä¸ç¡®å®šæ€§æ„ŸçŸ¥ FEPU                                    |
| **stats** | **_ct.semantic_brand_score(text, brands, lang='chinese')_**                                    | è¡¡é‡å“ç‰Œï¼ˆä¸ªä½“ã€å…¬å¸ã€å“ç‰Œã€å…³é”®è¯ç­‰ï¼‰çš„é‡è¦æ€§                                 |
| **stats** | **_ct.cosine_sim(text1, text2, lang='chinese')_**                                              | ä½™å¼¦ç›¸ä¼¼åº¦                                                                     |
| **stats** | `ct.jaccard_sim(text1, text2, lang='chinese')`                                                 | Jaccard ç›¸ä¼¼åº¦                                                                 |
| **stats** | `ct.minedit_sim(text1, text2, lang='chinese')`                                                 | æœ€å°ç¼–è¾‘è·ç¦»                                                                   |
| **stats** | `ct.word_hhi(text)`                                                                            | æ–‡æœ¬çš„èµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°                                                      |
| **_plot_**  | `ct.matplotlib_chinese()`                                                                      | æ”¯æŒ matplotlib ä¸­æ–‡ç»˜å›¾                                                       |
| **plot**  | `ct.lexical_dispersion_plot1(text, targets_dict, lang, title, figsize)`                        | å¯¹æŸä¸€ä¸ªæ–‡æœ¬ textï¼Œ å¯è§†åŒ–ä¸åŒç›®æ ‡ç±»åˆ«è¯ targets_dict åœ¨æ–‡æœ¬ä¸­å‡ºç°ä½ç½®         |
| **plot**  | `ct.lexical_dispersion_plot2(texts_dict, targets, lang, title, figsize)`                       | å¯¹æŸå‡ ä¸ªæ–‡æœ¬ texts_dictï¼Œ å¯è§†åŒ–æŸäº›ç›®æ ‡è¯ targets åœ¨æ–‡æœ¬ä¸­å‡ºç°ç›¸å¯¹ä½ç½®(0~100) |
| **mind**  | `ct.generate_concept_axis(wv, poswords, negwords)`                                                 | ç”Ÿæˆæ¦‚å¿µè½´å‘é‡ã€‚                                                               |
| **mind**  | **_tm = ct.Text2Mind(wv)_**<br>                                                                | å•ä¸ª word2vec å†…æŒ–æ˜æ½œåœ¨çš„æ€åº¦åè§ã€åˆ»æ¿å°è±¡ç­‰ã€‚tm å«å¤šé‡æ–¹æ³•                  |
| **mind**  | `sematic_projection(wv, words, poswords, negwords, return_full=False, cosine=False)`                                            | æµ‹é‡è¯­ä¹‰æŠ•å½±                                                                   |
| **mind**  | `ct.project_word(wv, a, b, cosine=False)`                                                                    | è®¡ç®—è¯è¯­ a åœ¨è¯è¯­ b ä¸Šçš„æŠ•å½±                                                   |
| **mind**  | `ct.project_text(wv, text, axis, lang='chinese', cosine=False)`                                                                    | è®¡ç®—è¯è¯­æ–‡æœ¬textåœ¨æ¦‚å¿µè½´å‘é‡axisä¸Šçš„æŠ•å½±å€¼|
| **mind**  | `ct.sematic_distance(wv, words1, words2)`                                                      | æµ‹é‡è¯­ä¹‰è·ç¦»                                                                   |
| **mind**  | `ct.divergent_association_task(wv, words)`                                                     | æµ‹é‡å‘æ•£æ€ç»´(åˆ›é€ åŠ›)                                                           |
| **mind**  | `ct.discursive_diversity_score(wv, words)`                                                     | æµ‹é‡è¯­è¨€å·®å¼‚æ€§(è®¤çŸ¥å·®å¼‚æ€§)                                                     |
| **mind**  | **ct.procrustes_align(base_wv, other_wv)**                                                   | ä¸¤ä¸ª word2vec è¿›è¡Œè¯­ä¹‰å¯¹é½ï¼Œå¯ååº”éšæ—¶é—´çš„ç¤¾ä¼šè¯­ä¹‰å˜è¿                         |
| **_LLM_**   | **ct.llm(text, prompt, output_format, task, backend, base_url, api_key, model_name, temperature)** | è°ƒç”¨å¤§æ¨¡å‹æ‰§è¡Œç»“æ„åŒ–æ–‡æœ¬åˆ†æä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æã€å…³é”®è¯æå–ã€åˆ†ç±»ç­‰ï¼‰ã€‚ |

<br><br>

## QuickStart

```python
import cntext as ct

print('å½“å‰cntextç‰ˆæœ¬: ', ct.__version__)
help(ct)
```

Run

```
å½“å‰cntextç‰ˆæœ¬: 2.1.7

Help on package cntext:

NAME
    cntext

PACKAGE CONTENTS
    io
    mind
    model
    stats
    llm
...
```

<br>

<br>

## ä¸€ã€IO æ¨¡å—

| æ¨¡å—     | å‡½æ•°                                      | åŠŸèƒ½                                                       |
| -------- | ----------------------------------------- | ---------------------------------------------------------- |
| **io** | **_ct.get_dict_list()_**                  | æŸ¥çœ‹ cntext å†…ç½®è¯å…¸                                       |
| **io** | **_ct.read_yaml_dict(yfile)_**            | è¯»å–å†…ç½® yaml è¯å…¸                                         |
| **io** | `ct.detect_encoding(file, num_lines=100)` | è¯Šæ–­ txtã€csv ç¼–ç æ ¼å¼                                     |
| **io** | `ct.get_files(fformat)`                   | æŸ¥çœ‹ç¬¦åˆ fformat è·¯å¾„è§„åˆ™çš„æ‰€æœ‰çš„æ–‡ä»¶                      |
| **io** | **_ct.read_yaml_dict(yfile)_**            | è¯»å–å†…ç½® yaml è¯å…¸                                         |
| **io** | **_ct.read_pdf(file)_**                   | è¯»å– PDF æ–‡ä»¶                                              |
| **io** | **_ct.read_file(file, encoding)_**        | è¯»å–æ–‡ä»¶                                                   |
| **io** | **_ct.read_files(fformat, encoding)_**    | è¯»å–ç¬¦åˆ fformat è·¯å¾„è§„åˆ™çš„æ‰€æœ‰çš„æ–‡ä»¶ï¼Œè¿”å› df             |
| **io** | **_ct.extract_mda(text, kws_pattern)_**   | æå– A è‚¡å¹´æŠ¥ä¸­çš„ MD&A æ–‡æœ¬å†…å®¹ã€‚å¦‚æœè¿”å›'',åˆ™æå–å¤±è´¥ã€‚   |
| **io** | **_ct.traditional2simple(text)_**         | ç¹ä½“è½¬ç®€ä½“                                                 |
| **io** | **_ct.fix_text(text)_**                   | å°†ä¸æ­£å¸¸çš„ã€æ··ä¹±ç¼–ç çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ­£å¸¸çš„æ–‡æœ¬ã€‚ä¾‹å¦‚å…¨è§’è½¬åŠè§’ |
| **io** | `ct.fix_contractions(text)`               | è‹±æ–‡ç¼©å†™(å«ä¿šè¯­è¡¨è¾¾)å¤„ç†ï¼Œ å¦‚ you're -> you are            |
| **io** | `ct.clean_text(text, lang='chinese')`               | ä¸­æ–‡ã€è‹±æ–‡æ–‡æœ¬æ¸…æ´—         |


### 1.1 get_dict_list()

æŸ¥çœ‹ cntext å†…ç½®è¯å…¸

```python
import cntext as ct

ct.get_dict_list()
```

Run

```
['zh_common_NTUSD.yaml',
 'zh_common_DUTIR.yaml',
 'enzh_common_StopWords.yaml',
 'en_valence_Concreteness.yaml',
 'en_common_LoughranMcDonald.yaml',
 'zh_common_FinanceSenti.yaml',
 'zh_common_FLS.yaml',
 'zh_common_TsinghuaPraiseDegrade.yaml',
 'zh_common_FEPU.yaml',
 'en_common_ANEW.yaml',
 'en_common_NRC.yaml',
 'zh_valence_ChineseEmoBank.yaml',
 'zh_valence_SixSemanticDimensionDatabase.yaml',
 'zh_common_FinacialFormalUnformal.yaml',
 'zh_common_LoughranMcDonald.yaml',
 'enzh_common_AdvConj.yaml',
 'en_common_SentiWS.yaml',
 'zh_common_Digitalization.yaml',
 'en_common_LSD2015.yaml',
 'zh_common_HowNet.yaml',
 'zh_common_EPU.yaml']
```

### 1.2 å†…ç½® yaml è¯å…¸

| pkl æ–‡ä»¶                                           | è¯å…¸                                                                                                                                                              | è¯­è¨€    | åŠŸèƒ½                                                                                                                              |
| -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------- |
| **_zh_valence_ChineseEmoBank.yaml_**               | ä¸­æ–‡æƒ…æ„Ÿè¯å…¸ï¼Œå«`æ•ˆä»·valence`å’Œ`å”¤é†’åº¦arousal`ã€‚åœ¨ cntext ä¸­ï¼Œæˆ‘ä»¬åªä½¿ç”¨äº† CVAW è¯è¡¨(å•è¯)ï¼Œå…¶ä»–è¯å…¸å¦‚ CVAP, CVAS, CVAT æ²¡æœ‰çº³å…¥åˆ° ChineseEmoBank.pkl.            | Chinese | `æ•ˆä»·valence`å’Œ`å”¤é†’åº¦arousal`                                                                                                    |
| **_zh_common_DUTIR.yaml_**                         | å¤§è¿ç†å·¥å¤§å­¦æƒ…æ„Ÿæœ¬ä½“åº“                                                                                                                                            | ä¸­æ–‡    | ä¸ƒå¤§ç±»æƒ…ç»ªï¼Œ`å“€, å¥½, æƒŠ, æƒ§, ä¹, æ€’, æ¶`                                                                                          |
| **_zh_common_HowNet.yaml_**                        | çŸ¥ç½‘ Hownet è¯å…¸                                                                                                                                                  | ä¸­æ–‡    | æ­£é¢è¯ã€è´Ÿé¢è¯                                                                                                                    |
| `en_common_SentiWS.yaml`                           | SentimentWortschatz (SentiWS)                                                                                                                                     | å¾·æ–‡    | æ­£é¢è¯ã€è´Ÿé¢è¯ï¼›<br>                                                                                                              |
| **_zh_common_FinacialFormalUnformal.yaml_**        | é‡‘èé¢†åŸŸæ­£å¼ã€éæ­£å¼ï¼›ç§¯ææ¶ˆæ                                                                                                                                    | ä¸­æ–‡    | formal-posã€<br>formal-negï¼›<br>unformal-posã€<br>unformal-neg                                                                    |
| `en_common_ANEW.yaml`                              | è‹±è¯­å•è¯çš„æƒ…æ„Ÿè§„èŒƒ Affective Norms for English Words (ANEW)                                                                                                       | è‹±æ–‡    | pleasure, arousal, dominance                                                                                                      |
| `en_common_LSD2015.yaml`                           | Lexicoder Sentiment Dictionary (2015)                                                                                                                             | è‹±æ–‡    | æ­£é¢è¯ã€è´Ÿé¢è¯                                                                                                                    |
| `en_common_NRC.yaml`                               | NRC Word-Emotion Association Lexicon                                                                                                                              | è‹±æ–‡    | ç»†ç²’åº¦æƒ…ç»ªè¯ï¼›                                                                                                                    |
| **_zh_valence_SixSemanticDimensionDatabase.yaml_** | [**é€šç”¨ä¸­è‹±æ–‡å…­ç»´è¯­ä¹‰æƒ…æ„Ÿè¯å…¸**](https://textdata.cn/blog/2023-03-20-nature-six-semantic-dimension-database/), å« 17940 ä¸ªä¸­æ–‡è¯çš„å…­ç»´åº¦è¯åº“ï¼Œ ä¸”æ¯ä¸ªç»´åº¦æœ‰æƒé‡ã€‚ | ä¸­æ–‡    | visionã€socialnessã€emotionã€timeã€spaceã€motor                                                                                   |
| `enzh_common_AdvConj.yaml`                         | å‰¯è¯è¿è¯                                                                                                                                                          | ä¸­ã€è‹±  |                                                                                                                                   |
| **_enzh_common_StopWords.yaml_**                   | ä¸­è‹±æ–‡åœç”¨è¯                                                                                                                                                      | ä¸­ã€è‹±  | åœç”¨è¯                                                                                                                            |
| **_en_valence_Concreteness.yaml_**                 | [è‹±æ–‡å…·ä½“æ€§è¯å…¸](https://textdata.cn/blog/jcr_concreteness_computation/)                                                                                          | English | word & concreateness score                                                                                                        |
| **_zh_common_LoughranMcDonald.yaml_**              | ä¸­æ–‡ LoughranMcDonald è¯å…¸                                                                                                                                        | ä¸­æ–‡    | æ­£é¢ã€è´Ÿé¢è¯                                                                                                                      |
| **_zh_common_Digitalization.yaml_**                | [ç®¡ç†ä¸–ç•Œ\|å´é(2021)æ•°å­—åŒ–è¯å…¸](https://textdata.cn/blog/2022-11-03-mda-measure-digitalization/)                                                                 | ä¸­æ–‡    | å«äººå·¥æ™ºèƒ½æŠ€æœ¯ã€å¤§æ•°æ®æŠ€æœ¯ã€äº‘è®¡ç®—æŠ€æœ¯ã€åŒºå—é“¾æŠ€æœ¯ã€æ•°å­—æŠ€æœ¯åº”ç”¨ç­‰å…³é”®è¯åˆ—è¡¨ã€‚                                                    |
| **_en_common_LoughranMcDonald.yaml_**              | è‹±æ–‡ LoughranMcDonald è¯å…¸                                                                                                                                        | è‹±æ–‡    | é‡‘è LM æƒ…ç»ªè¯å…¸ 2018 å¹´ç‰ˆæœ¬ï¼Œå«ä¸ƒä¸ªè¯è¡¨ï¼Œåˆ†åˆ«æ˜¯ Negative, Positive, Uncertainty, Litigious, StrongModal, WeakModal, Constraining |
| **_zh_common_FLS.yaml_**                           | [**ä¸šç»©è¯´æ˜ä¼šå‰ç»æ€§è¯å…¸é›†**](https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/)                       | ä¸­æ–‡    | å« 174 ä¸ªè¯è¯­                                                                                                                     |
| **_zh_common_RhetoricalNationalism.yaml_**         | ä¿®è¾æ°‘æ—ä¸»ä¹‰                                                                                                                                                      | ä¸­æ–‡    | å«å››ä¸ªç»´åº¦ï¼Œæ°‘æ—è‡ªè±ªæ„Ÿã€æ°‘æ—å¤å…´ã€ä¼ä¸šè§’è‰²ã€æ’å¤–ä¸»ä¹‰ï¼Œæ¯ä¸ªç»´åº¦ 100 ä¸ªè¯ã€‚                                                         |

<br>

### 1.3 read_dict_yaml()

ä½¿ç”¨ cntext è¯»å– **_.yaml_** è¯å…¸æ–‡ä»¶ï¼› è¿”å›çš„ä¿¡æ¯åŒ…æ‹¬

- Name è¯å…¸çš„åå­—
- Desc è¯å…¸çš„å«ä¹‰ã€æ¦‚å¿µè§£é‡Š
- Refer è¯å…¸æ–‡çŒ®å‡ºå¤„
- Category è¯å…¸ Dictionary çš„å…³é”®è¯
- Dictionary è¯å…¸, python å­—å…¸æ ¼å¼

```python
import cntext as ct
print(ct.read_yaml_dict('zh_common_Digitalization.yaml'))
```

Run

```
{'Name': 'ä¸­æ–‡æ•°å­—åŒ–è¯å…¸',
'Desc': 'åŸºäºè¿™ç¯‡è®ºæ–‡ï¼Œæ„å»ºäº†ä¸­æ–‡æ•°å­—åŒ–è¯å…¸ï¼Œå«äººå·¥æ™ºèƒ½æŠ€æœ¯ã€å¤§æ•°æ®æŠ€æœ¯ã€äº‘è®¡ç®—æŠ€æœ¯ã€åŒºå—é“¾æŠ€æœ¯ã€æ•°å­—æŠ€æœ¯åº”ç”¨ç­‰å…³é”®è¯åˆ—è¡¨ã€‚ ', 'Refer': 'å´é,èƒ¡æ…§èŠ·,æ—æ…§å¦,ä»»æ™“æ€¡. ä¼ä¸šæ•°å­—åŒ–è½¬å‹ä¸èµ„æœ¬å¸‚åœºè¡¨ç°â€”â€”æ¥è‡ªè‚¡ç¥¨æµåŠ¨æ€§çš„ç»éªŒè¯æ®[J]. ç®¡ç†ä¸–ç•Œ,2021,37(07):130-144+10.',
'Category': ['Artificial_Intelligence', 'Big_Data', 'Cloud_Computing', 'Block_Chains', 'Usage_of_Digitalization'],

'Dictionary':
    {'Artificial_Intelligence': ['äººå·¥æ™ºèƒ½', 'å•†ä¸šæ™ºèƒ½', 'å›¾åƒç†è§£', 'æŠ•èµ„å†³ç­–è¾…åŠ©ç³»ç»Ÿ', 'æ™ºèƒ½æ•°æ®åˆ†æ', 'æ™ºèƒ½æœºå™¨äºº', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'è¯­ä¹‰æœç´¢', 'ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯', 'äººè„¸è¯†åˆ«', 'è¯­éŸ³è¯†åˆ«', 'èº«ä»½éªŒè¯', 'è‡ªåŠ¨é©¾é©¶', 'è‡ªç„¶è¯­è¨€å¤„ç†'],
    'Big_Data': ['å¤§æ•°æ®', 'æ•°æ®æŒ–æ˜', 'æ–‡æœ¬æŒ–æ˜', 'æ•°æ®å¯è§†åŒ–', 'å¼‚æ„æ•°æ®', 'å¾ä¿¡', 'å¢å¼ºç°å®', 'æ··åˆç°å®', 'è™šæ‹Ÿç°å®'],
    'Cloud_Computing': ['äº‘è®¡ç®—', 'æµè®¡ç®—', 'å›¾è®¡ç®—', 'å†…å­˜è®¡ç®—', 'å¤šæ–¹å®‰å…¨è®¡ç®—', 'ç±»è„‘è®¡ç®—', 'ç»¿è‰²è®¡ç®—', 'è®¤çŸ¥è®¡ç®—', 'èåˆæ¶æ„', 'äº¿çº§å¹¶å‘', 'EBçº§å­˜å‚¨', 'ç‰©è”ç½‘', 'ä¿¡æ¯ç‰©ç†ç³»ç»Ÿ'],
    'Block_Chains': ['åŒºå—é“¾', 'æ•°å­—è´§å¸', 'åˆ†å¸ƒå¼è®¡ç®—', 'å·®åˆ†éšç§æŠ€æœ¯', 'æ™ºèƒ½é‡‘èåˆçº¦'],
    'Usage_of_Digitalization': ['ç§»åŠ¨äº’è”ç½‘', 'å·¥ä¸šäº’è”ç½‘', 'ç§»åŠ¨äº’è”', 'äº’è”ç½‘åŒ»ç–—', 'ç”µå­å•†åŠ¡', 'ç§»åŠ¨æ”¯ä»˜', 'ç¬¬ä¸‰æ–¹æ”¯ä»˜', 'NFCæ”¯ä»˜', 'æ™ºèƒ½èƒ½æº', 'B2B', 'B2C', 'C2B', 'C2C', 'O2O', 'ç½‘è”', 'æ™ºèƒ½ç©¿æˆ´', 'æ™ºæ…§å†œä¸š', 'æ™ºèƒ½äº¤é€š', 'æ™ºèƒ½åŒ»ç–—', 'æ™ºèƒ½å®¢æœ', 'æ™ºèƒ½å®¶å±…', 'æ™ºèƒ½æŠ•é¡¾', 'æ™ºèƒ½æ–‡æ—…', 'æ™ºèƒ½ç¯ä¿', 'æ™ºèƒ½ç”µç½‘', 'æ™ºèƒ½è¥é”€', 'æ•°å­—è¥é”€', 'æ— äººé›¶å”®', 'äº’è”ç½‘é‡‘è', 'æ•°å­—é‡‘è', 'Fintech', 'é‡‘èç§‘æŠ€', 'é‡åŒ–é‡‘è', 'å¼€æ”¾é“¶è¡Œ']}}
```

<br>

### 1.4 detect_encoding()

```
ct.detect_encoding(file)
```

é€šè¿‡è¯»å–å‰ num_lines æ¥è¯†åˆ« txt/csv æ–‡ä»¶çš„ç¼–ç æ ¼å¼

- **_file_** æ–‡ä»¶è·¯å¾„

```python
import cntext as ct

#è¯»å–dataæ–‡ä»¶å¤¹ä¸‹çš„ã€ä¸‰ä½“.txtã€‘
#è¯†åˆ«ç¼–ç æ–¹å¼
ct.detect_encoding(file='data/ä¸‰ä½“.txt')
```

Run

```
utf-8
```

<br>

### 1.5 get_files(fformat)

- **fformat** fformat æ ¼å¼æ”¯æŒ txt/pdf/docx/xlsx/csv ç­‰ã€‚ `*`è¡¨ç¤ºé€šé…ç¬¦

æŸ¥çœ‹ç¬¦åˆ fformat è·¯å¾„è§„åˆ™çš„æ‰€æœ‰çš„æ–‡ä»¶ï¼Œ fformat æ ¼å¼æ”¯æŒ txt/pdf/docx/xlsx/csv ç­‰ã€‚ `*`è¡¨ç¤ºé€šé…ç¬¦

| fformat æ ¼å¼ | è¯†åˆ«çš„æ–‡ä»¶                       |
| ------------ | -------------------------------- |
| `*.txt`      | åŒ¹é…å½“å‰ä»£ç æ‰€åœ¨è·¯å¾„å†…çš„æ‰€æœ‰ txt |
| `*.pdf`      | åŒ¹é…å½“å‰ä»£ç æ‰€åœ¨è·¯å¾„å†…çš„æ‰€æœ‰ pdf |
| `data/*.txt` | åŒ¹é…ã€Œæ–‡ä»¶å¤¹ dataã€å†…æ‰€æœ‰çš„ txt  |
| <br>         |                                  |

```python
#æŸ¥çœ‹ã€æ–‡ä»¶å¤¹dataã€‘å†…æ‰€æœ‰çš„ txtæ–‡ä»¶ã€‚
ct.get_files(fformat='data/*.txt')
```

Run

```
['data/ä¸‰ä½“.txt',
 'data/santi.txt',
 'data/w2v_corpus.txt',
 'data/sopmi_corpus.txt',
 'data/brown_corpus.txt',
 'data/sopmi_seed_words.txt']
```

<br>

### 1.6 read_pdf

è¯»å– PDFï¼Œè¿”å›æ–‡æœ¬å†…å®¹

```python
ct.read_pdf(file)
```

- **_file_** PDF æ–‡ä»¶è·¯å¾„

ç‚¹å‡» [**æ ¼åŠ›ç”µå™¨ 2023.pdf**](https://textdata.cn/data/æ ¼åŠ›ç”µå™¨2023.pdf)

```python
import cntext as ct

text = ct.read_pdf('æ ¼åŠ›ç”µå™¨2023.pdf')
print(text)
```

Run

```
ç æµ·æ ¼åŠ›ç”µå™¨è‚¡ä»½æœ‰é™å…¬å¸ 2023å¹´å¹´åº¦æŠ¥å‘Šå…¨æ–‡
ç æµ·æ ¼åŠ›ç”µå™¨è‚¡ä»½æœ‰é™å…¬å¸
2023å¹´å¹´åº¦æŠ¥å‘Š


äºŒã€‡äºŒå››å¹´å››æœˆ
ç æµ·æ ¼åŠ›ç”µå™¨è‚¡ä»½æœ‰é™å…¬å¸ 2023å¹´å¹´åº¦æŠ¥å‘Šå…¨æ–‡
 ç¬¬ 2 é¡µ å…± 249 é¡µ ç¬¬ä¸€èŠ‚ é‡è¦æç¤ºã€ç›®å½•å’Œé‡Šä¹‰
å…¬å¸è‘£äº‹ä¼šã€ç›‘äº‹ä¼šåŠè‘£äº‹ã€ç›‘äº‹ã€é«˜çº§ç®¡ç†äººå‘˜ä¿è¯å¹´åº¦æŠ¥å‘Šå†…å®¹
çš„çœŸå®ã€å‡†ç¡®ã€å®Œæ•´ï¼Œä¸å­˜åœ¨è™šå‡è®°è½½ã€è¯¯å¯¼æ€§é™ˆè¿°æˆ–é‡å¤§é—æ¼ï¼Œå¹¶æ‰¿æ‹…
ä¸ªåˆ«å’Œè¿å¸¦çš„æ³•å¾‹
......
```

<br>

### 1.7 read_docx

è¯»å– docxï¼Œè¿”å›æ–‡æœ¬å†…å®¹

```python
ct.read_docx(file)
```

- **_file_** docx æ–‡ä»¶è·¯å¾„

```python
import cntext as ct

text = ct.read_docx('test.docx')
text
```

Run

```
è¿™æ˜¯æ¥è‡ªtest.docxé‡Œå†…å®¹
```

<br>

### 1.8 read_file()

```
ct.read_file(file, encoding='utf-8')
```

- **file** å¾…è¯»å–çš„æ–‡ä»¶è·¯å¾„ï¼› æ”¯æŒ txtã€pdfã€docxã€xlsxã€xlsï¼Œ è¿”å› DataFrame(å« doc å’Œ file ä¸¤ä¸ªå­—æ®µ)ã€‚
- **encoding** å¾…è¯»å–æ–‡ä»¶çš„ç¼–ç æ–¹å¼

ä»¥ `data/ä¸‰ä½“.txt` ä¸ºä¾‹

```python
import cntext as ct

#é»˜è®¤encoding='utf-8'
#sdf = ct.read_file(file='data/ä¸‰ä½“.txt')

sdf = ct.read_file(file='data/ä¸‰ä½“.txt', encoding='utf-8')
sdf
```

![](img/01-san_ti_df.png)

<br>

### 1.9 read_files()

```
ct.read_files(fformat, encoding='utf-8'ï¼‰
```

æ‰¹é‡è¯»å–ç¬¦åˆ fformat æ ¼å¼çš„æ‰€æœ‰æ–‡ä»¶æ•°æ®ï¼Œè¿”å› DataFrame(å« doc å’Œ file ä¸¤ä¸ªå­—æ®µ)ã€‚

è¯»å–[æ–‡ä»¶å¤¹ data é‡Œæ‰€æœ‰ txt]

```python
import cntext as ct

#é»˜è®¤encoding='utf-8'
#ddf = ct.read_files(fformat='data/*.txt')

ddf = ct.read_files(fformat='data/*.txt', encoding='utf-8')
ddf
```

![](img/02-ddf.png)

<br>

### 1.10 extract_mda

æå– A è‚¡å¹´æŠ¥ä¸­çš„ MD&A æ–‡æœ¬å†…å®¹ã€‚å¦‚æœè¿”å›'',åˆ™æå–å¤±è´¥ã€‚

```
ct.extract_mda(text, kws_pattern='')
```

- text ä¸­å›½ A è‚¡å¹´æŠ¥åŸå§‹æ–‡æœ¬
- kws_pattern ç®¡ç†å±‚è®¨è®ºä¸åˆ†æç« èŠ‚è¯†åˆ«å…³é”®è¯çš„æ¨¡æ¿ã€‚cntext å†…ç½®çš„ kws_pattern å†…å®¹å¦‚ä¸‹

```
kws_pattern = 'è‘£äº‹ä¼šæŠ¥å‘Š|è‘£äº‹ä¼šæŠ¥å‘Šä¸ç®¡ç†è®¨è®º|ä¼ä¸šè¿è¥ä¸ç®¡ç†è¯„è¿°|ç»è¥æ€»ç»“ä¸åˆ†æ|ç®¡ç†å±‚è¯„ä¼°ä¸æœªæ¥å±•æœ›|è‘£äº‹å±€æŠ¥å‘Š|ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ|ç»è¥æƒ…å†µè®¨è®ºä¸åˆ†æ|ç»è¥ä¸šç»©åˆ†æ|ä¸šåŠ¡å›é¡¾ä¸å±•æœ›|å…¬å¸ç»è¥åˆ†æ|ç®¡ç†å±‚è¯„è®ºä¸åˆ†æ|æ‰§è¡Œæ‘˜è¦ä¸ä¸šåŠ¡å›é¡¾|ä¸šåŠ¡è¿è¥åˆ†æ'
```

<br>

```python
import cntext as ct

text = ct.read_pdf('æ ¼åŠ›ç”µå™¨2023.pdf')
mda_text = ct.extract_mda(text)
print(mda_text)
```

Run

```
'ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ  \nä¸€ã€æŠ¥å‘ŠæœŸå†…å…¬å¸æ‰€å¤„è¡Œä¸šæƒ…å†µ  \nï¼ˆä¸€ï¼‰è¡Œä¸šå‘å±•ç°çŠ¶  \n1.æ¶ˆè´¹é¢†åŸŸ â€”â€”å®¶ç”µè¡Œä¸šç¨³å®šå¢é•¿ï¼Œç©ºè°ƒå¸‚åœºæ¢å¤æ˜æ˜¾  \n2023å¹´ï¼Œä¸­å›½ç»æµä¿æŒäº†æ•´ä½“æ¢å¤å‘å¥½çš„æ€åŠ¿ï¼Œæ¿€å‘æ¶ˆè´¹æ˜¯ç¨³å¢é•¿çš„é‡ä¸­ä¹‹é‡ã€‚å›½å®¶é¼“åŠ±å’Œæ¨åŠ¨æ¶ˆè´¹å“ä»¥æ—§æ¢\næ–°ï¼Œä¿ƒè¿›æ¶ˆè´¹ç»æµå¤§å¾ªç¯ï¼ŒåŠ é€Ÿæ›´æ–°éœ€æ±‚é‡Šæ”¾ï¼Œæ¨åŠ¨é«˜èƒ½æ•ˆäº§å“è®¾å¤‡é”€å”®å’Œå‡ºå£å¢é•¿ï¼Œè¿›ä¸€æ­¥æ¿€å‘ç»¿è‰²æ¶ˆè´¹æ½œåŠ›ã€‚  \n1ï¼‰å®¶ç”µè¡Œä¸šç¨³å®šå¢é•¿  \n2023å¹´ï¼Œå›½å†…ç»æµæ¢å¤æ˜æ˜¾ï¼Œå®¶ç”µè¡Œä¸šç¨³å®šå¢é•¿ã€‚æ ¹æ®å…¨å›½å®¶ç”¨ç”µå™¨å·¥ä¸šä¿¡æ¯ä¸­å¿ƒå‘å¸ƒçš„ã€Š 2023å¹´ä¸­å›½å®¶ç”µ\nè¡Œä¸šå¹´åº¦æŠ¥å‘Šã€‹ï¼Œå®¶ç”µè¡Œä¸šå¤–é”€æ˜æ˜¾å¢é•¿ï¼Œå‡ºå£è§„æ¨¡ä¸º 6,174äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿ 9.9%ï¼›å›½å†…å¸‚åœºå®ç°ç¨³æ­¥å¢é•¿ï¼Œé”€å”®\nè§„æ¨¡ä¸º7'
.......
.......
```

<br>

ä»¥[2001 å¹´~2023 ä¼šè®¡å¹´åº¦æŠ¥å‘Šæ•°æ®é›†](https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/)ä¸ºä¾‹ï¼Œ æŸ¥çœ‹ **_extract_mda_** çš„æŠ½å– mda çš„èƒ½åŠ›ã€‚

```python
import glob
import cntext as ct

print('extract_mdaè¯†åˆ«èƒ½åŠ›')
for year in range(2001, 2024):
    num = 0
    for file in glob.glob(f'å¹´æŠ¥txt/{year}/*.txt'):
        mda_text = ct.extract_mda(open(file).read())
        if mda_text!='':
            num = num + 1

    volume = len(glob.glob(f'å¹´æŠ¥txt/{year}/*.txt'))
    ratio = num/volume

    print(f'{year}: {ratio:.2f}')
```

Run

```
2001: 0.24
2002: 0.37
2003: 0.43
2004: 0.70
2005: 0.77
2006: 0.78
2007: 0.79
2008: 0.77
2009: 0.79
2010: 0.82
2011: 0.84
2012: 0.96
2013: 0.95
2014: 0.98
2015: 0.98
2016: 0.99
2017: 0.98
2018: 0.98
2019: 0.99
2020: 0.97
2021: 0.98
2022: 0.99
2023: 0.99
```

å»ºè®®å„ä½ç”¨æœ€è¿‘ 10 å¹´çš„å¹´æŠ¥æ•°æ®ï¼Œé€šè¿‡ extract_mda æå– mda æ–‡æœ¬ï¼Œæˆ–è€…ç›´æ¥è´­ä¹° [æ•°æ®é›† | 2001-2023 å¹´ A è‚¡ä¸Šå¸‚å…¬å¸å¹´æŠ¥&ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ](æ•°æ®é›† | 2001-2023 å¹´ A è‚¡ä¸Šå¸‚å…¬å¸å¹´æŠ¥&ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ)

<br>

### 1.11 traditional2simple()

ç¹ä½“è½¬ç®€ä½“

```
ct.traditional2simple(text, mode='t2s')
```

- **_text_** å¾…è½¬æ¢çš„æ–‡æœ¬
- **_mode_** è½¬æ¢æ¨¡å¼ï¼Œ é»˜è®¤ mode='t2s'ç¹è½¬ç®€; mode è¿˜æ”¯æŒ s2t

 <br>

```python
import cntext as ct

text = 'ç°¡é«”æ¼¢å­—'
ct.traditional2simple(text)
```

Run

```
'ç®€ä½“æ±‰å­—'
```

<br>

```python
text = 'ç®€ä½“æ±‰å­—'
ct.traditional2simple(text, mode='s2t')
```

Run

```
'ç°¡é«”æ¼¢å­—'
```

<br>

### 1.12 fix_text()

å°†ä¸æ­£å¸¸çš„ã€æ··ä¹±ç¼–ç çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ­£å¸¸çš„æ–‡æœ¬ã€‚ä¾‹å¦‚å…¨è§’è½¬åŠè§’

```python
import cntext as ct

raw_text = 'ä»Šæ—¥èµ·å¯ä¸­é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œå¯ä»¥æ‹¨æ‰“ç”µè¯ï¼ï¼“ï¼—ï¼‘ï¼ï¼–ï¼–ï¼“ï¼’ï¼‘ï¼™ï¼™ï¼‘ã€ï¼–ï¼–ï¼“ï¼’ï¼‘ï¼™ï¼—ï¼“å’¨è¯¢ã€‚'

text = ct.fix_text(raw_text)
text
```

Run

```
ä»Šæ—¥èµ·å¯ä¸­é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œå¯ä»¥æ‹¨æ‰“ç”µè¯0371-66321991ã€66321973å’¨è¯¢ã€‚
```

<br>

### 1.13 fix_contractions(text)

å°†è‹±æ–‡ç¼©å†™(å«ä¿šè¯­è¡¨è¾¾)è½¬åŒ–ä¸ºå®Œæ•´çš„è¡¨è¾¾ï¼Œå¦‚å¦‚

```
- you're -> you are
- yall  -> you all
- gotta  -> got to
...
```

<br>

```python
import cntext as ct

raw_text = "yall're happy now"

text = ct.fix_contractions(raw_text)
text
```

Run

```
"you all are happy now"
```

<br>


### 1.14 clean_text(text)

```python
ct.clean_text(text, lang='chinese')
```

- **_text_** å¾…å¤„ç†çš„æ–‡æœ¬
- **_lang_** è¯­è¨€ç±»å‹ï¼Œ é»˜è®¤ lang='chinese', æ”¯æŒ"english"ã€"chinese"

```python
import cntext as ct

chinese_text = ("ä»Šå¤©çš„è®­ç»ƒå¾ˆæ£’ï¼è·‘äº†5.6å…¬é‡Œï¼Œå¿ƒç‡ç¨³å®šã€‚"
                "æŸ¥çœ‹ https://example.com/data ğŸ˜Š #å¥èº«æ‰“å¡")

print(">>> ä¸­æ–‡æ¸…æ´—")
print("åŸå§‹:", repr(chinese_text))
print("æ¸…æ´—:", repr(ct.clean_text(chinese_text, lang="chinese")))
print()

    # è‹±æ–‡æµ‹è¯•
english_text = ("Great workout today! Ran 5.6 miles, HR stable. "
                "Check https://example.com/data ğŸ˜Š #Fitness")
print(">>> è‹±æ–‡æ¸…æ´—")
print("åŸå§‹:", repr(english_text))
print("æ¸…æ´—:", repr(ct.clean_text(english_text, lang="english")))
```
Run
```
>>> ä¸­æ–‡æ¸…æ´—
åŸå§‹: 'ä»Šå¤©çš„è®­ç»ƒå¾ˆæ£’ï¼è·‘äº†5.6å…¬é‡Œï¼Œå¿ƒç‡ç¨³å®šã€‚æŸ¥çœ‹ https://example.com/data ğŸ˜Š #å¥èº«æ‰“å¡'
æ¸…æ´—: 'ä»Šå¤©çš„è®­ç»ƒå¾ˆæ£’ï¼è·‘äº†æ•°å­—å…¬é‡Œï¼Œå¿ƒç‡ç¨³å®šã€‚æŸ¥çœ‹   å¥èº«æ‰“å¡'

>>> è‹±æ–‡æ¸…æ´—
åŸå§‹: 'Great workout today! Ran 5.6 miles, HR stable. Check https://example.com/data ğŸ˜Š #Fitness'
æ¸…æ´—: 'great workout today! ran NUMBER miles, hr stable. check  ğŸ˜Š #fitness'
```

<br><br>

## äºŒã€Stats æ¨¡å—

| æ¨¡å—        | å‡½æ•°                                                               | åŠŸèƒ½                                                           |
| ----------- | ------------------------------------------------------------------ | -------------------------------------------------------------- |
| **stats** | `ct.word_count(text, lang='chinese')`                              | è¯é¢‘ç»Ÿè®¡                                                       |
| **stats** | `ct.readability(text, lang='chinese')`                             | æ–‡æœ¬å¯è¯»æ€§                                                     |
| **stats** | **_ct.sentiment(text, diction, lang='chinese')_**                  | æ— (ç­‰)æƒé‡è¯å…¸çš„æƒ…æ„Ÿåˆ†æ                                       |
| **stats** | `ct.sentiment_by_valence(text, diction, lang='chinese')`           | å¸¦æƒé‡çš„è¯å…¸çš„æƒ…æ„Ÿåˆ†æ                                         |
| **stats** | **_ct.word_in_context(text, keywords, window=3, lang='chinese')_** | åœ¨ text ä¸­æŸ¥æ‰¾ keywords å‡ºç°çš„ä¸Šä¸‹æ–‡å†…å®¹(çª—å£ window)ï¼Œè¿”å› df |
| **stats** | **_ct.epu(text, e_pattern, p_pattern, u_pattern)_**                | ä½¿ç”¨æ–°é—»æ–‡æœ¬æ•°æ®è®¡ç®—ç»æµæ”¿ç­–ä¸ç¡®å®šæ€§ EPUï¼Œè¿”å› df              |
| **stats** | **_ct.fepu(text, ep_pattern='', u_pattern='')_**                   | ä½¿ç”¨ md&a æ–‡æœ¬æ•°æ®è®¡ç®—ä¼ä¸šä¸ç¡®å®šæ€§æ„ŸçŸ¥ FEPU                    |
| **stats** | **_ct.semantic_brand_score(text, brands, lang='chinese')_**        | è¡¡é‡å“ç‰Œï¼ˆä¸ªä½“ã€å…¬å¸ã€å“ç‰Œã€å…³é”®è¯ç­‰ï¼‰çš„é‡è¦æ€§                 |
| **stats** | **_ct.cosine_sim(text1, text2, lang='chinese')_**                  | ä½™å¼¦ç›¸ä¼¼åº¦                                                     |
| **stats** | `ct.jaccard_sim(text1, text2, lang='chinese')`                     | Jaccard ç›¸ä¼¼åº¦                                                 |
| **stats** | `ct.minedit_sim(text1, text2, lang='chinese')`                     | æœ€å°ç¼–è¾‘è·ç¦»                                                   |
| **stats** | `ct.word_hhi(text)`                                                | æ–‡æœ¬çš„èµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°                                      |

<br>

### 2.1 word_count()

ç»Ÿè®¡è¯é¢‘ï¼Œ è¿”å› Counter(ç±»ä¼¼äº python å­—å…¸) ï¼› æ”¯æŒä¸­è‹±æ–‡

```
ct.word_count(text, lang='chinese', return_df=False)
```

- **text** å¾…åˆ†æçš„æ–‡æœ¬å­—ç¬¦ä¸²
- **lang** æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼Œ ä¸­æ–‡ chineseã€è‹±æ–‡ englishï¼Œé»˜è®¤ä¸­æ–‡ã€‚
- **return_df** è¿”å›ç»“æœæ˜¯å¦ä¸º dataframeï¼Œé»˜è®¤ False

```python
import cntext as ct

text = 'è‡´åŠ›äºè‡´åŠ›äºä»¥é›¶æ–‡ç« å¤„ç†è´¹æˆ–è®¢é˜…è´¹å‘å¸ƒä¼˜è´¨ç ”ç©¶è½¯ä»¶ã€‚'

#ct.word_count(text, lang='chinese')
ct.word_count(text)
```

Run

```
Counter({'è‡´åŠ›äº': 2,
         'æ–‡ç« ': 1,
         'å¤„ç†è´¹': 1,
         'è®¢é˜…è´¹': 1,
         'å‘å¸ƒ': 1,
         'ä¼˜è´¨': 1,
         'ç ”ç©¶': 1,
         'è½¯ä»¶': 1})
```

<br>

```python
ct.word_count(text, return_df=True)
```

![](img/09-term_freq.png)

<br>

### 2.2 readability()

```
ct.readability(text, lang='chinese', syllables=3, return_series=False)
```

è®¡ç®—æ–‡æœ¬å¯è¯»æ€§å¸¸è§æŒ‡æ ‡ï¼› å« Gunning Fog Indexã€ SMOG Indexã€Coleman Liau Indexã€ Automated Readability Index(ARI)ã€Readability Index(Rix)ï¼› æŒ‡æ ‡è¶Šå¤§ï¼Œå¤æ‚åº¦è¶Šé«˜ï¼Œæ–‡æœ¬çš„å¯è¯»æ€§è¶Šå·®ã€‚

- **text** å¾…åˆ†æçš„æ–‡æœ¬å­—ç¬¦ä¸²
- **lang** æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼Œ ä¸­æ–‡ chineseã€è‹±æ–‡ englishï¼Œé»˜è®¤ä¸­æ–‡ã€‚
- **syllables** éŸ³èŠ‚æ•°(æ±‰å­—æ•°)å¤§äºç­‰äº syllables ä¸ºå¤æ‚è¯. é»˜è®¤å€¼ä¸º 3
- **return_series**: è®¡ç®—ç»“æœæ˜¯å¦è¾“å‡ºä¸º pd.Series ç±»å‹ï¼Œé»˜è®¤ä¸º False

<br>

```
Gunning Fog Index = 0.4 * (Total_Words/Total_Sentences + 100 * Complex_Words/Total_Words)
SMOG Index = 1.0430 * sqrt(Complex_Words/Total_Sentences) * 30 + 3.1291
Coleman-Liau Index = 0.0588 * (100*Total_Letters/Total_Words) -0.296*(100*Total_Sentences/Total_Words) - 15.8
Automated Readability Index(ARI) = 4.71 * (Total_Characters/Total_Words) + 0.5*(Total_Words/Total_Sentences) - 21.43
Readability Index(RIX) = Complex_Words * (6 + Total_characters) / Total_Sentences
```

<br>

```python
import cntext as ct

text = 'è‡´åŠ›äºä»¥é›¶æ–‡ç« å¤„ç†è´¹æˆ–è®¢é˜…è´¹å‘å¸ƒä¼˜è´¨ç ”ç©¶è½¯ä»¶ã€‚'

ct.readability(text, lang='chinese', syllables=3)
```

Run

```
{'fog_index': 120.4,
 'flesch_kincaid_grade_level': 20.2,
 'smog_index': 57.32,
 'coleman_liau_index': 83.96,
 'ari': 87.4,
 'rix': 87.0}
```

<br>

### 2.3 sentiment(text, diction, lang)

å¸¸è§çš„æƒ…æ„Ÿåˆ†æé»˜è®¤æƒ…ç»ªè¯æ— (ç­‰)æƒé‡ï¼Œ é€šè¿‡ç»Ÿè®¡è¯è¯­ä¸ªæ•°æ¥ååº”æƒ…æ„Ÿä¿¡æ¯ã€‚

```
sentiment(text, diction, lang='chinese', return_series=False)
```

- **text** å¾…åˆ†æçš„æ–‡æœ¬å­—ç¬¦ä¸²
- **diction** æ ¼å¼ä¸º Python å­—å…¸ç±»å‹ã€‚å½¢å¦‚ä¸‹é¢çš„æ¡ˆä¾‹
- **lang** æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼Œ ä¸­æ–‡ chineseã€è‹±æ–‡ englishï¼Œé»˜è®¤ä¸­æ–‡ã€‚
- **return_series** è®¡ç®—ç»“æœæ˜¯å¦è¾“å‡ºä¸º pd.Series ç±»å‹ï¼Œé»˜è®¤ä¸º False

```python
import cntext as ct

diction = {'pos': ['é«˜å…´', 'å¿«ä¹', 'åˆ†äº«'],
           'neg': ['éš¾è¿‡', 'æ‚²ä¼¤'],
           'adv': ['å¾ˆ', 'ç‰¹åˆ«']}

text = 'æˆ‘ä»Šå¤©å¾—å¥–äº†ï¼Œå¾ˆé«˜å…´ï¼Œæˆ‘è¦å°†å¿«ä¹åˆ†äº«å¤§å®¶ã€‚'
ct.sentiment(text=text,
             diction=diction,
             lang='chinese')
```

Run

```
{'pos_num': 3,
 'neg_num': 0,
 'adv_num': 1,
 'stopword_num': 8,
 'word_num': 14,
 'sentence_num': 1}
```

<br>

### 2.4 sentiment_by_valence()

```
ct.sentiment_by_valence(text, diction, lang='chinese', return_series=False)
```

- **text** å¾…åˆ†æçš„æ–‡æœ¬å­—ç¬¦ä¸²
- **diction** æ ¼å¼ä¸º Python å­—å…¸ç±»å‹ã€‚å½¢å¦‚ä¸‹é¢çš„æ¡ˆä¾‹
- **lang** æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼Œ ä¸­æ–‡ chineseã€è‹±æ–‡ englishï¼Œé»˜è®¤ä¸­æ–‡ã€‚
- **return_series** è®¡ç®—ç»“æœæ˜¯å¦è¾“å‡ºä¸º pd.Series ç±»å‹ï¼Œé»˜è®¤ä¸º False

å¸¸è§çš„æƒ…æ„Ÿåˆ†ææ˜¯æ— (ç­‰)æƒé‡, ä½†å®é™…ä¸Šä¸åŒçš„è¯è¯­æ‰€æºå¸¦çš„æƒ…æ„Ÿä¿¡æ¯çš„å¼ºåº¦å·®å¼‚æ˜¯å¾ˆå¤§çš„ã€‚æ®æ­¤å­¦è€…ä»¬å¼€å‘å‡ºå¾ˆå¤šå¸¦æƒé‡çš„è¯å…¸ï¼Œä¾‹å¦‚

- è‹±æ–‡å…·ä½“æ€§è¯å…¸ en_valence_Concreteness.yamlï¼Œ è¯å…¸ä¸­æ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ª concreteness å€¼
- ä¸­æ–‡å…­ç»´åº¦è¯­ä¹‰è¯å…¸ zh_valence_SixSemanticDimensionDatabase.yaml, æ¯ä¸ªä¸­æ–‡è¯æœ‰å…­ä¸ªå€¼ã€‚

ä»¥å…·ä½“æ€§ä¸ºä¾‹ï¼Œ **è¯­è¨€å…·ä½“æ€§ Concreteness**æè¿°äº†ä¸€ä¸ªè¯åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ˜¯æŒ‡ä¸€ä¸ªå®é™…çš„ã€æœ‰å½¢çš„æˆ–â€œçœŸå®çš„â€å®ä½“ï¼Œä»¥ä¸€ç§æ›´å…·ä½“ã€æ›´ç†Ÿæ‚‰ã€æ›´å®¹æ˜“è¢«çœ¼ç›æˆ–å¿ƒçµæ„ŸçŸ¥çš„æ–¹å¼æè¿°å¯¹è±¡å’Œè¡Œä¸ºï¼ˆå³ï¼Œå¯æƒ³è±¡æˆ–ç”ŸåŠ¨ï¼›Brysbaert, Warriner, and Kuperman 2014; Semin and Fiedler 1988)

```python
import cntext as ct
import pandas as pd

concreteness_dict = ct.read_yaml_dict('en_valence_Concreteness.yaml')['Dictionary']
concreteness_dict
```

Run

```
{'roadsweeper': {'concreteness': 4.85},
 'traindriver': {'concreteness': 4.54},
 'tush': {'concreteness': 4.45},
 'hairdress': {'concreteness': 3.93},
 'pharmaceutics': {'concreteness': 3.77},
 'hoover': {'concreteness': 3.76},
 'shopkeeping': {'concreteness': 3.18},
 'pushiness': {'concreteness': 2.48},
 ......
 }
```

å¯èƒ½ **_concreteness_dict_**ä¸å¤Ÿç›´è§‚ï¼Œ å¦‚æœæ•´ç†è½¬åŒ–ä¸€ä¸‹å¤§æ¦‚ç±»ä¼¼äº

![](img/11-concreteness_df.png)

[**JCR2021 | è®¡ç®—æ–‡æœ¬çš„è¯­è¨€å…·ä½“æ€§**](https://textdata.cn/blog/jcr_concreteness_computation/) æ–‡ä¸­æä¾›äº†ä¸€ä¸ªæ¡ˆä¾‹

```python
reply = "I'll go look for that"

score=ct.sentiment_by_valence(text=reply,
                              diction=concreteness_dict,
                              lang='english')

score
```

Run

```
{'concreteness': 9.28,
'word_num': 6}
```

<br>

```python
employee_replys = ["I'll go look for that",
                   "I'll go search for that",
                   "I'll go search for that top",
                   "I'll go search for that t-shirt",
                   "I'll go look for that t-shirt in grey",
                   "I'll go search for that t-shirt in grey"]

for idx, reply in enumerate(employee_replys):
    score=ct.sentiment_by_valence(text=reply,
                                  diction=concreteness_dict,
                                  lang='english')

    template = "Concreteness Score: {score:.2f} | Example-{idx}: {exmaple}"

    print(template.format(score=score['concreteness'],
                          idx=idx,
                          exmaple=reply))
```

Run

```
Concreteness Score: 9.28 | Example-0: I'll go look for that
Concreteness Score: 9.32 | Example-1: I'll go search for that
Concreteness Score: 13.25 | Example-2: I'll go search for that top
Concreteness Score: 14.25 | Example-3: I'll go search for that t-shirt
Concreteness Score: 21.32 | Example-4: I'll go look for that t-shirt in grey
Concreteness Score: 21.36 | Example-5: I'll go search for that t-shirt in grey
```

<br>

### 2.5 word_in_context()

You shall know a word by the company it keeps é€šè¿‡ä¸€ä¸ªå•è¯æ‰€å¤„çš„è¯­å¢ƒï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£è¯¥å•è¯çš„å«ä¹‰ã€‚

åœ¨ text ä¸­æŸ¥æ‰¾ keywords å‡ºç°çš„ä¸Šä¸‹æ–‡å†…å®¹(çª—å£ window)ï¼Œè¿”å› dfã€‚

```
ct.word_in_context(text, keywords, window=3, lang='chinese')
```

- **text** å¾…åˆ†ææ–‡æœ¬
- **keywords** å…³é”®è¯åˆ—è¡¨
- **window** å…³é”®è¯ä¸Šä¸‹æ–‡çª—å£å¤§å°
- **lang** æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼Œ ä¸­æ–‡ chineseã€è‹±æ–‡ englishï¼Œé»˜è®¤ä¸­æ–‡ã€‚

```python
import cntext as ct

#æµ‹è¯•ä»£ç ï¼Œå‡è®¾zh_textæ˜¯å¹´æŠ¥æ–‡æœ¬ï¼Œä»æ‰¾æ‰¾å‡ºä¸ç½‘è¯ç›¸å…³è¯çš„ä¸Šä¸‹æ–‡
zh_text = """
ã€æ’å…¥ä¸€æ¡è‡ªå®¶å¹¿å‘Šã€‘å¤§é‚“è‡ªå·±å®¶çš„å®¶ï¼Œ
å®‰å¹³å¿å¤šéš†ä¸ç½‘åˆ¶å“ï¼Œç”Ÿäº§é”€å”®ä¸é”ˆé’¢è½§èŠ±ç½‘ã€
ç”µç„Šç½‘ã€çŸ³ç¬¼ç½‘ã€åˆ€ç‰‡åˆºç»³ã€å†²å­”ç½‘ç­‰ä¸ç½‘åˆ¶å“ã€‚
è”ç³»äºº é‚“é¢–é™ 0318-7686899

äººç”Ÿè‹¦çŸ­ï¼Œæˆ‘å­¦Python
åœ¨ç¤¾ç§‘ä¸­ï¼Œå¯ä»¥ç”¨Pythonåšæ–‡æœ¬åˆ†æ
Pythonæ˜¯ä¸€é—¨åŠŸèƒ½å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›åº”ç”¨åœ¨ç»ç®¡ç¤¾ç§‘é¢†åŸŸã€‚
å¯ä»¥åšç½‘ç»œçˆ¬è™«ã€æ–‡æœ¬åˆ†æã€LDAè¯é¢˜æ¨¡å‹ã€ç›¸ä¼¼åº¦åˆ†æç­‰ã€‚

ä»Šå¹´ç»æµä¸æ™¯æ°”ï¼Œå½¢åŠ¿å¼‚å¸¸ä¸¥å³»ã€‚
ç”±äºç–«æƒ…ä¸æ™¯æ°”ï¼Œé™é»˜ç®¡ç†ï¼Œ äº§å“ç§¯å‹ï¼Œ å…¬å¸ç»è¥å›°éš¾ã€‚
ä¿å°±ä¸šä¿ƒå°±ä¸šï¼Œä»»åŠ¡ååˆ†è‰°å·¨ã€‚
"""

#ã€pythonã€‘ä¸Šä¸‹æ–‡
ct.word_in_context(text = zh_text,
                   keywords = ['python'],
                   window=10,
                   lang='chinese')
```

![](img/20-word-in-context.png)

<br>

### 2.6 epu()

[**ä»£ç  | ä½¿ç”¨æ–°é—»æ•°æ®æµ‹é‡ç»æµæ”¿ç­–ä¸ç¡®å®šæ€§ EPU**](https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/)

![](img/13-epu-plot.png)

```
epu(df, freq='Y', e_pattern='', p_pattern='', u_pattern='')
```

- **df** æ–°é—»æ•°æ® DataFrameï¼Œ å« text å’Œ date ä¸¤ä¸ªå­—æ®µã€‚ æ¯ä¸€è¡Œä»£è¡¨ä¸€æ¡æ–°é—»è®°å½•
- **freq** å­—ç¬¦ä¸²ï¼› ç¡®å®š EPU æŒ‡æ•°çš„æ—¶é—´é¢—ç²’åº¦ï¼› å¦‚å¹´ Y, æœˆ m, æ—¥ d, é»˜è®¤ freq='Y'
- **e_pattern** å­—ç¬¦ä¸²ï¼›ç»æµç±»è¯å…¸ï¼Œç”¨`|`é—´éš”è¯è¯­ï¼Œå½¢å¦‚ **e_pattern = â€˜ç»æµ|é‡‘èâ€™**
- **p_pattern** å­—ç¬¦ä¸²ï¼›æ”¿ç­–è¯å…¸ï¼Œç”¨`|`é—´éš”è¯è¯­ï¼Œå½¢å¦‚ **p_pattern = â€˜æ”¿ç­–|æ²»ç†|è¡Œæ”¿â€™**
- **u_pattern** å­—ç¬¦ä¸²ï¼›ä¸ç¡®å®šæ€§è¯å…¸ï¼Œç”¨`|`é—´éš”è¯è¯­ï¼Œå½¢å¦‚ **u_pattern = â€˜é£é™©|å±æœº|éš¾ä»¥é¢„æµ‹â€™**

å‡†å¤‡å¦‚ä¸‹å›¾æ ¼å¼çš„æ•°æ® **_news_df_**

![](img/12-news-df.png)

```python
import cntext as ct

#çœç•¥ï¼Œè¯»å–æ•°æ®å¾—åˆ° news_df

epu_df = ct.epu(df=news_df, freq='m')
epu_df
```

![](img/13-epu-df.png)

<br>

### 2.7 fepu()

[ä½¿ç”¨ç®¡ç†å±‚è®¨è®ºä¸åˆ†ææ–‡æœ¬æ•°æ®æµ‹é‡ã€Œä¼ä¸šæ„ŸçŸ¥ä¸ç¡®å®šæ€§ã€(Subjective perception of economic policy uncertainty, FEPU)](https://textdata.cn/blog/2024-04-25-firm-economic-policy-uncertainty/)

![](img/16-fepu-plot.png)

```
ct.fepu(text, ep_pattern, u_pattern)
```

- **_text_** ï¼›æŸæ—¶æœŸ t æŸä¼ä¸š i çš„ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ md&a æ–‡æœ¬
- **_ep_pattern_** å­—ç¬¦ä¸²ï¼›ç»æµæ”¿ç­–ç±»è¯å…¸ï¼Œç”¨`|`é—´éš”è¯è¯­ï¼Œå½¢å¦‚ **ep_pattern = â€˜ç»æµ|é‡‘è|æ”¿ç­–|æ²»ç†|è¡Œæ”¿â€™**
- **_u_pattern_** å­—ç¬¦ä¸²ï¼›ä¸ç¡®å®šæ€§è¯å…¸ï¼Œç”¨`|`é—´éš”è¯è¯­ï¼Œå½¢å¦‚ **u_pattern = â€˜é£é™©|å±æœº|éš¾ä»¥é¢„æµ‹â€™**

å‡†å¤‡å¦‚ä¸‹å›¾æ ¼å¼çš„æ•°æ® **_mda_df_**

![](img/14-mdadf.png)

```python
import cntext as ct

#çœç•¥ï¼Œè¯»å–æ•°æ®å¾—åˆ° mda_df

fepu_df = df['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'].apply(ct.fepu)
res_df = pd.concat([df[['ä¼šè®¡å¹´åº¦', 'è‚¡ç¥¨ä»£ç ']], fepu_df],   axis=1)
res_df
```

![](img/15-fepu.png)

<br>

<br>

### 2.8 semantic_brand_score()

[æ–‡çŒ®&ä»£ç  | ä½¿ç”¨ Python è®¡ç®—è¯­ä¹‰å“ç‰Œè¯„åˆ†(Semantic Brand Score, SBS)](https://textdata.cn/blog/2024-04-12-semantic-brand-score/) ï¼Œ é€šè¿‡ SBS æ¥è¡¡é‡å“ç‰Œï¼ˆä¸ªä½“ã€å…¬å¸ã€å“ç‰Œã€å…³é”®è¯ç­‰ï¼‰çš„é‡è¦æ€§ã€‚

```
ct.semantic_brand_score(text, brands, lang='chinese')
```

- **_text_** å¾…åˆ†ææ–‡æœ¬
- **_brands_** è¯è¯­åˆ—è¡¨ï¼›
- **_lang_** è¯­è¨€ç±»å‹ï¼Œ"chinese"æˆ–"english"ï¼Œé»˜è®¤"chinese"

ä»¥ä¸‰ä½“å°è¯´ä¸ºä¾‹ï¼Œé€šè¿‡æµ‹é‡å“ç‰Œè¯­ä¹‰è¯„åˆ† SBS æ¥åæ˜ å°è¯´è§’è‰²çš„é‡è¦æ€§ã€‚

```python
import cntext as ct

brands = ['æ±ªæ·¼', 'å²å¼º', 'ç½—è¾‘', 'å¶æ–‡æ´', 'ä¼Šæ–‡æ–¯']

#å‡†å¤‡santi_test_text
#å°è¯´ç­‰åˆ†20ä»½ï¼Œ è¯»å–ç¬¬ä¸€ä»½å¾—åˆ°santi_test_text

sbs_df = ct.semantic_brand_score(text=santi_test_text,
                               brands=brands,
                               lang='chinese')
sbs_df
```

![](img/19-1st-sbs.png)

å¦‚æœå°†ä¸‰ä½“å°è¯´åˆ†æˆ 20 ä»½ï¼Œ æ¯ä¸€ä»½éƒ½æµ‹ç®—å‡ºæ¯ä¸ªè§’è‰²çš„ SBSï¼Œç»˜åˆ¶å‡ºæŠ˜çº¿å›¾å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](img/18-sbs-plot.png)

### 2.9 æ–‡æœ¬ç›¸ä¼¼åº¦

```
ct.cosine_sim(text1, text2, lang='chinese')   cosä½™å¼¦ç›¸ä¼¼
ct.jaccard_sim(text1, text2, lang='chinese')  jaccardç›¸ä¼¼
ct.minedit_sim(text1, text2, lang='chinese')  æœ€å°ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦ï¼›
ct.simple_sim(text1, text2, lang='chinese')   æ›´æ”¹å˜åŠ¨ç®—æ³•
```

ç®—æ³•å®ç°å‚è€ƒè‡ª `Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.`

```python
import cntext as ct

text1 = 'ç¼–ç¨‹çœŸå¥½ç©ç¼–ç¨‹çœŸå¥½ç©'
text2 = 'æ¸¸æˆçœŸå¥½ç©ç¼–ç¨‹çœŸå¥½ç©'

print('cosine', ct.cosine_sim(text1, text2, lang='chinese'))
print('jaccard', ct.jaccard_sim(text1, text2, lang='chinese'))
print('minedit', ct.minedit_sim(text1, text2, lang='chinese'))
print('simple', ct.simple_sim(text1, text2, lang='chinese'))
```

Run

```
cosine  0.82
jaccard 0.67
minedit 1.00
simple 0.84
```

<br>

```python
import cntext as ct


text1 = 'Programming is fun!'
text2 = 'Programming is interesting!'

print('cosine', ct.cosine_sim(text1, text2, lang='english'))
print('jaccard', ct.jaccard_sim(text1, text2, lang='english'))
print('minedit', ct.minedit_sim(text1, text2, lang='english'))
print('simple', ct.simple_sim(text1, text2, lang='english'))
```

Run

```
cosine  0.67
jaccard 0.50
minedit 1.00
simple 0.78
```

<br>

### 2.10 word_hhi

æ–‡æœ¬çš„èµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°ã€‚ct.word_hhi(text, lang='chinese')

<br>

**èµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°**(**Herfindahl-Hirschman Index**)ä½œä¸ºä¸€ç§è¡¡é‡å¸‚åœºé›†ä¸­åº¦çš„ç»æµæŒ‡æ ‡ï¼Œé€šå¸¸ç”¨äºåˆ†æäº§ä¸šæˆ–å¸‚åœºä¸­ä¼ä¸šä»½é¢çš„åˆ†å¸ƒæƒ…å†µã€‚

![](img/word-hhi-algo.png)

å‰äººç±»æ¯”å¸‚åœºé›†ä¸­ç¨‹åº¦ï¼Œç”¨äºæµ‹é‡ä¸“åˆ©è´¨é‡(çŸ¥è¯†å®½åº¦)ã€‚ é‚£æ”¾åœ¨æ–‡æœ¬è¯­è¨€ä¸­ï¼Œæˆ‘ä»¬æ˜¯å¦å¯èƒ½åˆ©ç”¨ HHI æ¥é‡åŒ–æŸä¸ªè¯­æ–™åº“ä¸­ä¸åŒè¯æ±‡çš„ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒï¼Œä»¥æ­¤æ¥åˆ†æä¸ªäººã€ç¾¤ä½“æˆ–æ—¶ä»£çš„è¯­è¨€é£æ ¼ã€è¯æ±‡ä¸°å¯Œåº¦ã€æˆ–æ˜¯è¯­è¨€æ ‡å‡†åŒ–ä¸å˜åŒ–çš„è¶‹åŠ¿ã€‚

- å¦‚æœè¯æ±‡åˆ†å¸ƒéå¸¸å‡åŒ€ï¼Œè¡¨æ˜è¯­è¨€ä½¿ç”¨ä¸­çš„è¯æ±‡å¤šæ ·æ€§é«˜ï¼ŒHHI å€¼å°±ä¼šè¾ƒä½ï¼›
- åä¹‹ï¼Œå¦‚æœå°‘æ•°è¯æ±‡å æ®äº†å¤§éƒ¨åˆ†æ–‡æœ¬ç©ºé—´ï¼Œè¡¨æ˜è¯æ±‡ä½¿ç”¨é›†ä¸­ï¼ŒHHI å€¼åˆ™è¾ƒé«˜ã€‚

ç»“åˆå…¶ä»–è¯­è¨€å­¦æŒ‡æ ‡ä¸€èµ·ä½¿ç”¨ï¼Œæ¯”å¦‚ TTRï¼ˆType-Token Ratioï¼Œç±»å‹-æ ‡è®°æ¯”ç‡ï¼‰ã€Shannon entropyï¼ˆé¦™å†œç†µï¼‰ç­‰ï¼Œå…±åŒè¯„ä¼°è¯­è¨€è¡¨è¾¾çš„å¤æ‚åº¦å’Œå¤šæ ·æ€§ã€‚ä¸è¿‡ï¼Œè¿™ç±»ç ”ç©¶çš„æ–‡çŒ®ç›¸å¯¹è¾ƒå°‘ï¼Œå› ä¸ºè¯­è¨€å­¦é¢†åŸŸæœ‰è‡ªå·±ä¸€å¥—æˆç†Ÿä¸”ä¸“ä¸šçš„åˆ†æå·¥å…·å’Œæ–¹æ³•ï¼ŒHHI æ›´å¤šåœ°è¢«è§†ä¸ºè·¨å­¦ç§‘åº”ç”¨çš„ä¸€ä¸ªåˆ›æ–°å°è¯•ã€‚

```python
import cntext as ct

personA = 'è¿™åœºéŸ³ä¹ä¼šå¤ªå—¨äº†'
personB = 'è¿™åœºéŸ³ä¹ä¼šè¯´å‡ºæ¥ä»¤ä½ ä¸æ•¢ç›¸ä¿¡ï¼Œä¸»åŠæ–¹ç­–åˆ’æœ‰æ–¹ï¼Œç¾¤ä¼—æ¿€æƒ…æ»¡æ»¡ï¼Œæˆ‘å°è±¡æ·±åˆ»ï¼Œä½“éªŒæ„Ÿæ‹‰æ»¡'


print('A-hhi', ct.word_hhi(personA))
print('B-hhi', ct.word_hhi(personB))

print('Aè¯æ±‡å¤šæ ·æ€§', 1 - ct.word_hhi(personA))
print('Bè¯æ±‡å¤šæ ·æ€§', 1 - ct.word_hhi(personB))
```

Run

```
A-hhi 0.20000000000000004
B-hhi 0.07024793388429751

Aè¯æ±‡å¤šæ ·æ€§ 0.7999999999999999
Bè¯æ±‡å¤šæ ·æ€§ 0.9297520661157025
```

<br>

<br>

## ä¸‰ã€Plot æ¨¡å—

| æ¨¡å—       | å‡½æ•°                                                                     | åŠŸèƒ½                                                                           |
| ---------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| **plot** | `ct.matplotlib_chinese()`                                                | æ”¯æŒ matplotlib ä¸­æ–‡ç»˜å›¾                                                       |
| **plot** | `ct.lexical_dispersion_plot1(text, targets_dict, lang, title, figsize)`  | å¯¹æŸä¸€ä¸ªæ–‡æœ¬ textï¼Œ å¯è§†åŒ–ä¸åŒç›®æ ‡ç±»åˆ«è¯ targets_dict åœ¨æ–‡æœ¬ä¸­å‡ºç°ä½ç½®         |
| **plot** | `ct.lexical_dispersion_plot2(texts_dict, targets, lang, title, figsize)` | å¯¹æŸå‡ ä¸ªæ–‡æœ¬ texts_dictï¼Œ å¯è§†åŒ–æŸäº›ç›®æ ‡è¯ targets åœ¨æ–‡æœ¬ä¸­å‡ºç°ç›¸å¯¹ä½ç½®(0~100) |

<br>

### 3.1 matplotlib_chinese()

matplotlib é»˜è®¤ä¸æ”¯æŒä¸­æ–‡å¯è§†åŒ–ï¼Œ cntext æ–°å¢è¯¥å‡½æ•°ï¼Œå¯ä»¥è§£å†³ä¸­æ–‡å¯è§†åŒ–é—®é¢˜

```python
import cntext as ct

plt = ct.matplotlib_chinese()
plt.figure(figsize=(7, 4))
plt.plot([1, 2, 3, 4], [1, 4, 9, 16])
plt.title('ä¸­æ–‡å›¾è¡¨', fontsize=10)
plt.show()
```

![](img/27-chinese-matplotlib.png)

<br>

### 3.2 lexical_dispersion_plot1()

è¯æ±‡åˆ†æ•£å›¾å¯è§†åŒ–ï¼Œ å¯¹æŸä¸€ä¸ªæ–‡æœ¬ textï¼Œ å¯è§†åŒ–ä¸åŒç›®æ ‡ç±»åˆ«è¯ targets_dict åœ¨æ–‡æœ¬ä¸­å‡ºç°ä½ç½®

```python
ct.lexical_dispersion_plot1(text, targets_dict, lang='chinese', figsize=(12, 6), title='ç‰¹å®šè¯æ±‡åœ¨ä¸åŒæ–‡æœ¬æ¥æºçš„ç›¸å¯¹ç¦»æ•£å›¾', prop=True)
```

- **_text_**: æ–‡æœ¬æ•°æ®
- **_targets_dict_**: ç›®æ ‡ç±»åˆ«è¯å­—å…¸ï¼› targets_dict={'pos': ['å¼€å¿ƒ', 'å¿«ä¹'], 'neg': ['æ‚²ä¼¤', 'éš¾è¿‡']}
- **_lang_**: æ–‡æœ¬æ•°æ® texts_dict çš„è¯­è¨€ç±»å‹ï¼Œé»˜è®¤'chinese'.
- **_figsize_**: å›¾çš„é•¿å®½å°ºå¯¸. é»˜è®¤ (8, 5).
- **_title_** : å›¾çš„æ ‡é¢˜ï¼›
- **_prop_**: æ¨ªåæ ‡å­—ç¬¦ä½ç½®æ˜¯å¦ä¸ºç›¸å¯¹ä½ç½®. é»˜è®¤ Trueï¼Œæ¨ªåæ ‡ç´¢å¼•å€¼å–å€¼èŒƒå›´ 0 ~ 100

<br>

ç‚¹å‡»ä¸‹è½½ [**ä¸‰ä½“.txt**](https://textdata.cn/data/ä¸‰ä½“.txt)ã€[**åŸºåœ°.txt**](https://textdata.cn/data/åŸºåœ°.txt)ä¸¤æœ¬å°è¯´æ–‡ä»¶ã€‚

```python
import cntext as ct

roles_dict = {
    "æ±ªæ·¼": ['æ±ªæ·¼'],
    "å¶æ–‡æ´": ['å¶æ–‡æ´'],
    "ç½—è¾‘": ['ç½—è¾‘']
}

santi_text = open('ä¸‰ä½“.txt', encoding='utf-8').read()

ax = ct.lexical_dispersion_plot1(text = santi_text,  #æ–‡æœ¬æ•°æ®
                            targets_dict = roles_dict, #è§’è‰²
                            figsize = (10, 4),  #å°ºå¯¸å¤§å°
                            lang = 'chinese',  #ä¸­æ–‡æ•°æ®
                            title = 'ã€Šä¸‰ä½“ã€‹å°è¯´è§’è‰²å‡ºç°ä½ç½®', #æ ‡é¢˜
                            prop = True)    #ç›¸å¯¹ä½ç½®(æ¨ªåæ ‡è½´å–å€¼èŒƒå›´0-100)
ax
```

![](img/23-lexical_dispersion_plot1-relative.png)

<br>

```python
ct.lexical_dispersion_plot1(text = santi_text,  #æ–‡æœ¬æ•°æ®
                            targets_dict = roles_dict, #è§’è‰²
                            figsize = (10, 4),  #å°ºå¯¸å¤§å°
                            lang = 'chinese',  #ä¸­æ–‡æ•°æ®
                            title = 'ã€Šä¸‰ä½“ã€‹å°è¯´è§’è‰²å‡ºç°ä½ç½®', #æ ‡é¢˜
                            prop = False)    #ç»å¯¹ä½ç½®(æ¨ªåæ ‡è½´å–å€¼èŒƒå›´ä¸å°è¯´æ–‡æœ¬é•¿åº¦æœ‰å…³)
```

![](img/24-lexical_dispersion_plot1-absolute.png)

<br>

```python
import cntext as ct

# diyäº†ä¸€ä¸ªå°è¯å…¸
senti_dict = {
    'pos': ['å¼€å¿ƒ', 'å¹¸ç¦', 'å¿«ä¹', 'å®‰å®', 'å¸Œæœ›'],
    'neg': ['ç´§å¼ ', 'ææƒ§', 'å®³æ€•', 'ç»æœ›']
}

santi_text = open('ä¸‰ä½“.txt', encoding='utf-8').read()

ax = ct.lexical_dispersion_plot1(text = santi_text,
                            targets_dict = senti_dict,
                            figsize = (10, 2),
                            lang = 'chinese',
                            title = 'ã€Šä¸‰ä½“ã€‹æƒ…ç»ªè¯å‡ºç°ä½ç½®',
                            prop = True)
ax
```

![](img/25-santi_sentiment.png)

<br>

### 3.3 lexical_dispersion_plot2()

è¯æ±‡åˆ†æ•£å›¾å¯è§†åŒ–ï¼Œ å¯¹æŸå‡ ä¸ªæ–‡æœ¬ texts_dictï¼Œ å¯è§†åŒ–æŸäº›ç›®æ ‡è¯ targets åœ¨æ–‡æœ¬ä¸­å‡ºç°ç›¸å¯¹ä½ç½®(0~100)

```python
ct.lexical_dispersion_plot2(texts_dict, targets, lang='chinese', figsize=(12, 6), title='ç‰¹å®šè¯æ±‡åœ¨ä¸åŒæ–‡æœ¬æ¥æºçš„ç›¸å¯¹ç¦»æ•£å›¾')
```

- **_texts_dict_**: å¤šä¸ªæ–‡æœ¬çš„å­—å…¸æ•°æ®ã€‚å½¢å¦‚{'source1': 'source1 çš„æ–‡æœ¬å†…å®¹', 'source2': 'source2 çš„æ–‡æœ¬å†…å®¹'}
- **_targets_**: ç›®æ ‡è¯åˆ—è¡¨
- **_lang_**: æ–‡æœ¬æ•°æ® texts_dict çš„è¯­è¨€ç±»å‹ï¼Œé»˜è®¤'chinese'.
- **_figsize_**: å›¾çš„é•¿å®½å°ºå¯¸. é»˜è®¤ (8, 5).
- **_title_** : å›¾çš„æ ‡é¢˜ï¼›

```python
targets = ['å¤ªç©º', 'å®‡å®™']

texts_dict = {'ä¸‰ä½“': open('ä¸‰ä½“.txt', encoding='utf-8').read(),
              'åŸºåœ°': open('åŸºåœ°.txt', encoding='utf-8').read()}

ax = ct.lexical_dispersion_plot2(texts_dict = texts_dict,
                            targets = targets,
                            figsize = (10, 2),
                            title = '"å¤ªç©º/å®‡å®™"è¯è¯­å‡ºç°ä½ç½®',
                            lang = 'chinese')
ax
```

![](img/26-santi_base.png)

<br><br>

## å››ã€Model æ¨¡å—

æœ¬éƒ¨åˆ†ä¸»è¦å†…å®¹æ˜¯è¯åµŒå…¥æ¨¡å‹ç›¸å…³æŠ€æœ¯ï¼Œ åŒ…æ‹¬ Word2Vec(GLove)çš„è®­ç»ƒã€è¯»å–ã€æ‰©å±•è¯å…¸ã€‚

| æ¨¡å—        | å‡½æ•°(ç±»)                                                                     | åŠŸèƒ½                                                                                                                                                          |
| ----------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **model** | **_ct.Word2Vec(corpus_file, encoding, lang, window_size, vector_size,...)_** | è®­ç»ƒ Word2Vec                                                                                                                                                 |
| **model** | **_ct.GloVe(corpus_file, encoding, lang, window_size, vector_size, ...)_**   | è®­ç»ƒ GLove æ¨¡å‹ã€‚                                                                                                                                             |
| **model** | **ct.evaluate_similarity(wv, file=None)**                                  | ä½¿ç”¨è¿‘ä¹‰æ³•è¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®çš„æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚                                                                                                          |
| **model** | **ct.evaluate_analogy(wv, file=None)**                                     | ä½¿ç”¨ç±»æ¯”æ³•è¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®çš„æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚                                                                                                          |
| **model** | **_ct.load_w2v(wv_path)_**                                                   | è¯»å– cntext2.x è®­ç»ƒå‡ºçš„ Word2Vec/GloVe æ¨¡å‹æ–‡ä»¶                                                                                                               |
| **model** | **_ct.glove2word2vec(glove_file, word2vec_file)_**                           | å°† GLoVe æ¨¡å‹.txt æ–‡ä»¶è½¬åŒ–ä¸º Word2Vec æ¨¡å‹.txt æ–‡ä»¶ï¼›æ³¨æ„è¿™é‡Œçš„ GLoVe æ¨¡å‹.txt æ˜¯é€šè¿‡[Standfordnlp/GloVe](https://github.com/standfordnlp/GloVe) è®­ç»ƒå¾—åˆ°çš„ã€‚ |
| **model** | **_ct.expand_dictionary(wv, seeddict, topn=100)_**                           | æ‰©å±•è¯å…¸, ç»“æœä¿å­˜åˆ°è·¯å¾„[output/Word2Vec]ä¸­                                                                                                                   |
| **model** | `ct.SoPmi(corpus_file, seed_file, lang='chinese')`                           | å…±ç°æ³•æ‰©å±•è¯å…¸                                                                                                                                                |

### 4.1 Word2Vec()

å¯ç›´æ¥å¯¹åŸå§‹è¯­æ–™ txt æ–‡ä»¶è¿›è¡Œè‡ªåŠ¨ Word2vec è®­ç»ƒã€‚è¯¥å‡½æ•°ä¼šè‡ªåŠ¨å¤„ç†æ–‡æœ¬é¢„å¤„ç†(åˆ†è¯ã€å»åœè¯)ã€å†…å­˜ç®¡ç†ã€å‚æ•°è°ƒæ•´ç­‰é—®é¢˜ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹é¡ºåˆ©è¿›è¡Œã€‚

åœ¨ **_gensim.models.word2vec.Word2Vec_** åŸºç¡€ä¸Šï¼Œå¢åŠ äº†ä¸­è‹±æ–‡çš„é¢„å¤„ç†ï¼Œ ç®€åŒ–äº†ä»£ç ä½¿ç”¨ã€‚é…ç½®å¥½ cntext2.x ç¯å¢ƒï¼Œ å¯ä»¥åšåˆ°

- 1. è®­ç»ƒåªç”¨ä¸€è¡Œä»£ç 
- 2. è¯»å–è°ƒç”¨åªç”¨ä¸€è¡Œä»£ç 

<br>

```
ct.Word2Vec(corpus_file, lang='chinese', dict_file=None, stopwords_file=None, vector_size=100, window_size=6, min_count=5, max_iter=5, chunksize=10000, only_binary=True, **kwargs)
```

- **_corpus_file_**: è¯­æ–™åº“æ–‡ä»¶çš„è·¯å¾„ã€‚
- **_lang_**: è¯­è¨€ç±»å‹ï¼Œæ”¯æŒ 'chinese' å’Œ 'english'ï¼Œé»˜è®¤ä¸º 'chinese'ã€‚
- **_dict_file_**: è‡ªå®šä¹‰è¯å…¸ txt æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º Noneã€‚utf-8 ç¼–ç ã€‚
- **_stopwords_file_**: åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º Noneã€‚utf-8 ç¼–ç ã€‚
- **_vector_size_**: è¯å‘é‡çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 50ã€‚
- **_window_size_**: ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°ï¼Œé»˜è®¤ä¸º 6ã€‚
- **_min_count_**: æœ€å°è¯é¢‘ï¼Œé»˜è®¤ä¸º 10ã€‚
- **_max_iter_**: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 5ã€‚
- **_chunksize_**: æ¯æ¬¡è¯»å–çš„è¡Œæ•°ã€‚é»˜è®¤ä¸º 10000ã€‚è¶Šå¤§é€Ÿåº¦è¶Šå¿«ã€‚
- **_only_binary_** : æ˜¯å¦åªä¿å­˜æ¨¡å‹ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ã€‚é»˜è®¤ä¸º Trueï¼Œ ä¿å­˜ä¸º binã€‚False æ—¶åªä¿å­˜ binã€txtã€‚
- **_kwargs_**: å…¶ä»– gensim å¯é€‰å‚æ•°ï¼Œå¦‚ negativeã€sampleã€hs ç­‰ã€‚

<br>

```python
import cntext as ct

w2v = ct.Word2Vec(corpus_file = 'data/ä¸‰ä½“.txt',
                  lang = 'chinese',
                  window_size = 6,
                  vector_size = 50)


w2v
```

Run

```
Mac(Linux) System, Enable Parallel Processing
Cache output/ä¸‰ä½“_cache.txt Not Found or Empty, Preprocessing Corpus
Reading Preprocessed Corpus from output/ä¸‰ä½“_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 10 s.
Output Saved To: output/Word2Vec/ä¸‰ä½“-Word2Vec.50.6.bin
```

[data/ä¸‰ä½“.txt]ä½“ç§¯ 2.7Mï¼Œ è®­ç»ƒæ—¶é—´ 10sï¼Œ æ¨¡å‹æ–‡ä»¶å­˜å‚¨äº **_output/Word2Vec/ä¸‰ä½“-Word2Vec.50.6.bin_**

![](img/03-word2vec.png)

<br>
<br>

### 4.2 GloVe()

ä½¿ç”¨ Stanford GloVe ä»£ç å·¥å…·è®­ç»ƒ GloVe æ¨¡å‹ã€‚è¯¥å‡½æ•°ä¼šè‡ªåŠ¨å¤„ç†æ–‡æœ¬é¢„å¤„ç†ã€å†…å­˜ç®¡ç†ã€å‚æ•°è°ƒæ•´ç­‰é—®é¢˜ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹é¡ºåˆ©è¿›è¡Œã€‚

```
ct.GloVe(corpus_file, lang='chinese', dict_file=None, stopwords_file=None, vector_size=100, window_size=15, min_count=5, max_memory=4.0, max_iter=15, x_max=10, only_binary=True, chunksize=10000)
```

- **_corpus_file_**: è¾“å…¥è¯­æ–™æ–‡ä»¶è·¯å¾„ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰ã€‚è¯¥æ–‡ä»¶ä¸ºåˆ†è¯åçš„è¯­æ–™æ–‡ä»¶ã€‚
- **_lang_**: è¯­æ–™æ–‡ä»¶çš„è¯­è¨€ç±»å‹ï¼Œé»˜è®¤ä¸º 'chinese'ã€‚
- **_dict_file_**: è‡ªå®šä¹‰è¯å…¸ txt æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º Noneã€‚utf-8 ç¼–ç ã€‚
- **_stopwords_file_**: åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º Noneã€‚utf-8 ç¼–ç ã€‚
- **_vector_size_**: è¯å‘é‡ç»´åº¦ï¼Œé»˜è®¤ 100ã€‚
- **_window_size_**: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œé»˜è®¤ 15ã€‚
- **_min_count_**: å¿½ç•¥å‡ºç°æ¬¡æ•°ä½äºæ­¤å€¼çš„å•è¯ï¼Œé»˜è®¤ 5ã€‚
- **_max_memory_**: å¯ä¾›ä½¿ç”¨çš„æœ€å¤§å†…å­˜å¤§å°ï¼Œå•ä½ä¸º GBï¼Œé»˜è®¤ 4; è¯¥å‚æ•°è¶Šå¤§ï¼Œè®­ç»ƒè¶Šå¿«ã€‚
- **_max_iter_**: è®­ç»ƒçš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ 15ã€‚
- **_x_max_**: å…±ç°çŸ©é˜µä¸­å…ƒç´ çš„æœ€å¤§è®¡æ•°å€¼ï¼Œé»˜è®¤ 10ã€‚
- **_chunksize_**: æ¯æ¬¡è¯»å–çš„è¡Œæ•°ã€‚é»˜è®¤ä¸º 10000ã€‚è¶Šå¤§é€Ÿåº¦è¶Šå¿«ã€‚
- **_only_binary_** : æ˜¯å¦åªä¿å­˜æ¨¡å‹ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ã€‚é»˜è®¤ä¸º Trueï¼Œ ä¿å­˜ä¸º binã€‚False æ—¶åªä¿å­˜ binã€txtã€‚

<br>

ct.GloVe å†…ç½® [Stanford GloVe](https://nlp.stanford.edu/projects/glove/)ç®—æ³•ï¼Œ è®­ç»ƒé€Ÿåº¦éå¸¸å¿«ã€‚

```python
import cntext as ct

glove = ct.GloVe(corpus_file='data/ä¸‰ä½“.txt',
                 lang='chinese',
                 vector_size=50,
                 window_size=15)

glove
```

Run

```
Mac(Linux) System, Enable Parallel Processing
Cache output/ä¸‰ä½“_cache.txt Not Found or Empty, Preprocessing Corpus
Start Training GloVe
BUILDING VOCABULARY
Using vocabulary of size 6975.

COUNTING COOCCURRENCES
Merging cooccurrence files: processed 2106999 lines.

Using random seed 1743474106
SHUFFLING COOCCURRENCES
Merging temp files: processed 2106999 lines.

TRAINING MODEL
Read 2106999 lines.
Using random seed 1743474106
04/01/25 - 10:21.46AM, iter: 001, cost: 0.055981
04/01/25 - 10:21.46AM, iter: 002, cost: 0.050632
......
04/01/25 - 10:21.48AM, iter: 014, cost: 0.030047
04/01/25 - 10:21.48AM, iter: 015, cost: 0.029100

GloVe Training Cost 9 s.
Output Saved To: output/ä¸‰ä½“-GloVe.50.15.bin
<gensim.models.keyedvectors.KeyedVectors at 0x331517440>
```

![](img/05-glove.png)

è®­ç»ƒç”Ÿæˆçš„ `output/GloVe/ä¸‰ä½“-GloVe.50.15.bin` å¯ç”¨ **_ct.load_w2v_** è¯»å–ï¼Œåœ¨åé¢ä¼šæœ‰å±•ç¤ºã€‚

<br>

### 4.3 evaluate_similarity()

è¯„ä¼°è¯å‘é‡æ¨¡å‹è¯­ä¹‰ç›¸ä¼¼è¡¨ç°ã€‚ ä½¿ç”¨ Spearman's Rank Coeficient ä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œ å–å€¼[-1, 1], 1 å®Œå…¨ç›¸å…³ï¼Œ-1 å®Œå…¨è´Ÿç›¸å…³ï¼Œ 0 æ¯«æ— ç›¸å…³æ€§ã€‚

cntext2.x å†…ç½® 537 æ¡è¿‘ä¹‰å®éªŒæ•°æ®ï¼Œ å¯ç›´æ¥ä½¿ç”¨ã€‚

![](img/01-similar.png)

```python
ct.evaluate_similarity(wv, file=None)
```

- **wv** è¯­æ–™ txt æ–‡ä»¶è·¯å¾„
- **file** è¯„ä¼°æ•°æ®æ–‡ä»¶ï¼Œtxt æ ¼å¼ï¼Œé»˜è®¤ä½¿ç”¨ cntext å†…ç½®çš„è¯„ä¼°æ•°æ®æ–‡ä»¶ã€‚ txt æ–‡ä»¶æ¯è¡Œä¸¤ä¸ªè¯ä¸€ä¸ªæ•°å­—ï¼Œå¦‚ä¸‹æ‰€ç¤º

<br>

```
è¶³çƒ	è¶³çƒ	4.98
è€è™	è€è™	4.8888888889
æ’æ˜Ÿ	æ’æ˜Ÿ	4.7222222222
å…¥åœºåˆ¸	é—¨ç¥¨	4.5962962963
ç©ºé—´	åŒ–å­¦	0.9222222222
è‚¡ç¥¨	ç”µè¯	0.92
å›½ç‹	è½¦	0.9074074074
ä¸­åˆ	å­—ç¬¦ä¸²	0.6
æ”¶éŸ³æœº	å·¥ä½œ	0.6
æ•™æˆ	é»„ç“œ	0.5
è‡ªè¡Œè½¦	é¸Ÿ	0.5
è›‹ç™½è´¨	æ–‡ç‰©	0.15
```

<br>

```python
import cntext as ct

# å¯åœ¨ https://cntext.readthedocs.io/zh-cn/latest/embeddings.html ä¸‹è½½è¯¥æ¨¡å‹æ–‡ä»¶
dm_w2v = ct.load_w2v('output/douban-movie-1000w-Word2Vec.200.15.bin')

# ä½¿ç”¨å†…ç½®è¯„ä¼°æ–‡ä»¶
ct.evaluate_similarity(wv=dm_w2v)
# ä½¿ç”¨è‡ªå®šä¹‰è¯„ä¼°æ–‡ä»¶
# ct.evaluate_similarity(wv=dm_w2v, file='diy_similarity.txt')
```

Run

```
è¿‘ä¹‰æµ‹è¯•: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt
Processing Similarity Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 537/537 [00:00<00:00, 85604.55it/s]

è¯„ä¼°ç»“æœï¼š
+----------+------------+----------------------------+
| å‘ç°è¯è¯­ | æœªå‘ç°è¯è¯­ | Spearman's Rank Coeficient |
+----------+------------+----------------------------+
|   459    |     78     |            0.43            |
+----------+------------+----------------------------+
```

<br>

### 4.4 evaluate_analogy()

ç”¨äºè¯„ä¼°è¯å‘é‡æ¨¡å‹åœ¨ç±»æ¯”æµ‹è¯•ï¼ˆanalogy testï¼‰ä¸­è¡¨ç°çš„å‡½æ•°ã€‚å®ƒé€šè¿‡è¯»å–æŒ‡å®šçš„ç±»æ¯”æµ‹è¯•æ–‡ä»¶ï¼Œè®¡ç®—æ¨¡å‹å¯¹è¯è¯­å…³ç³»é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡ã€å‘ç°è¯è¯­æ•°é‡ã€æœªå‘ç°è¯è¯­æ•°é‡ä»¥åŠå¹³å‡æ’åç­‰æŒ‡æ ‡ã€‚

- é›…å…¸ä¹‹äºå¸Œè…Šï¼Œä¼¼å¦‚å·´æ ¼è¾¾ä¹‹äºä¼Šæ‹‰å…‹ã€‚
- å“ˆå°”æ»¨ä¹‹äºé»‘é¾™æ±Ÿï¼Œä¼¼å¦‚é•¿æ²™ä¹‹äºæ¹–å—ã€‚
- å›½ç‹ä¹‹äºç‹åï¼Œä¼¼å¦‚ç”·äººä¹‹äºå¥³äººã€‚

![](img/02-analogy-woman.png)

cntext2.x å†…ç½® 1194 æ¡ç±»æ¯”ï¼Œ æ ¼å¼å¦‚ä¸‹

![](img/03-analogy.png)

ç±»æ¯”æµ‹è¯•çš„æ ¸å¿ƒæ˜¯è§£å†³å½¢å¦‚ "A : B :: C : D" çš„é—®é¢˜ï¼Œç¿»è¯‘è¿‡æ¥å°±æ˜¯"A ä¹‹äº Bï¼Œä¼¼å¦‚ C ä¹‹äº D"ï¼› å³é€šè¿‡ AB ç±»æ¯”å…³ç³»ï¼Œæ‰¾åˆ° C çš„å…³ç³»è¯ Dã€‚è¯¥å‡½æ•°é€šè¿‡è¯å‘é‡æ¨¡å‹çš„ç›¸ä¼¼æ€§æœç´¢åŠŸèƒ½ï¼Œè®¡ç®—é¢„æµ‹ç»“æœä¸çœŸå®ç­”æ¡ˆçš„åŒ¹é…ç¨‹åº¦ã€‚

```python
ct.evaluate_analogy(wv, file=None)
```

- **wv** è¯­æ–™ txt æ–‡ä»¶è·¯å¾„
- **file** è¯„ä¼°æ•°æ®æ–‡ä»¶ï¼Œtxt æ ¼å¼ï¼Œé»˜è®¤ä½¿ç”¨ cntext å†…ç½®çš„è¯„ä¼°æ•°æ®æ–‡ä»¶ã€‚ txt æ–‡ä»¶æ¯è¡Œä¸¤ä¸ªè¯ä¸€ä¸ªæ•°å­—ï¼Œå¦‚ä¸‹æ‰€ç¤º

<br>

è¯„ä¼°æ•°æ® txt æ–‡ä»¶æ ¼å¼ï¼Œå¦‚ä¸‹

```
: CapitalOfCountries
é›…å…¸ å¸Œè…Š å·´æ ¼è¾¾ ä¼Šæ‹‰å…‹
å“ˆç“¦é‚£ å¤å·´ é©¬å¾·é‡Œ è¥¿ç­ç‰™
æ²³å†… è¶Šå— ä¼¦æ•¦ è‹±å›½
: CityInProvince
çŸ³å®¶åº„ æ²³åŒ— å—æ˜Œ æ±Ÿè¥¿
æ²ˆé˜³ è¾½å® å—æ˜Œ æ±Ÿè¥¿
å—äº¬ æ±Ÿè‹ éƒ‘å· æ²³å—
: FamilyRelationship
ç”·å­© å¥³å­© å…„å¼Ÿ å§å¦¹
ç”·å­© å¥³å­© å›½ç‹ ç‹å
çˆ¶äº² æ¯äº² å›½ç‹ ç‹å
ä¸ˆå¤« å¦»å­ å”å” é˜¿å§¨
: SocialScience-Concepts
ç¤¾ä¼š ç¤¾ä¼šç»“æ„ å®¶åº­ å®¶åº­ç»“æ„
æ–‡åŒ– æ–‡åŒ–ä¼ æ‰¿ è¯­è¨€ è¯­è¨€ä¼ æ‰¿
ç¾¤ä½“ ç¾¤ä½“è¡Œä¸º ç»„ç»‡ ç»„ç»‡è¡Œä¸º
```

<br>

```python
import cntext as ct

# å¯åœ¨ https://cntext.readthedocs.io/zh-cn/latest/embeddings.html ä¸‹è½½è¯¥æ¨¡å‹æ–‡ä»¶
dm_w2v = ct.load_w2v('output/douban-movie-1000w-Word2Vec.200.15.bin')

# ä½¿ç”¨å†…ç½®è¯„ä¼°æ–‡ä»¶
ct.evaluate_analogy(wv=dm_w2v)
# ä½¿ç”¨è‡ªå®šä¹‰è¯„ä¼°æ–‡ä»¶
# ct.evaluate_analogy(wv=dm_w2v, file='diy_analogy.txt')
```

Run

```
ç±»æ¯”æµ‹è¯•: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1198/1198 [00:11<00:00, 103.52it/s]

è¯„ä¼°ç»“æœï¼š
+--------------------+----------+------------+------------+----------+
|      Category      | å‘ç°è¯è¯­ | æœªå‘ç°è¯è¯­ | å‡†ç¡®ç‡ (%) | å¹³å‡æ’å |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   615    |     62     |   39.02    |   2.98   |
|   CityInProvince   |   175    |     0      |   28.57    |   4.74   |
| FamilyRelationship |   272    |     0      |   92.65    |   1.48   |
|   SocialScience    |    8     |     62     |   25.00    |   6.00   |
+--------------------+----------+------------+------------+----------+
```

è±†ç“£ç”µå½±åœ¨ FamilyRelationship è¯„ä¼°ä¸­è¡¨ç°è¾ƒå¥½ï¼Œå¤§æ¦‚ç‡æ˜¯å› ä¸ºç”µå½±ä¸»è¦åæ˜ çš„æ˜¯äººä¸äººä¹‹é—´çš„å…³ç³»ï¼Œè¦†ç›–äº†ç»å¤§å¤šæ•° FamilyRelationship å®¶åº­ç±»æ¯”å…³ç³»ï¼Œæ‰€ä»¥ç±»æ¯”è¡¨ç°å·¨å¥½ï¼Œä½†åœ¨å…¶ä»–æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚

å¦‚æœæ˜¯ç»´åŸºç™¾ç§‘è¯­æ–™ï¼Œå¯èƒ½åœ¨ CapitalOfCountriesã€CityInProvinceã€SocialScience ä¸­è¡¨ç°è¾ƒå¥½ã€‚

<br>

### 4.5 SoPmi()

```python
ct.SoPmi(corpus_file, seed_file)       #äººå·¥æ ‡æ³¨çš„åˆå§‹ç§å­è¯
```

- **corpus_file** è¯­æ–™ txt æ–‡ä»¶è·¯å¾„
- **seed_file** åˆå§‹ç§å­è¯ txt æ–‡ä»¶è·¯å¾„

å…±ç°æ³•

```python
import cntext as ct

ct.SoPmi(corpus_file='data/sopmi_corpus.txt',
         seed_file='data/sopmi_seed.txt')       # äººå·¥æ ‡æ³¨çš„åˆå§‹ç§å­è¯

```

Run

```
Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 19.74 s
```

![](img/06-sopmi.png)

<br>

### 4.6 load_w2v()

å¯¼å…¥ cntext2.x é¢„è®­ç»ƒçš„ word2vec æ¨¡å‹ .txt æ–‡ä»¶

```python
ct.load_w2v(w2v_path)
```

- **w2v_path** æ¨¡å‹æ–‡ä»¶è·¯å¾„

è¯»å– **_output/ä¸‰ä½“.100.6.txt_** æ¨¡å‹æ–‡ä»¶, è¿”å› `gensim.models.word2vec.Word2Vec` ç±»å‹ã€‚

```python
import cntext as ct

santi_w2v = ct.load_w2v(w2v_path='output/ä¸‰ä½“-Word2Vec.50.6.bin')
# santi_w2v = ct.load_wv(wv_path='output/ä¸‰ä½“-Word2Vec.50.6.txt')

santi_glove = ct.load_w2v(w2v_path='output/ä¸‰ä½“-GloVe.50.15.bin')
# santi_glove = ct.load_wv(wv_path='output/ä¸‰ä½“-GloVe.50.15.bin')

santi_w2v
```

Run

```
Loading output/ä¸‰ä½“-Word2Vec.50.6.bin...
Loading output/ä¸‰ä½“-GloVe.50.15.bin...
<gensim.models.keyedvectors.KeyedVectors at 0x33aa9cf80>
```

<br>

### 4.7 glove2word2vec()

å°† GLoVe æ¨¡å‹.txt æ–‡ä»¶è½¬åŒ–ä¸º Word2Vec æ¨¡å‹.txt æ–‡ä»¶ï¼› é™¤éä»ç½‘ç»œä¸‹è½½çš„ GloVe æ¨¡å‹èµ„æºï¼Œ å¦åˆ™ä¸€èˆ¬æƒ…å†µç”¨ä¸åˆ°è¿™ä¸ªå‡½æ•°ã€‚

```python
ct.glove2word2vec(glove_file, word2vec_file)
```

- **_glove_file_**: GLoVe æ¨¡å‹.txt æ–‡ä»¶è·¯å¾„
- **_word2vec_file_**: Word2Vec æ¨¡å‹.txt æ–‡ä»¶è·¯å¾„

<br>

æ³¨æ„è¿™é‡Œçš„ GLoVe æ¨¡å‹.txt æ˜¯é€šè¿‡[Standfordnlp/GloVe](https://github.com/standfordnlp/GloVe) è®­ç»ƒå¾—åˆ°çš„

<br>

```python
import cntext as ct
ct.glove2word2vec(glove_file='data/GloVe.6B.50d.txt',
                  word2vec_file='output/word2vec_format_GloVe.6B.50d.txt')
```

<br>

### æ³¨æ„

- **_ct.load_w2v()_** å¯¼å…¥åå¾—åˆ°çš„æ•°æ®ç±»å‹æ˜¯ **_gensim.models.keyedvectors.KeyedVectors_** ã€‚
- **_gensim.models.word2vec.Word2Vec_** å¯ä»¥è½¬åŒ–ä¸º **_gensim.models.keyedvectors.KeyedVectors_** ï¼Œ

<br>

### 4.8 expand_dictionary()

```
ct.expand_dictionary(wv,  seeddict, topn=100)
```

- **wv** é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ•°æ®ç±»å‹ä¸º gensim.models.keyedvectors.KeyedVectorsã€‚
- **seeddict** å‚æ•°ç±»ä¼¼äºç§å­è¯ï¼›æ ¼å¼ä¸º PYTHON å­—å…¸ï¼›
- **topn** è¿”å› topn ä¸ªè¯­ä¹‰æœ€æ¥è¿‘ seeddict çš„è¯

æ ¹æ®è®¾ç½®çš„ seeddict, å¯æŒ‰ç±»åˆ«æ‰©å±•å¹¶ç”Ÿæˆå¯¹åº”çš„è¯å…¸ txt æ–‡ä»¶ï¼Œ txt æ–‡ä»¶ä½äº[output]æ–‡ä»¶å¤¹å†…ã€‚

```python
seeddict = {
    'äººç‰©': ['å¶æ–‡æ´', 'å²å¼º', 'ç½—è¾‘'],
    'ç‰©ä½“': ['é£èˆ¹', 'è½¦è¾†']
}


ct.expand_dictionary(wv=santi_w2v.wv,
                     seeddict=seeddict,
                     topn=10)
```

![](img/04-expand.png)

<br>

<br>

## äº”ã€Mind æ¨¡å—

è¯åµŒå…¥ä¸­è•´å«ç€äººç±»çš„è®¤çŸ¥ä¿¡æ¯ï¼Œä»¥å¾€çš„è¯åµŒå…¥å¤§å¤šæ˜¯æ¯”è¾ƒä¸€ä¸ªæ¦‚å¿µä¸­ä¸¤ç»„åä¹‰è¯ä¸æŸå¯¹è±¡çš„è·ç¦»è®¡ç®—è®¤çŸ¥ä¿¡æ¯ã€‚

- **å¤šä¸ªå¯¹è±¡ä¸æŸæ¦‚å¿µçš„è¯­ä¹‰è¿œè¿‘**ï¼ŒèŒä¸šä¸æ€§åˆ«ï¼ŒæŸä¸ªèŒä¸šæ˜¯å¦å­˜åœ¨äº²è¿‘ç”·æ€§ï¼Œè€Œæ’æ–¥å¥³æ€§

- å¤šä¸ªå¯¹è±¡åœ¨æŸæ¦‚å¿µå‘é‡æŠ•å½±çš„å¤§å°ï¼Œ äººç±»è¯­è¨€ä¸­ç•™å­˜ç€å¯¹ä¸åŒåŠ¨ç‰©ä½“ç§¯çš„è®¤çŸ¥è®°å¿†ï¼Œå¦‚å°é¼ å¤§è±¡ã€‚åŠ¨ç‰©è¯åœ¨è¯å‘é‡ç©ºé—´ä¸­æ˜¯å¦èƒ½ç•™å­˜ç€è¿™ç§å¤§å°çš„è®°å¿†

æœ¬æ¨¡å—ä¸»è¦æ˜¯åˆ©ç”¨å·²è®­ç»ƒå‡ºçš„ word2vec æ¨¡å‹ï¼ŒæŒ–æ˜æ½œåœ¨çš„æ€åº¦åè§ã€åˆ»æ¿å°è±¡ç­‰ã€‚ è¿™éƒ¨åˆ†éš¾åº¦è¾ƒå¤§ï¼Œ å»ºè®®æœ‰ç²¾åŠ›ä¸”ç”µè„‘æ€§èƒ½å¥½çš„åŒå­¦å¯ä»¥ç”¨ cntext è®­ç»ƒæ¨¡å‹ï¼Œ å†æ¥å®éªŒ Mind æ¨¡å—ã€‚
| æ¨¡å— | å‡½æ•°(ç±») | åŠŸèƒ½ |
| --------------- | ---------------------------------------------------- | ---------------------------------------------------------- |
| **mind** | `ct.semantic_centroid(wv, words)` | è®¡ç®—å¤šä¸ªè¯è¯­çš„è¯­ä¹‰ä¸­å¿ƒå‘é‡ |
| **mind** | `ct.generate_concept_axis(wv, poswords, negwords)` | ç”Ÿæˆæ¦‚å¿µè½´å‘é‡ã€‚ |
| **mind** | `sematic_projection(wv, words, poswords, negwords)` | æµ‹é‡è¯­ä¹‰æŠ•å½± |
| **mind** | `ct.project_word(wv, a, b, cosine=False)` | åœ¨è¯å‘é‡ç©ºé—´ä¸­ï¼Œ è®¡ç®—è¯è¯­ a åœ¨è¯è¯­ b ä¸Šçš„æŠ•å½± |
| **mind**  | `ct.project_text(wv, text, axis, lang='chinese', cosine=False)`                                                                    | è®¡ç®—è¯è¯­æ–‡æœ¬textåœ¨æ¦‚å¿µè½´å‘é‡axisä¸Šçš„æŠ•å½±å€¼|
| **mind** | `ct.sematic_distance(wv, words1, words2)` | æµ‹é‡è¯­ä¹‰è·ç¦» |
| **mind** | `ct.divergent_association_task(wv, words)` | æµ‹é‡å‘æ•£æ€ç»´(åˆ›é€ åŠ›) |
| **mind** | `ct.discursive_diversity_score(wv, words)` | æµ‹é‡è¯­è¨€å·®å¼‚æ€§(è®¤çŸ¥å·®å¼‚æ€§) |
| **mind** | **ct.procrustes_align(base_wv, other_wv)** | ä¸¤ä¸ª word2vec è¿›è¡Œè¯­ä¹‰å¯¹é½ï¼Œå¯ååº”éšæ—¶é—´çš„ç¤¾ä¼šè¯­ä¹‰å˜è¿ |

<br>

### 5.1 semantic_centroid(wv, words)

è®¡ç®—å¤šä¸ªè¯è¯­çš„è¯­ä¹‰ä¸­å¿ƒå‘é‡

```python
import cntext as ct

# è·å–è¯å‘é‡æ–‡ä»¶ https://cntext.readthedocs.io/zh-cn/latest/embeddings.html
w2v = ct.load_w2v('ä¸“åˆ©æ‘˜è¦-Word2Vec.200.15.bin')
semantic_centroid(wv=w2v, words=['åˆ›æ–°', 'é¢ è¦†'])
```

Run

```
array([ 0.15567462, -0.05117003, -0.18534171,  0.20808656, -0.01133028,
        0.10738188, -0.02571066,  0.06051835,  0.00107351,  0.08017981,
        0.08914138,  0.01845527,  0.06232869, -0.03851539, -0.17092938,
        0.02196799, -0.04136903,  0.11350462, -0.09539546,  0.04907424,
        0.01268489,  0.05294977,  0.08449743, -0.02762416,  0.02332745,
        0.08865491, -0.06260188, -0.0378293 ,  0.04771722,  0.05745243,
        0.04417403, -0.04126203, -0.02403288, -0.03834526,  0.08115771,
        0.01508994,  0.07678635,  0.01395652,  0.1360324 ,  0.03027042,
       -0.02819572,  0.02339242,  0.11504567,  0.02910597,  0.06149592,
        0.01126606, -0.10132807,  0.07762785, -0.01214836,  0.03780747,
        0.12758181, -0.03115267, -0.19343086, -0.21930983,  0.05253006,
       -0.01452067, -0.07067247, -0.04237257, -0.08911953,  0.08573315,
        0.02742999,  0.05392318,  0.02916237,  0.04465031, -0.0788566 ,
       -0.07088121,  0.03111146,  0.00387428, -0.04032568,  0.14935694,
       -0.03880607,  0.07259471,  0.01711774, -0.05551507,  0.01039889,
        0.00666137,  0.03313185,  0.03169986,  0.08127907,  0.0239668 ,
       -0.00991806, -0.04201584,  0.01199235, -0.08669737, -0.02087858,
       -0.03440931,  0.02360864,  0.06623896, -0.01020982,  0.01200165,
        0.01059455,  0.13041293,  0.01103112,  0.03814259, -0.01519256,
        0.02946554,  0.00593279,  0.08796389,  0.0198915 , -0.0569265 ,
       -0.14622693,  0.07680258, -0.02288322, -0.04959924,  0.03325186,
        0.11031196,  0.06893978,  0.04289736, -0.0307357 , -0.09662723,
        0.02554002,  0.05394766,  0.047071  , -0.09522557, -0.08160087,
       -0.01467315, -0.01304489,  0.07513782,  0.04484766, -0.0516454 ,
        0.00648148,  0.01093231, -0.00303798, -0.06217093,  0.02755075,
       -0.10749754, -0.05205868, -0.02562402,  0.09068517,  0.05208463,
       -0.11790312,  0.02881086, -0.02414756,  0.00192055,  0.03881926,
       -0.05390498,  0.06648378,  0.02055933, -0.07083403, -0.07248309,
       -0.12991821,  0.0603951 ,  0.14131376, -0.01507344, -0.06480791,
       -0.08994781, -0.03397571,  0.0108852 , -0.02777362,  0.01159309,
        0.00121858, -0.0690551 , -0.07747664,  0.03437752, -0.14576062,
        0.06320656, -0.10743124, -0.01910913,  0.15803815, -0.03027673,
       -0.02909171, -0.03350233, -0.0694584 , -0.09807504, -0.09133697,
       -0.01123043,  0.04894681, -0.01971908, -0.08290677, -0.00336836,
        0.09619438, -0.03496556,  0.09733834, -0.0421683 ,  0.01408717,
        0.03355598,  0.00748263,  0.011903  , -0.12909584,  0.01545653,
        0.07656407,  0.09496018,  0.0608537 ,  0.00597665, -0.01628997,
        0.06285962, -0.16796936, -0.0486528 ,  0.01525079, -0.03067709,
       -0.02952635, -0.02731965, -0.06351878,  0.03577968,  0.0457835 ,
        0.08370785, -0.03491699, -0.12606403, -0.08686454, -0.04782247])
```

<br>

### 5.2 generate_concept_axis(wv, poswords, negwords)

ç”Ÿæˆæ¦‚å¿µè½´å‘é‡ã€‚

- **_wv_** ç”Ÿæˆæ¦‚å¿µè½´å‘é‡ã€‚
- **_poswords_** ç¬¬ä¸€ä¸ªè¯è¯­åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¦‚å¿µæ­£ä¹‰è¯ã€‚
- **_negwords_** ç¬¬äºŒä¸ªè¯è¯­åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¦‚å¿µåä¹‰è¯ã€‚

éœ€è¦æ³¨æ„ï¼Œ æ¦‚å¿µ 1 ä¸ æ¦‚å¿µ 2 æ˜¯æ€§è´¨(æ–¹å‘)ç›¸åçš„ä¸¤ä¸ªæ¦‚å¿µï¼Œ å¦‚

- æ€§åˆ«(ç”·, å¥³)
- å°ºå¯¸(å¤§, å°)
- æ–¹å‘(é«˜, ä½)
- æ–¹å‘(å‰, å)
- æ¹¿åº¦(å¹², æ¹¿)
- è´¢å¯Œ(è´«, å¯Œ)

```python
import cntext as ct

# è·å–è¯å‘é‡æ–‡ä»¶
# https://github.com/hiDaDeng/Chinese-Pretrained-Word-Embeddings
dm_w2v = ct.load_w2v('douban-movie-1000w-Word2Vec.200.15.bin')
gender_axis_vector = ct.generate_concept_axis(wv=dm_w2v,
                                              poswords=['ç”·', 'ç”·äºº', 'çˆ¶äº²'],
                                              negwords=['å¥³', 'å¥³äºº', 'æ¯äº²'])
gender_axis_vector
```

Run

```
array([-0.0118976 ,  0.03178174, -0.04656127,  0.00613294, -0.03692355,
       -0.06293361, -0.04739443,  0.01368712,  0.02603469, -0.02268519,
       -0.09925436,  0.05780286,  0.11218373,  0.07519485,  0.06885784,
        0.05505687, -0.04097392,  0.1737831 ,  0.05118835, -0.06879821,
        0.04762978,  0.02224233, -0.04891564, -0.08712718, -0.01432874,
       -0.07395219,  0.01229804,  0.06655715, -0.01864985, -0.04864848,
        0.00260787,  0.06843776,  0.00472286,  0.03623124,  0.11959086,
       -0.04683099, -0.11005358,  0.0271024 , -0.05976011,  0.12669185,
        0.03592191, -0.01125782, -0.02587771, -0.02719228,  0.0507662 ,
       -0.09198377,  0.09546432, -0.01937146,  0.06106697, -0.0405688 ,
       -0.1311393 ,  0.06090249,  0.03515694,  0.01364273, -0.02491697,
        0.03379048, -0.06635275,  0.01432849,  0.01212378, -0.0625283 ,
       -0.03481676, -0.0422427 , -0.17145215, -0.06323837,  0.02563147,
       -0.02371969,  0.01217621, -0.00346871,  0.07024875,  0.08295133,
        0.00731711, -0.01932047,  0.02165518, -0.09927654, -0.08531073,
        0.01949702,  0.00536061,  0.10426087, -0.02010326,  0.02297032,
       -0.10657956,  0.1035546 ,  0.00569263, -0.0849498 ,  0.1098236 ,
        0.05310893, -0.0802139 , -0.01034231, -0.12204715,  0.01407488,
       -0.01781198, -0.0134118 ,  0.09836894,  0.16098371,  0.00609895,
        0.05433145, -0.08940306,  0.00136946, -0.08455469, -0.08432727,
        0.04675778, -0.03415223, -0.18552355, -0.05219543, -0.01127822,
        0.02059881, -0.08120015, -0.15610164,  0.01439221,  0.01727759,
       -0.14516874,  0.01783531, -0.13099317,  0.03820422,  0.03033866,
       -0.01779634,  0.07759558,  0.15866944,  0.00191632, -0.00905253,
        0.0312649 , -0.05698524,  0.07270953, -0.00734233,  0.06289094,
        0.01014149, -0.0052088 ,  0.02478063, -0.0112649 , -0.0930789 ,
        0.14639418, -0.08183327, -0.08392337, -0.01458992, -0.0163887 ,
        0.06790476, -0.03252221,  0.08593727,  0.10469338, -0.01363467,
        0.00749907, -0.01320484,  0.08405331,  0.0489707 , -0.11343482,
       -0.10319041, -0.02415894,  0.13382405, -0.01983603, -0.00990637,
       -0.03335103,  0.11718886, -0.05802442, -0.18935862, -0.07409969,
       -0.08306517, -0.04423901,  0.11331058,  0.00588326,  0.06339834,
        0.04405889,  0.1263905 , -0.007273  , -0.02706875,  0.02325469,
       -0.13092995,  0.02056245, -0.0442118 , -0.01964739, -0.06501938,
        0.02196051, -0.1823353 ,  0.04273191,  0.01935809, -0.01464438,
       -0.02626805,  0.09194217,  0.02489716,  0.05376589, -0.00484252,
        0.02822759,  0.06744799, -0.14196248,  0.03016541, -0.05347864,
       -0.16907257,  0.05094757,  0.0721257 , -0.00421157,  0.03022675,
       -0.00047884,  0.07792547, -0.00209365,  0.0669208 ,  0.02009218,
        0.11358768, -0.05002993,  0.01760067,  0.03407429, -0.0893421 ],
      dtype=float32)
```

<br>

### 5.3 sematic_distance()

**å¤šä¸ªå¯¹è±¡ä¸æŸæ¦‚å¿µçš„è¯­ä¹‰è¿œè¿‘**ï¼Œä¾‹å¦‚æˆåŠŸä¸æ€§åˆ«ï¼ŒæˆåŠŸæ˜¯å¦å­˜åœ¨äº²è¿‘ç”·æ€§ï¼Œè€Œæ’æ–¥å¥³æ€§

![](img/21-music-success-genderbias.png)

```
ct.sematic_distance(wv, words1, words2)
```

- **_wv_** æ¨¡å‹æ•°æ®ï¼Œ æ•°æ®ç±»å‹ä¸º gensim.models.keyedvectors.KeyedVectorsã€‚
- **_words1_**ã€**words2** å‡ä¸ºè¯è¯­åˆ—è¡¨

åˆ†åˆ«è®¡ç®— **_words1_** ä¸ **_words2_** è¯­ä¹‰è·ç¦»ï¼Œè¿”å›è·ç¦»å·®å€¼ã€‚ä¾‹å¦‚

```
male_concept = ['male', 'man', 'he', 'him']
female_concept = ['female', 'woman', 'she', 'her']
engineer_concept  = ['engineer',  'programming',  'software']

dist(male, engineer) = distance(male_concept,  engineer_concept)
dist(female, engineer) = distance(female_concept,  engineer_concept)
```

å¦‚æœ **_dist(male, engineer)-dist(female, engineer)<0_**ï¼Œè¯´æ˜åœ¨è¯­ä¹‰ç©ºé—´ä¸­ï¼Œ**_engineer_concept_** æ›´æ¥è¿‘ **_male_concept_** ï¼Œæ›´è¿œç¦» **_female_concept_** ã€‚

æ¢è¨€ä¹‹ï¼Œåœ¨è¯¥è¯­æ–™ä¸­ï¼Œäººä»¬å¯¹è½¯ä»¶å·¥ç¨‹å¸ˆè¿™ä¸€ç±»å·¥ä½œï¼Œå¯¹å¥³æ€§å­˜åœ¨åˆ»æ¿å°è±¡(åè§)ã€‚

```python
import cntext as ct

# glove_w2v.6B.100d.txté“¾æ¥: https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw æå–ç : 72l0
g_wv = ct.load_w2v('data/glove_w2v.6B.100d.txt')

engineer = ['program', 'software', 'computer']
man_words =  ["man", "he", "him"]
woman_words = ["woman", "she", "her"]

dist_male_engineer = ct.sematic_distance(male_concept,  engineer_concept)
dist_female_engineer = ct.sematic_distance(female_concept,  engineer_concept)

dist_male_engineer - dist_female_engineer
```

Run

```
-0.5
```

dist_male_engineer < dist_female_engineerï¼Œåœ¨è¯­ä¹‰ç©ºé—´ä¸­ï¼Œå·¥ç¨‹å¸ˆæ›´æ¥è¿‘äºç”·äººï¼Œè€Œä¸æ˜¯å¥³äººã€‚

<br>

### 5.4 sematic_projection()

å¤šä¸ªå¯¹è±¡åœ¨æŸæ¦‚å¿µå‘é‡æŠ•å½±çš„å¤§å°

```python
ct.sematic_projection(wv, words, poswords, negwords, return_full=False, cosine=False)
```

- **_wv_** æ¨¡å‹æ•°æ®ï¼Œ æ•°æ®ç±»å‹ä¸º gensim.models.keyedvectors.KeyedVectorsã€‚
- **_words_**ã€**_poswords_**ã€**_negwords_** å‡ä¸ºè¯è¯­åˆ—è¡¨
- **cosine**: æ˜¯å¦ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé»˜è®¤ä¸ºFalseï¼Œè¿”å›æŠ•å½±å€¼ï¼›Trueæ—¶è¿”å›ä½™å¼¦ç›¸ä¼¼åº¦
- **return_full**: æ˜¯å¦è¿”å›å®Œæ•´å…ƒç»„åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºTrue

<br>

ä¸ºäº†è§£é‡Šè¯å‘é‡æ¨¡å‹çš„è¯­ä¹‰æŠ•å½±ï¼Œæˆ‘ä½¿ç”¨äº† 2022 å¹´ Nature è®ºæ–‡ä¸­çš„å›¾ç‰‡[@Grand2022SemanticPR]ã€‚ å…³äºåŠ¨ç‰©çš„åå­—ï¼Œäººç±»å¯¹åŠ¨ç‰©å¤§å°çš„è®¤çŸ¥ä¿¡æ¯éšè—åœ¨è¯­æ–™åº“æ–‡æœ¬ä¸­ã€‚ é€šè¿‡å°†**LARGE WORDS** å’Œ**SMALL WORDS**çš„å«ä¹‰ç”¨ä¸åŒçš„**animals**çš„å‘é‡æŠ•å½±ï¼ŒåŠ¨ç‰©åœ¨**size å‘é‡**ä¸Šçš„æŠ•å½±ï¼ˆå°±åƒä¸‹å›¾ä¸­çš„çº¢çº¿ ) å¾—åˆ°ï¼Œå› æ­¤å¯ä»¥é€šè¿‡è®¡ç®—æ¯”è¾ƒåŠ¨ç‰©çš„å¤§å°ã€‚

æ ¹æ®ä¸¤ç»„åä¹‰è¯ **_poswords_** , **_negwords_** æ„å»ºä¸€ä¸ªæ¦‚å¿µ(è®¤çŸ¥)å‘é‡, words ä¸­çš„æ¯ä¸ªè¯å‘é‡åœ¨æ¦‚å¿µå‘é‡ä¸­æŠ•å½±ï¼Œå³å¯å¾—åˆ°è®¤çŸ¥ä¿¡æ¯ã€‚

åˆ†å€¼è¶Šå¤§ï¼Œ**_words_** è¶Šä½äº **_poswords_** ä¸€ä¾§ã€‚

> Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. _Nature Human Behaviour_, pp.1-13."

![](img/22-semantic_projection.png)

ä¾‹å¦‚ï¼Œäººç±»çš„è¯­è¨€ä¸­ï¼Œå­˜åœ¨å°ºå¯¸ã€æ€§åˆ«ã€å¹´é¾„ã€æ”¿æ²»ã€é€Ÿåº¦ã€è´¢å¯Œç­‰ä¸åŒçš„æ¦‚å¿µã€‚æ¯ä¸ªæ¦‚å¿µå¯ä»¥ç”±ä¸¤ç»„åä¹‰è¯ç¡®å®šæ¦‚å¿µçš„å‘é‡æ–¹å‘ã€‚

ä»¥å°ºå¯¸ä¸ºä¾‹ï¼ŒåŠ¨ç‰©åœ¨äººç±»è®¤çŸ¥ä¸­å¯èƒ½å­˜åœ¨ä½“ç§¯å°ºå¯¸å¤§å°å·®å¼‚ã€‚

```python
import cntext as ct
animals = ['mouse', 'cat', 'horse',  'pig', 'whale']
small_words= ["small", "little", "tiny"]
large_words = ["large", "big", "huge"]

# wiki_wv = ct.load_w2v('wikiçš„word2vecæ¨¡å‹æ–‡ä»¶è·¯å¾„')
# wiki_wv

# In size conception, mouse is smallest, horse is biggest.
# åœ¨å¤§å°æ¦‚å¿µä¸Šï¼Œè€é¼ æœ€å°ï¼Œé©¬æ˜¯æœ€å¤§çš„ã€‚
ct.sematic_projection(wv=wiki_wv,
                      words=animals,
                      poswords=large_words,
                      negwords=small_words,
                      )
```

Run

```
[('mouse', -1.68),
 ('cat', -0.92),
 ('pig', -0.46),
 ('whale', -0.24),
 ('horse', 0.4)]
```

å…³äºå°ºå¯¸çš„è®¤çŸ¥ï¼Œäººç±»åœ¨æ–‡æœ¬ä¸­éšå«ç€è€é¼ è¾ƒå°ï¼Œé©¬è¾ƒå¤§ã€‚

<br>

### 5.5 project_word

åœ¨å‘é‡ç©ºé—´ä¸­ï¼Œ è®¡ç®—è¯è¯­aåœ¨è¯è¯­bä¸Šçš„æŠ•å½±(ä½™å¼¦ç›¸ä¼¼åº¦)ã€‚é»˜è®¤è¿”å›çš„æ˜¯æŠ•å½±å€¼ã€‚
    å¦‚æœ cosine=Trueï¼Œè¿”å›è¯è¯­aä¸è¯è¯­bçš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

```python
project_word(wv, a, b, cosine=False)
```

- **wv** è¯­æ–™ txt æ–‡ä»¶è·¯å¾„
- **a** è¯è¯­ a å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
- **b** è¯è¯­å­—ç¬¦ä¸²ã€è¯è¯­åˆ—è¡¨ã€æˆ–æŸæ¦‚å¿µå‘é‡
- ***cosine***: æ˜¯å¦ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œ é»˜è®¤ä¸ºFalseï¼Œè¿”å›aåœ¨bä¸Šçš„æŠ•å½±å€¼ï¼› Trueæ—¶ï¼Œè¿”å›aä¸bçš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚



```python
b='è‹—æ¡'
for a in ['æ€§æ„Ÿ','ç¾ä¸½', 'å¯çˆ±', 'ä¸‘é™‹']:
    proj = ct.project_word(dm_w2v, a, b)
    print(f'[{a}]åœ¨[{b}]æŠ•å½±å€¼: {proj}')


b='ä¿®é•¿'
for a in ['æ€§æ„Ÿ','ç¾ä¸½', 'å¯çˆ±', 'ä¸‘é™‹']:
    proj = ct.project_word(dm_w2v, a, b)
    print(f'[{a}]åœ¨[{b}]æŠ•å½±å€¼: {proj}')
```

Run

```
[æ€§æ„Ÿ]åœ¨[è‹—æ¡]æŠ•å½±å€¼: 14.172947883605957
[ç¾ä¸½]åœ¨[è‹—æ¡]æŠ•å½±å€¼: 7.0944623947143555
[å¯çˆ±]åœ¨[è‹—æ¡]æŠ•å½±å€¼: 6.935092926025391
[ä¸‘é™‹]åœ¨[è‹—æ¡]æŠ•å½±å€¼: 1.235807180404663

[æ€§æ„Ÿ]åœ¨[ä¿®é•¿]æŠ•å½±å€¼: 14.599699974060059
[ç¾ä¸½]åœ¨[ä¿®é•¿]æŠ•å½±å€¼: 9.360642433166504
[å¯çˆ±]åœ¨[ä¿®é•¿]æŠ•å½±å€¼: 4.740543842315674
[ä¸‘é™‹]åœ¨[ä¿®é•¿]æŠ•å½±å€¼: 4.010622501373291
```

å¯ä»¥çœ‹åˆ°ï¼Œ åœ¨è±†ç“£ç”µå½±è¯­æ–™ä¸­ï¼Œ åœ¨[è‹—æ¡ã€ä¿®é•¿]ç»´åº¦çš„è®¤çŸ¥ä¸­ï¼Œéƒ½è®¤ä¸º

- [æ€§æ„Ÿ]æ„å‘³ç€èº«ææœ€ç˜¦é•¿
- [ç¾ä¸½]æ¬¡ä¹‹ã€[å¯çˆ±]ç•¥æ˜¾ä¸é‚£ä¹ˆä¿®é•¿è‹—æ¡
- [ä¸‘é™‹]æ„å‘³ç€åŸºæœ¬ä¸[è‹—æ¡ã€ä¿®é•¿]æ— å…³ï¼Œæ•°å€¼æœ€å°ã€‚

<br>

ä¸ºäº†è®©æŠ•å½±å€¼æ›´ç¨³å®šï¼Œå¯ä»¥é€‰æ‹©è¯ç»„ï¼Œç¡®å®š[è‹—æ¡ã€ä¿®é•¿]è¿™ä¸ªæ¦‚å¿µçš„æ¦‚å¿µè½´å‘é‡

```python
for a in ['æ€§æ„Ÿ','ç¾ä¸½', 'å¯çˆ±', 'ä¸‘é™‹']:
    proj = ct.project_word(wv=dm_w2v, a=a, b=['ä¿®é•¿', 'è‹—æ¡'])
    print(f'[{a}]åœ¨[ä¿®é•¿ï¼Œè‹—æ¡]æŠ•å½±å€¼: {proj}')
```

Run

```
[æ€§æ„Ÿ]åœ¨[ä¿®é•¿ï¼Œè‹—æ¡]æŠ•å½±å€¼: 15.807487487792969
[ç¾ä¸½]åœ¨[ä¿®é•¿ï¼Œè‹—æ¡]æŠ•å½±å€¼: 9.040315628051758
[å¯çˆ±]åœ¨[ä¿®é•¿ï¼Œè‹—æ¡]æŠ•å½±å€¼: 6.414511203765869
[ä¸‘é™‹]åœ¨[ä¿®é•¿ï¼Œè‹—æ¡]æŠ•å½±å€¼: 2.882350444793701
```

<br>

### 5.6 project_text()

åœ¨å‘é‡ç©ºé—´ä¸­ï¼Œè®¡ç®—æ–‡æœ¬åœ¨æ¦‚å¿µè½´å‘é‡ä¸Šçš„æŠ•å½±å€¼ã€‚

```python
ct.project_text(wv, text, axis, lang='chinese', cosine=False)
```
- **wv**: è¯­è¨€æ¨¡å‹çš„KeyedVectors
- **text**: æ–‡æœ¬å­—ç¬¦ä¸²
- **lang**:  è¯­è¨€,æœ‰chineseå’Œenglishä¸¤ç§; é»˜è®¤"chinese"
- **axis**:  æ¦‚å¿µå‘é‡
- **cosine**: æŠ•å½±å€¼æ˜¯å¦ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œ é»˜è®¤ä¸ºFalseï¼Œè¿”å›textåœ¨axisä¸Šçš„æŠ•å½±å€¼ï¼› Trueæ—¶ï¼Œè¿”å›textä¸axisçš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

```python
import cntext as ct

# 1. è¯»å–è¯åµŒå…¥æ¨¡å‹æ–‡ä»¶
embeddings_model = ct.load_w2v('cntext2xè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„')
#dm_w2v = ct.load_w2v('douban-movie-1000w-Word2Vec.200.15.bin')

# 2. å®šä¹‰æƒ…ç»ªæ­£è´Ÿè¯è¯­ï¼Œç¡®å®šæƒ…ç»ªæ¦‚å¿µè½´å‘é‡sentiment_axis
sentiment_pos = ['å¿«ä¹', 'å¹¸ç¦', 'å–œæ‚¦', 'æ»¡è¶³', 'æ¬£æ…°', 'æ¿€åŠ¨', 'å…´å¥‹', 'æ„Ÿæ©', 'çƒ­çˆ±', 'èµç¾']
sentiment_neg = ['ç—›è‹¦', 'æ‚²ä¼¤', 'éš¾è¿‡', 'å¤±æœ›', 'æ„¤æ€’', 'æ€¨æ¨', 'ç»æœ›', 'ææƒ§', 'ç„¦è™‘', 'å‹æŠ‘']
sentiment_axis = ct.generate_concept_axis(wv = embeddings_model, 
                                         poswords=sentiment_pos,
                                         negwords=sentiment_neg)
# 3. åˆ›å»ºå®éªŒæ–‡æœ¬ï¼ˆä»æ­£é¢åˆ°è´Ÿé¢ï¼‰
texts = [
    "ä»Šå¤©é˜³å…‰æ˜åªšï¼Œæˆ‘å’Œå®¶äººä¸€èµ·å‡ºæ¸¸ï¼Œæ„Ÿåˆ°æ— æ¯”å¹¸ç¦å’Œå¿«ä¹ã€‚",
    "å·¥ä½œæœ‰äº†æ–°è¿›å±•ï¼Œå¾—åˆ°äº†é¢†å¯¼çš„è¡¨æ‰¬ï¼Œå†…å¿ƒå……æ»¡æˆå°±æ„Ÿã€‚",
    "è™½ç„¶é‡åˆ°äº†å°æŒ«æŠ˜ï¼Œä½†æˆ‘ä¾ç„¶ä¿æŒä¹è§‚ï¼Œç›¸ä¿¡æ˜å¤©ä¼šæ›´å¥½ã€‚",
    "ç”Ÿæ´»å¹³æ·¡ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„äº‹å‘ç”Ÿï¼Œå¿ƒæƒ…ä¸€èˆ¬ã€‚",
    "æœ€è¿‘å‹åŠ›æœ‰ç‚¹å¤§ï¼Œç¡çœ ä¸å¥½ï¼Œæ„Ÿè§‰æœ‰ç‚¹ç„¦è™‘å’Œç–²æƒ«ã€‚",
    "é¡¹ç›®å¤±è´¥äº†ï¼Œè¿˜è¢«é¢†å¯¼æ‰¹è¯„ï¼Œå¿ƒé‡Œéå¸¸éš¾è¿‡å’Œå¤±æœ›ã€‚",
    "äº²äººç¦»ä¸–ï¼Œæˆ‘æ„Ÿåˆ°æåº¦æ‚²ä¼¤å’Œç—›è‹¦ï¼Œä¸–ç•Œä»¿ä½›å¤±å»äº†é¢œè‰²ã€‚"
]


# 4. è®¡ç®—æ¯æ¡æ–‡æœ¬åœ¨æƒ…ç»ªè½´ä¸Šçš„æŠ•å½±
print("æ–‡æœ¬æƒ…ç»ªæŠ•å½±åˆ†æï¼ˆè¶Šå¤§è¶Šæ­£é¢ï¼‰ï¼š\n")
results = []
for i, text in enumerate(texts):
    # ä½¿ç”¨æŠ•å½±å‡½æ•°ï¼ˆè¿”å›åœ¨ axis æ–¹å‘ä¸Šçš„æŠ•å½±å€¼ï¼‰
    #project_text(wv, text, axis, lang='chinese', cosine=False)
    proj_value = ct.project_text(wv=embeddings_model, 
                                 text=text, 
                                 axis=sentiment_axis, 
                                 lang='chinese')
    results.append((proj_value, text))
    print(f"[{i+1}] æŠ•å½±å€¼: {proj_value:+.4f} | {text}\n")

# 5. æŒ‰æŠ•å½±å€¼æ’åºï¼ŒæŸ¥çœ‹æƒ…ç»ªå¼ºåº¦æ’åº
print("\n" + "="*60)
print("æŒ‰æƒ…ç»ªæ­£é¢æ€§æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š")
print("="*60)
for value, text in sorted(results, key=lambda x: x[0], reverse=True):
    print(f"{value:+.4f} â†’ {text}")
```
Run
```
[1] æŠ•å½±å€¼: +0.8213 | ä»Šå¤©é˜³å…‰æ˜åªšï¼Œæˆ‘å’Œå®¶äººä¸€èµ·å‡ºæ¸¸ï¼Œæ„Ÿåˆ°æ— æ¯”å¹¸ç¦å’Œå¿«ä¹ã€‚
[2] æŠ•å½±å€¼: +0.5641 | å·¥ä½œæœ‰äº†æ–°è¿›å±•ï¼Œå¾—åˆ°äº†é¢†å¯¼çš„è¡¨æ‰¬ï¼Œå†…å¿ƒå……æ»¡æˆå°±æ„Ÿã€‚
[3] æŠ•å½±å€¼: +0.1205 | è™½ç„¶é‡åˆ°äº†å°æŒ«æŠ˜ï¼Œä½†æˆ‘ä¾ç„¶ä¿æŒä¹è§‚ï¼Œç›¸ä¿¡æ˜å¤©ä¼šæ›´å¥½ã€‚
[4] æŠ•å½±å€¼: -0.0321 | ç”Ÿæ´»å¹³æ·¡ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„äº‹å‘ç”Ÿï¼Œå¿ƒæƒ…ä¸€èˆ¬ã€‚
[5] æŠ•å½±å€¼: -0.3178 | æœ€è¿‘å‹åŠ›æœ‰ç‚¹å¤§ï¼Œç¡çœ ä¸å¥½ï¼Œæ„Ÿè§‰æœ‰ç‚¹ç„¦è™‘å’Œç–²æƒ«ã€‚
[6] æŠ•å½±å€¼: -0.6124 | é¡¹ç›®å¤±è´¥äº†ï¼Œè¿˜è¢«é¢†å¯¼æ‰¹è¯„ï¼Œå¿ƒé‡Œéå¸¸éš¾è¿‡å’Œå¤±æœ›ã€‚
[7] æŠ•å½±å€¼: -0.9012 | äº²äººç¦»ä¸–ï¼Œæˆ‘æ„Ÿåˆ°æåº¦æ‚²ä¼¤å’Œç—›è‹¦ï¼Œä¸–ç•Œä»¿ä½›å¤±å»äº†é¢œè‰²ã€‚
```



<br>

### 5.7 divergent_association_task()

[PNAS | ä½¿ç”¨è¯­ä¹‰è·ç¦»æµ‹é‡ä¸€ä¸ªäººçš„åˆ›æ–°åŠ›(å‘æ•£æ€ç»´)å¾—åˆ†](https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/)ã€‚ä¸€äº›ç†è®ºè®¤ä¸ºï¼Œæœ‰ åˆ›é€ åŠ› çš„äººèƒ½å¤Ÿäº§ç”Ÿæ›´å¤š å‘æ•£æ€§ çš„æƒ³æ³•ã€‚å¦‚æœè¿™æ˜¯æ­£ç¡®çš„ï¼Œç®€å•åœ°è®©è¢«è¯•å†™ N ä¸ªä¸ç›¸å…³çš„å•è¯ï¼Œç„¶åæµ‹é‡è¿™ N ä¸ªè¯çš„è¯­ä¹‰è·ç¦»ï¼Œ ä½œä¸ºå‘æ•£æ€ç»´çš„å®¢è§‚è¡¡é‡æ ‡å‡†ã€‚

```
ct.divergent_association_task(wv, words)
```

- **wv** æ¨¡å‹æ•°æ®ï¼Œ æ•°æ®ç±»å‹ä¸º gensim.models.keyedvectors.KeyedVectorsã€‚
- **words**è¯è¯­åˆ—è¡¨

```
low_words = ["arm", "eyes", "feet", "hand", "head", "leg", "body"]
average_words = ["bag", "bee", "burger", "feast", "office", "shoes", "tree"]
high_words = ["hippo", "jumper", "machinery", "prickle", "tickets", "tomato", "violin"]

# å¯¼å…¥æ¨¡å‹ï¼Œå¾—åˆ°wvã€‚
# wv = ct.load_w2v('wikiçš„word2vecæ¨¡å‹æ–‡ä»¶è·¯å¾„')


print(ct.divergent_association_task(wv, low_words)) # 50
print(ct.divergent_association_task(wv, average_words)) # 78
print(ct.divergent_association_task(wv, high_words)) # 95
```

Run

```
50
78
95
```

<br>

### 5.8 discursive_diversity_score()

[MS2022 | ä½¿ç”¨è¯­è¨€å·®å¼‚æ€§æµ‹é‡å›¢é˜Ÿè®¤çŸ¥å·®å¼‚æ€§](https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/)

```
ct.discursive_diversity_score(wv, words)
```

- **_wv_** æ¨¡å‹æ•°æ®ï¼Œ æ•°æ®ç±»å‹ä¸º gensim.models.keyedvectors.KeyedVectorsã€‚
- **_words_**è¯è¯­åˆ—è¡¨
- è¿”å›ä¸€ä¸ªæ•°å€¼

![](img/23-low-and-high-examples-of-discursive-diversity.jpeg)

é«˜ç»©æ•ˆå›¢é˜Ÿæ˜¯é‚£äº›å…·æœ‰è°ƒèŠ‚å…±äº«è®¤çŸ¥ä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„ä»»åŠ¡è¦æ±‚çš„é›†ä½“èƒ½åŠ›çš„å›¢é˜Ÿï¼šåœ¨è¿›è¡Œæ„æ€ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬è¡¨ç°å‡ºæ›´é«˜çš„è¯è¯­å¤šæ ·æ€§ï¼Œåœ¨æ‰§è¡Œåè°ƒä»»åŠ¡æ—¶ï¼Œè¡¨ç°å‡ºè¾ƒä½çš„è¯è¯­å¤šæ ·æ€§ã€‚

<br>

### 5.8 procrustes_align()

è¯¥å‡½æ•°ä¸»è¦ç”¨äºåæ˜ åŒä¸€ç ”ç©¶å¯¹è±¡éšç€æ—¶é—´æ¨è¿›çš„ç¤¾ä¼šæ–‡åŒ–å˜è¿ï¼Œæˆ–è€…åŒä¸€æ—¶é—´èŒƒå›´å†…ä¸¤ä¸ªè¢«ç ”ç©¶ä¸»ä½“é—´çš„å·®å¼‚ã€‚

```
ct.procrustes_align(base_wv, other_wv, words=None)
```

- base_wv (gensim.models.keyedvectors.KeyedVectors): åŸºå‡†è¯­è¨€æ¨¡å‹
- other_wv (gensim.models.keyedvectors.KeyedVectors): å…¶ä»–è¯­è¨€æ¨¡å‹
- words (list, optional): æ˜¯å¦æ ¹æ®è¯å…¸ words å¯¹æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œ å¯¹é½ç»“æŸåçš„æ¨¡å‹ä¸­å«æœ‰çš„è¯ä¸ä¼šè¶…å‡º words çš„èŒƒå›´ï¼› é»˜è®¤ None.

ç”±äºä¸åŒè¯­æ–™è®­ç»ƒçš„ Word2Vec æ¨¡å‹æ— æ³•ç›´æ¥æ¯”è¾ƒï¼Œ éœ€è¦å…ˆé€‰å®šä¸€ä¸ªåŸºå‡†æ¨¡å‹ **_base_embed_**ï¼Œ ä¹‹åæ ¹æ® **_base_embed_** å¯¹å…¶ä»–æ¨¡å‹ **_other_embed_** è¿›è¡Œè°ƒæ•´ï¼Œè°ƒæ•´åçš„æ¨¡å‹å°±å¯ä»¥ä½¿ç”¨å‰é¢çš„è¯­ä¹‰è·ç¦»å‡½æ•°æˆ–è€…è¯­ä¹‰æŠ•å½±å‡½æ•°ã€‚ è¿™ä¸€è¿‡ç¨‹ç”¨åˆ°çš„ç®—æ³•å«åš procrustes æ­£äº¤ç®—æ³•ã€‚

è¿™é‡Œæ¨èä¸€ç¯‡ [å¯è§†åŒ– | äººæ°‘æ—¥æŠ¥è¯­æ–™åæ˜ ä¸ƒåå¹´æ–‡åŒ–æ¼”å˜](https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/)

<br><br>

## å…­ã€LLM æ¨¡å—

ç›®å‰å¤§æ¨¡å‹æœ¬åœ°åŒ–ä½¿ç”¨è¶Šæ¥è¶Šæ–¹ä¾¿ï¼Œ

| æ¨¡å—      | å‡½æ•°(ç±»)                                                                                            | åŠŸèƒ½                   |
| --------- | --------------------------------------------------------------------------------------------------- | ---------------------- |
| **_LLM_**   | **ct.llm(text, prompt, output_format, task, backend, base_url, api_key, model_name, temperature)** | è°ƒç”¨å¤§æ¨¡å‹æ‰§è¡Œç»“æ„åŒ–æ–‡æœ¬åˆ†æä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æã€å…³é”®è¯æå–ã€åˆ†ç±»ç­‰ï¼‰ã€‚ |


### 6.1 ct.llm()

ä½¿ç”¨å¤§æ¨¡å‹ï¼ˆæœ¬åœ°æˆ– APIï¼‰è¿›è¡Œæ–‡æœ¬åˆ†æï¼Œä»éç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®ä¸­è¯†åˆ«æ¨¡å¼ã€æå–å…³é”®ä¿¡æ¯ã€ç†è§£è¯­ä¹‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®ä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æå’Œåº”ç”¨ã€‚

<br>

```python
ct.llm(text, prompt, output_format, task, backend, base_url, api_key, model_name, temperature)
```

- **text**: å¾…åˆ†æçš„æ–‡æœ¬å†…å®¹
- **task**: é¢„è®¾ä»»åŠ¡åç§°ï¼Œé»˜è®¤ä¸º 'sentiment'ã€‚
- **prompt**: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯­
- **output_format**: è‡ªå®šä¹‰è¾“å‡ºç»“æ„ï¼Œå¦‚ {'label': str, 'score': float}
- **backend**: å¿«æ·åç«¯åˆ«åï¼š
            - 'ollama' â†’ http://127.0.0.1:11434/v1
            - 'lmstudio' æˆ– 'lms' â†’ http://localhost:1234/v1
            - None â†’ éœ€é…åˆ base_url ä½¿ç”¨
- **base_url**: è‡ªå®šä¹‰æ¨¡å‹æœåŠ¡åœ°å€ï¼Œä¼˜å…ˆçº§é«˜äº backend
            ç¤ºä¾‹ï¼š
            - è¿œç¨‹ï¼šhttps://dashscope.aliyuncs.com/compatible-mode/v1
            - å†…ç½‘ï¼šhttp://192.168.1.10:11434/v1
            - æœ¬åœ°ï¼šhttp://localhost:1234/v1
- **api_key**: API å¯†é’¥ï¼Œè¿œç¨‹æœåŠ¡å¿…å¡«ï¼Œæœ¬åœ°é€šå¸¸ä¸º "EMPTY"
- **model_name**: æ¨¡å‹åç§°ï¼ˆéœ€æœåŠ¡ç«¯å·²åŠ è½½ï¼‰
- **temperature**: ç”Ÿæˆæ¸©åº¦ï¼Œ0 è¡¨ç¤ºç¡®å®šæ€§è¾“å‡º

<br>

**å®éªŒæ•°æ®ä¸ºå¤–å–è¯„è®ºï¼Œ ä»Šå¤©å’±ä»¬åšä¸ªæœ‰éš¾åº¦çš„æ–‡æœ¬åˆ†æä»»åŠ¡ï¼Œä»ä¸åŒç»´åº¦(å‘³é“ã€é€Ÿåº¦ã€æœåŠ¡)å¯¹å¤–å–è¯„è®ºè¿›è¡Œæ‰“åˆ†(-1.0~1.0)**ã€‚

![](img/28-llm-analysis.png)<br>

```python
import cntext as ct

PROMPT = 'ä»å£å‘³tasteã€é€Ÿåº¦speedã€æœåŠ¡serviceä¸‰ä¸ªç»´åº¦ï¼Œ å¯¹å¤–å–è¯„è®ºå†…å®¹è¿›è¡Œæ–‡æœ¬åˆ†æï¼Œ åˆ†åˆ«è¿”å›ä¸åŒç»´åº¦çš„åˆ†å€¼(åˆ†å€¼èŒƒå›´-1.0 ~ 1.0)'
BASE_URL = 'https://dashscope.aliyuncs.com/compatible-mode/v1'
API_KEY = 'ä½ çš„API-KEY'
MODEL_NAME = 'qwen-max'

#å‘³é“ã€é€Ÿåº¦ã€æœåŠ¡
OUTPUT_FORMAT = {'taste': float, 'speed': float, 'service': float}

COMMENT_CONTENT = 'å¤ªéš¾åƒäº†'

# ä½¿ç”¨
# result = ct.llm(text=COMMENT_CONTENT,
# æˆ–
result = ct.llm(text=COMMENT_CONTENT,
                prompt=PROMPT,
                base_url=BASE_URL,
                api_key=API_KEY,
                model_name=MODEL_NAME,
                temperature=0,
                output_format=OUTPUT_FORMAT)

result
```

Run

```
{'taste': -1.0, 'speed': 0.0, 'service': 0.0}
```

<br>

æ‰¹é‡è¿ç®—

```python
import pandas as pd
import cntext as ct


# æ„é€ å®éªŒæ•°æ®
data = ['é€Ÿåº¦éå¸¸å¿«ï¼Œå£å‘³éå¸¸å¥½ï¼Œ æœåŠ¡éå¸¸æ£’ï¼',
        'é€é¤æ—¶é—´è¿˜æ˜¯æ¯”è¾ƒä¹…',
        'é€å•å¾ˆå¿«ï¼Œèœä¹Ÿä¸é”™èµ',
        'å¤ªéš¾åƒäº†']
df = pd.DataFrame(data, columns=['comment'])


# åˆ†æå‡½æ•°
def llm_analysis(text):
    result = ct.llm(text=text,
                    prompt= 'ä»å£å‘³tasteã€é€Ÿåº¦speedã€æœåŠ¡serviceä¸‰ä¸ªç»´åº¦ï¼Œ å¯¹å¤–å–è¯„è®ºå†…å®¹è¿›è¡Œæ–‡æœ¬åˆ†æï¼Œ åˆ†åˆ«è¿”å›ä¸åŒç»´åº¦çš„åˆ†å€¼(åˆ†å€¼èŒƒå›´-1.0 ~ 1.0)',
                    base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',
                    api_key='ä½ çš„API-KEY',
                    model_name='qwen-max',
                    output_format={'taste': float, 'speed': float, 'service': float}
                               )
    return pd.Series(result)


# æ‰¹é‡è¿ç®—
df2 = df['comment'].apply(llm_analysis)
res_df = pd.concat([df, df2], axis=1)
# ä¿å­˜åˆ†æç»“æœ
res_df.to_csv('result.csv', index=False)
res_df
```

![](img/28-llm-analysis.png)

<br>

LLM æ›´å¤šè¯¦ç»†å†…å®¹ï¼Œè¯·é˜…è¯» [**æ•™ç¨‹ | ä½¿ç”¨åœ¨çº¿å¤§æ¨¡å‹å°†æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®**](https://textdata.cn/blog/2025-02-14-using-online-large-model-api-to-transform-text-data-into-structured-data/)

<br>

### 6.2 å†…ç½®prompt
cntext
```python
ct.llm.tasks_list()
```
Run
```
['sentiment',
 'emotion',
 'classify',
 'intent',
 'keywords',
 'entities',
 'summarize',
 'rewrite',
 'quality',
 'similarity']
```

<br>

```python
# è·å–sentimentæ¨¡æ¿
ct.llm.tasks_get('sentiment')
```
Run
```
{'prompt': 'åˆ†æè¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼šè¿”å›æƒ…æ„Ÿç±»åˆ« labelï¼ˆpos è¡¨ç¤ºæ­£é¢ï¼Œneg è¡¨ç¤ºè´Ÿé¢ï¼Œneutral è¡¨ç¤ºä¸­æ€§ï¼‰å’Œæƒ…æ„Ÿåˆ†å€¼ scoreï¼ˆå–å€¼èŒƒå›´ -1~1ï¼Œè´Ÿæ•°ä¸ºè´Ÿé¢ï¼‰',
 'output_format': {'label': 'str', 'score': 'float'}}
```

<br>


```python
# ä½¿ç”¨sentimentæç¤ºè¯æ¨¡æ¿ã€‚
# å¯ç”¨OllamaæœåŠ¡ï¼Œè°ƒç”¨qwen2.5:7bæ¨¡å‹
ct.llm("æœåŠ¡å¾ˆæ£’ï¼", task="sentiment", backend="ollama",  model_name="qwen2.5:7b")
```
Run
```
[cntext2x] âœ… è¿æ¥æ¨¡å‹æœåŠ¡: http://127.0.0.1:11434/v1
{'label': 'pos', 'score': 0.8}
```



<br><br>

## ä½¿ç”¨å£°æ˜

å¦‚åœ¨ç ”ç©¶æˆ–é¡¹ç›®ä¸­ä½¿ç”¨åˆ° **cntext** ï¼Œè¯·ç®€è¦ä»‹ç» cntext ï¼Œå¹¶é™„åŠ ä½¿ç”¨å£°æ˜å‡ºå¤„ã€‚

### apalike

Deng, X., & Nan, P. (2022). **cntext: a Python tool for text mining** [Computer software]. Zenodo. https://doi.org/10.5281/zenodo.7063523

Source Code URL: [https://github.com/hiDaDeng/cntext](https://github.com/hiDaDeng/cntext)

<br>

### bibtex

```
@misc{deng2022cntext,
  author       = {Deng, X. and Nan, P.},
  title        = {cntext: a Python tool for text mining},
  year         = {2022},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.7063523},
  url          = {https://doi.org/10.5281/zenodo.7063523},
  howpublished = {[Computer software]},
  note         = {Source Code URL: \url{https://github.com/hiDaDeng/cntext}}
}
```

<br>

### endnote

```
%0 Generic
%A Deng, X.
%A Nan, P.
%T cntext: a Python tool for text mining
%Y [Computer software]
%D 2022
%I Zenodo
%R 10.5281/zenodo.7063523
%U https://doi.org/10.5281/zenodo.7063523
%Z Source Code URL: https://github.com/hiDaDeng/cntext
%@
```

