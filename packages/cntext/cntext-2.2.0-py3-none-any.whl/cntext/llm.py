import asyncio
import nest_asyncio
import pandas as pd
from typing import Union, List, Dict, Any, Optional
from openai import AsyncOpenAI
import instructor
from pydantic import create_model
from aiolimiter import AsyncLimiter
import warnings

# åº”ç”¨ nest_asyncioï¼Œè§£å†³ Jupyter ä¸­ event loop å·²è¿è¡Œçš„é—®é¢˜
nest_asyncio.apply()


# ======================
# å·¥å…·å‡½æ•°ï¼šå®‰é™æ‰“å°
# ======================

def _is_notebook():
    try:
        shell = get_ipython().__class__.__name__
        return shell in ['ZMQInteractiveShell', 'TerminalInteractiveShell']
    except:
        return False

_printed_messages = set()

def _quiet_print(msg: str, verbose: bool = True, once: bool = True):
    if not verbose:
        return
    if once and msg in _printed_messages:
        return
    _printed_messages.add(msg)

    if _is_notebook():
        from IPython.display import display, HTML
        html_msg = f"<small style='color: #555;'>[cntext2x] {msg}</small>"
        display(HTML(html_msg))
    else:
        print(f"[cntext2x] {msg}")


# ======================
# å¼‚æ­¥æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ======================

async def _llm_async_batch(
    inputs: List[str],  # åªæ¥å—å­—ç¬¦ä¸²åˆ—è¡¨
    task: str,
    prompt: Optional[str],
    output_format: Optional[Dict[str, Any]],
    base_url: str,
    api_key: str,
    model_name: str,
    temperature: float,
    max_retries: int,
    rate_limit: Optional[Union[int, float]] = None,  # æ–°ï¼šç»Ÿä¸€é™é€Ÿå‚æ•°
):
    """å†…éƒ¨å¼‚æ­¥æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆä»…æ”¯æŒå­—ç¬¦ä¸²è¾“å…¥ï¼‰"""
    

    limiter = None

    if rate_limit is not None:
        if isinstance(rate_limit, int):
            # æ•´æ•°ï¼šæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼ˆé€‚åˆ API æ–‡æ¡£å¦‚ 100 æ¬¡/åˆ†é’Ÿï¼‰
            limiter = AsyncLimiter(rate_limit, 60)
        elif isinstance(rate_limit, (float, int)):
            # æµ®ç‚¹æ•°ï¼šæ¯ç§’è¯·æ±‚æ•°ï¼ˆå¦‚ 5.0 è¡¨ç¤º 5 QPSï¼‰
            limiter = AsyncLimiter(rate_limit, 1)
    
    # åŠ è½½ä»»åŠ¡é…ç½®
    if prompt is None or output_format is None:
        if task not in TASKS:
            available = ", ".join(TASKS.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡: {task}ï¼Œå¯ç”¨ä»»åŠ¡: {available}")
        config = TASKS[task]
        prompt = prompt or config["prompt"]
        output_format = output_format or config["output_format"]

    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    aclient = instructor.from_openai(
        AsyncOpenAI(base_url=base_url, api_key=api_key, timeout=30),
        mode=instructor.Mode.MD_JSON,
    )

    # æ„å»º Pydantic æ¨¡å‹
    type_map = {
        'str': str, 'int': int, 'float': float,
        'bool': bool, 'list[str]': List[str], List[str]: List[str]
    }
    fields = {}
    for k, v in output_format.items():
        typ = type_map.get(v.lower() if isinstance(v, str) else v, v)
        fields[k] = (typ, ...)
    ResponseModel = create_model('ResponseModel', **fields)

    # å•ä¸ªè¯·æ±‚åç¨‹ï¼ˆåªå¤„ç†å­—ç¬¦ä¸²ï¼‰
    async def _call(text: str):
        try:
            if limiter:
                await limiter.acquire()
            resp = await aclient.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text}
                ],
                response_model=ResponseModel,
                temperature=temperature,
                max_retries=max_retries,
            )
            result = resp.model_dump()
        except Exception as e:
            return {"error": str(e), "text": text}
        return result

    # å¹¶å‘æ‰§è¡Œ
    tasks = [_call(text) for text in inputs]
    return await asyncio.gather(*tasks, return_exceptions=False)


# ======================
# ä¸»å‡½æ•° llmï¼ˆä»…æ”¯æŒ str æˆ– List[str]ï¼‰
# ======================

def llm(
    text: Union[str, List[str]],  # æ˜ç¡®åªæ”¯æŒ str å’Œ List[str]
    task: str = "sentiment",
    prompt: Optional[str] = None,
    output_format: Optional[Dict[str, Any]] = None,
    backend: Optional[str] = "ollama",
    base_url: Optional[str] = None,
    api_key: str = "",
    model_name: str = "qwen2.5:3b",
    temperature: float = 0,
    max_retries: int = 3,
    rate_limit: Optional[Union[int, float]] = None,  # å¦‚ 100ï¼ˆæ¬¡/åˆ†é’Ÿï¼‰æˆ– 5.0ï¼ˆQPSï¼‰
    return_df: bool = False,
    verbose: bool = True,
) -> Union[Dict[str, Any], List[Dict[str, Any]], pd.DataFrame]:
    """
    è°ƒç”¨å¤§æ¨¡å‹æ‰§è¡Œç»“æ„åŒ–æ–‡æœ¬åˆ†æä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æã€å…³é”®è¯æå–ã€åˆ†ç±»ç­‰ï¼‰ã€‚

    æ”¯æŒï¼š
    - æœ¬åœ°æ¨¡å‹ï¼šOllama (11434), LM Studio (1234)
    - è¿œç¨‹æœåŠ¡ï¼šé˜¿é‡Œäº‘ã€ç™¾åº¦åƒå¸†ã€è‡ªå»ºAPIç­‰ï¼ˆé€šè¿‡ base_urlï¼‰

    Args:
        text (str): å¾…åˆ†æçš„æ–‡æœ¬å†…å®¹
        task (str): é¢„è®¾ä»»åŠ¡åç§°ï¼Œé»˜è®¤ä¸º 'sentiment'ã€‚å¯ç”¨ä»»åŠ¡è§ TASKS.keys()
        backend (str, optional): å¿«æ·åç«¯åˆ«åï¼š
            - 'ollama' â†’ http://127.0.0.1:11434/v1
            - 'lmstudio' æˆ– 'lms' â†’ http://localhost:1234/v1
            - None â†’ éœ€é…åˆ base_url ä½¿ç”¨
        base_url (str, optional): è‡ªå®šä¹‰æ¨¡å‹æœåŠ¡åœ°å€ï¼Œä¼˜å…ˆçº§é«˜äº backend
            ç¤ºä¾‹ï¼š
            - è¿œç¨‹ï¼šhttps://dashscope.aliyuncs.com/compatible-mode/v1
            - å†…ç½‘ï¼šhttp://192.168.1.10:11434/v1
            - æœ¬åœ°ï¼šhttp://localhost:1234/v1
        api_key (str): API å¯†é’¥ï¼Œè¿œç¨‹æœåŠ¡å¿…å¡«ï¼Œæœ¬åœ°é€šå¸¸ä¸º "EMPTY"
        model_name (str): æ¨¡å‹åç§°ï¼ˆéœ€æœåŠ¡ç«¯å·²åŠ è½½ï¼‰
        temperature (float): ç”Ÿæˆæ¸©åº¦ï¼Œ0 è¡¨ç¤ºç¡®å®šæ€§è¾“å‡º
        max_retries (int): å¤±è´¥é‡è¯•æ¬¡æ•°
        rate_limit (int or float, optional): é™é€Ÿ
            - int: æ¯åˆ†é’Ÿæœ€å¤šè¯·æ±‚æ•°ï¼Œå¦‚ 100
            - float: æ¯ç§’æœ€å¤šè¯·æ±‚æ•°ï¼ˆQPSï¼‰ï¼Œå¦‚ 5.0
        return_df (bool): æ˜¯å¦è¿”å› DataFrame
        verbose (bool): æ˜¯å¦è¾“å‡ºè¿æ¥ä¿¡æ¯
        prompt (str, optional): è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯­
        output_format (dict, optional): è‡ªå®šä¹‰è¾“å‡ºç»“æ„ï¼Œå¦‚ {'label': str, 'score': float}

    Returns:
        dict or pd.DataFrame: ç»“æ„åŒ–ç»“æœï¼Œå¦‚ {'label': 'pos', 'score': 0.95}

    Example:
        # æœ¬åœ° Ollama
        llm("æœåŠ¡å¾ˆæ£’", backend="ollama", model_name="qwen2.5:3b")

        # é˜¿é‡Œäº‘é€šä¹‰åƒé—®
        llm("æœåŠ¡å¾ˆæ£’",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            api_key="sk-xxx",
            model_name="qwen-plus")

        # è‡ªå®šä¹‰ä»»åŠ¡
        llm("æ€»ç»“è¿™æ®µè¯",
            prompt="è¯·ç”Ÿæˆä¸€å¥è¯æ‘˜è¦",
            output_format={"summary": str},
            base_url="http://127.0.0.1:11434/v1")
    """
    # ====== 1. ç¡®å®š base_url ======
    final_base_url = base_url
    if final_base_url is None:
        if backend == "ollama":
            final_base_url = "http://127.0.0.1:11434/v1"
        elif backend in ("lmstudio", "lms"):
            final_base_url = "http://localhost:1234/v1"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ backend: {backend}ã€‚è¯·æä¾› base_url")
    else:
        if verbose:
            _quiet_print(f"ğŸŒ ä½¿ç”¨è‡ªå®šä¹‰ base_url: {final_base_url}", once=True)

    if verbose:
        _quiet_print(f"âœ… è¿æ¥æ¨¡å‹æœåŠ¡: {final_base_url}", once=True)

    # ====== 2. è¾“å…¥ç±»å‹æ£€æŸ¥ ======
    if isinstance(text, str):
        is_single = True
        inputs = [text]
    elif isinstance(text, list):
        if not all(isinstance(t, str) for t in text):
            raise TypeError("å½“è¾“å…¥ä¸ºåˆ—è¡¨æ—¶ï¼Œæ‰€æœ‰å…ƒç´ å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
        is_single = False
        inputs = text
    else:
        raise TypeError("text å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨")

    # ====== 3. å¼‚æ­¥ä¸»å‡½æ•° ======
    async def main():
        return await _llm_async_batch(
            inputs=inputs,
            task=task,
            prompt=prompt,
            output_format=output_format,
            base_url=final_base_url,
            api_key=api_key or "EMPTY",
            model_name=model_name,
            temperature=temperature,
            max_retries=max_retries,
            rate_limit=rate_limit
        )

    # ====== 4. æ‰§è¡Œ ======
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None

    if loop is None or not loop.is_running():
        results = asyncio.run(main())
    else:
        results = asyncio.run(main())  # nest_asyncio å·² apply

    # ====== 5. è¿”å›æ ¼å¼å¤„ç† ======
    if return_df:
        return pd.DataFrame(results)
    return results[0] if is_single else results


# ======================
# è€å‡½æ•°ï¼šè½¯å¼ƒç”¨ï¼ˆå…¼å®¹æ—§ç”¨æˆ·ï¼‰
# ======================

def text_analysis_by_llm(*args, **kwargs):
    warnings.warn("å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ ct.llm()", DeprecationWarning, stacklevel=2)
    return llm(*args, **kwargs)

def analysis_by_llm(*args, **kwargs):
    warnings.warn("å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ ct.llm()", DeprecationWarning, stacklevel=2)
    return llm(*args, **kwargs)


# ======================
# ä»»åŠ¡æ¨¡æ¿ï¼ˆä¿æŒä¸å˜ï¼‰
# ======================







llm.tasks_list = lambda: list(TASKS.keys())
llm.tasks_get = lambda name: TASKS.get(name) or _raise(...)


# æˆ–æ›´ä¼˜é›…ï¼šå°è£…ä¸ºå¯¹è±¡
class _LLMTasks:
    @staticmethod
    def list():
        return list(TASKS.keys())
    
    @staticmethod
    def get(name: str):
        if name not in TASKS:
            available = ", ".join(TASKS.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡: {name}ï¼Œå¯ç”¨ä»»åŠ¡: {available}")
        return TASKS[name]
    
    @staticmethod
    def show(name: str):
        return _show_task(name)

llm.tasks = _LLMTasks()


#10 ä¸ªä¸“ä¸ºä¸­æ–‡åœºæ™¯è®¾è®¡çš„ç»“æ„åŒ–ä»»åŠ¡æ¨¡æ¿
TASKS = {
    "sentiment": {
        "prompt": "åˆ†æè¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼šè¿”å›æƒ…æ„Ÿç±»åˆ« labelï¼ˆpos è¡¨ç¤ºæ­£é¢ï¼Œneg è¡¨ç¤ºè´Ÿé¢ï¼Œneutral è¡¨ç¤ºä¸­æ€§ï¼‰å’Œæƒ…æ„Ÿåˆ†å€¼ scoreï¼ˆå–å€¼èŒƒå›´ -1~1ï¼Œè´Ÿæ•°ä¸ºè´Ÿé¢ï¼‰ã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'label': 'pos', 'score': 0.5}",
        "output_format": {
            "label": "str",
            "score": "float"
        }
    },
    "emotion": {
        "prompt": "è¯†åˆ«æ–‡æœ¬ä¸­çš„ä¸»è¦æƒ…ç»ªç±»å‹ï¼šä» [å¼€å¿ƒ, æ„¤æ€’, æ‚²ä¼¤, æƒŠè®¶, åŒæ¶, ææƒ§, ä¸­æ€§] ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€é¡¹ï¼Œè¿”å›æƒ…ç»ªç±»å‹ emotion å’Œç½®ä¿¡åº¦ confidenceï¼ˆ0~1ï¼‰ã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'emotion': 'å¼€å¿ƒ', 'confidence': 0.8}",
        "output_format": {
            "emotion": "str",
            "confidence": "float"
        }
    },
    "classify": {
        "prompt": "å°†æ–‡æœ¬åˆ†ç±»åˆ°æœ€åŒ¹é…çš„ç±»åˆ«ä¸­ï¼šå¯é€‰ç±»åˆ«ä¸º [ç§‘æŠ€, ä½“è‚², å¨±ä¹, è´¢ç», æ•™è‚², åŒ»ç–—, å†›äº‹, å…¶ä»–]ï¼Œè¿”å›åˆ†ç±» category å’Œç®€è¦ç†ç”± reasonã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'category': 'ç§‘æŠ€', 'reason': 'æ–‡æœ¬ä¸­å‡ºç°å¤šæ¬¡ITã€AIç­‰ç‰¹å¾è¯ï¼Œå› æ­¤æ ‡è®°ä¸ºç§‘æŠ€ã€‚'}",
        "output_format": {
            "category": "str",
            "reason": "str"
        }
    },
    "intent": {
        "prompt": "è¯†åˆ«ç”¨æˆ·çš„æ„å›¾ï¼šä» [å’¨è¯¢, æŠ•è¯‰, è¡¨æ‰¬, è´­ä¹°, å»ºè®®, å…¶ä»–] ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€é¡¹ï¼Œè¿”å›æ„å›¾ intent å’Œç½®ä¿¡åº¦ confidenceï¼ˆ0~1ï¼‰ã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'intent': 'å’¨è¯¢', 'confidence': 0.8'}",
        "output_format": {
            "intent": "str",
            "confidence": "float"
        }
    },
    "keywords": {
        "prompt": "æå–æ–‡æœ¬ä¸­æœ€ç›¸å…³çš„ 3 ä¸ªå…³é”®è¯ï¼ŒæŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ’åºï¼Œè¿”å›å…³é”®è¯åˆ—è¡¨ keywordsã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'keywords': ['IT', 'ç§‘æŠ€', 'AI']}",
        "output_format": {
            "keywords": "list[str]"
        }
    },
    "entities": {
        "prompt": "æå–æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€ç»„ç»‡åï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºåˆ—è¡¨ï¼Œè¿”å› personsï¼ˆäººåï¼‰ã€locationsï¼ˆåœ°åï¼‰ã€organizationsï¼ˆç»„ç»‡åï¼‰ã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'persons': ['å¼ ä¸‰', 'æå››'], 'locations': ['åŒ—äº¬', 'ä¸Šæµ·'], 'organizations': ['å…¬å¸A', 'å…¬å¸B']}",
        "output_format": {
            "persons": "list[str]",
            "locations": "list[str]",
            "organizations": "list[str]"
        }
    },
    "summarize": {
        "prompt": "ç”¨ä¸€å¥è¯æ€»ç»“æ–‡æœ¬å†…å®¹ï¼Œä¸è¶…è¿‡ 30 ä¸ªæ±‰å­—ï¼Œè¿”å›æ‘˜è¦ summaryã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'summary': 'è¿™æ˜¯ä¸€ä¸ªå…³äºITçš„æ–‡ç« ï¼Œä¸»è¦ä»‹ç»äº†AIçš„å‘å±•ã€‚'}",
        "output_format": {
            "summary": "str"
        }
    },
    "rewrite": {
        "prompt": "ç”¨æ›´ç®€æ´ã€æµç•…çš„æ–¹å¼é‡å†™è¯¥å¥ï¼Œä¿æŒåŸæ„ï¼Œè¿”å›æ”¹å†™åæ–‡æœ¬ rewrittenã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'rewritten': 'è¿™æ˜¯ä¿®æ”¹åçš„å†…å®¹: ã€‚ã€‚ã€‚'}",
        "output_format": {
            "rewritten": "str"
        }
    },
    "quality": {
        "prompt": "å¯¹æ–‡æœ¬è´¨é‡è¿›è¡Œè¯„åˆ†ï¼ˆ0~1ï¼‰ï¼šç»¼åˆè€ƒè™‘é€»è¾‘æ€§ã€è¡¨è¾¾æ¸…æ™°åº¦å’Œä¿¡æ¯å®Œæ•´æ€§ï¼Œè¿”å›è¯„åˆ† score å’Œç®€è¦åé¦ˆ feedbackã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'score': 0.8, 'feedback': 'æ–‡æœ¬é€»è¾‘æ€§å¼ºï¼Œè¡¨è¾¾æ¸…æ™°ï¼Œä¿¡æ¯å®Œæ•´ï¼Œç»™å‡º0.8åˆ†ã€‚'}",
        "output_format": {
            "score": "float",
            "feedback": "str"
        }
    },
    "similarity": {
        "prompt": "åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆ-1~1ï¼‰ï¼š-1 è¡¨ç¤ºå®Œå…¨ç›¸åï¼Œ0 è¡¨ç¤ºæ— å…³ï¼Œ1 è¡¨ç¤ºå‡ ä¹ç›¸åŒï¼Œè¿”å›ç›¸ä¼¼åº¦ similarity å’Œåˆ¤æ–­ç†ç”± reasonã€‚ç»“æœè¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{'similarity': 0.8, 'reason': 'è¿™ä¸¤æ®µæ–‡æœ¬çš„å†…å®¹éå¸¸ç›¸ä¼¼ï¼Œéƒ½åœ¨è®¨è®ºITé¢†åŸŸçš„å‘å±•ã€‚'}",
        "output_format": {
            "similarity": "float",
            "reason": "str"
        }
    }
}