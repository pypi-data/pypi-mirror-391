Metadata-Version: 2.4
Name: dsf-aml-sdk
Version: 2.1.11
Summary: DSF AML SDK â€” Automated ML Robustness & Failure Correction Framework
Home-page: https://dsfuptech.cloud
Author: Jaime Alexander Jimenez
Author-email: contacto@dsfuptech.cloud
License: Proprietary
Keywords: dsf aml ml machine-learning robustness auto-healing sdk
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Requires-Dist: requests>=2.25.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

DSF AML SDK
Fix ML failures in production â†’ Generate critical variants automatically â†’ Retrain in <30s
ðŸŽ¯ Primary Use Cases
1. ðŸ”¥ Failure Correction Loop (Production-Ready)
Problem: When your LLM/ML model fails in production (adversarial prompt, edge case), fixing it is slow and manual.
Solution: API takes the failure as seed â†’ generates critical variants automatically â†’ delivers labeled dataset for retraining.
pythonfrom dsf_aml_sdk import AMLSDK, Config

sdk = AMLSDK(license_key='PRO-XXXX', tier='professional')

# Production failure detected
failed_case = {'input': 'malicious prompt', 'context': {...}}

# Generate variants for retraining
variants = sdk.fix_failure(
    failed_input=failed_case,
    config=your_config,
    k_variants=20
)

# Retrain your model with variants['critical_samples']
# â†’ Model now robust to similar attacks
ðŸ’¡ What are variants? Each variant represents a new, labeled data point similar to the failure case, designed to improve robustness during retraining.
Differentiator: Automated end-to-end in <30s. No ML expertise required. No direct competitors.

2. ðŸ›¡ï¸ Preventive Robustness Datasets
Problem: Models trained on clean data fail on edge cases and adversarial inputs.
Solution: Pre-generate datasets focused on decision boundaries before deployment.
pythonfrom dsf_aml_sdk import AMLSDK, Config

sdk = AMLSDK(license_key='PRO-XXXX', tier='professional')

# Before deployment: curate critical training data
seeds = sdk.pipeline_identify_seeds(
    dataset=training_data,
    config=config,
    top_k_percent=0.1  # Focus on hardest 10%
)

# Generate synthetic variants at boundaries
critical_data = sdk.pipeline_generate_critical(
    config=config,
    original_dataset=training_data,
    k_variants=10,
    epsilon=0.08
)

# Train model with critical_data â†’ 70-90% less data, more robust
Differentiator: Automated uncertainty-based curation. No packaged competitor.

3. ðŸ“Š Labeled Training Data Generation
Problem: Creating high-quality labeled datasets for ML training is expensive and time-consuming.
Solution: Generate synthetic labeled datasets at scale for training your own models.
pythonfrom dsf_aml_sdk import AMLSDK, Config

sdk = AMLSDK(license_key='PRO-XXXX', tier='professional')

# Generate 1000 labeled samples
result = sdk.distill_train(config, samples=1000)

# Export for training (Enterprise)
dataset = sdk.distill_export()

# Use dataset['distilled_samples'] to train YOUR model
# Each sample contains: features + target_score
# â†’ Train sklearn, PyTorch, TensorFlow models with 70-90% less data
ðŸ’¡ What you receive: A ready-to-use dataset with pre-computed labels based on your config. Perfect for training lightweight surrogates or custom ML models.
Differentiator: Automated data generation focused on critical regions. 10-50Ã— faster training with minimal data.

ðŸš€ Why DSF AML?
Competitive landscape gaps:

Scale AI/Gretel: Generic synthetic data (no failure focus)
Cleanlab/Snorkel: Clean existing data (don't generate variants)
Lakera/Giskard: Test post-deployment (don't generate retraining data)

Your position: Only automated pipeline from failure â†’ variants â†’ labeled datasets.

ðŸ“¦ Installation
bashpip install dsf-aml-sdk

ðŸŽ¯ Quick Start: Failure Correction
pythonfrom dsf_aml_sdk import AMLSDK, Config

sdk = AMLSDK(license_key='PRO-2026-12-31-XXXX', tier='professional')

# ðŸ’¡ Config defines metrics your model evaluates (accuracy, latency, etc.)
config = (sdk.create_config()
    .add_field('model_accuracy', 0.95, 2.5, 2.0)
    .add_field('latency_ms', 100, 1.8, 1.5)
    .add_field('error_rate', 0.05, 2.2, 2.5)
)

# 1. Report failure
failed_input = {'model_accuracy': 0.60, 'latency_ms': 500, 'error_rate': 0.20}

# 2. Generate correction variants
fix = sdk.fix_failure(failed_input, config)

# 3. Use fix['critical_samples'] to retrain
print(f"Generated {len(fix['critical_samples'])} variants for retraining")

ðŸ†š Tier Comparison

| Feature                | Community         |   Professional     |    Enterprise       |
|------------------------|------------------:|-------------------:|--------------------:|
| Failure correction     | âŒ                |  âœ…               | âœ… (unlimited)      |
| Preventive datasets    | Limited (100/day) |  âœ…                | âœ… (full pipeline)  |
| Batch evaluation       | âŒ                |  âœ… (â‰¤1000)       | âœ… (â‰¤1000)          |
| Distillation           | âŒ                |  âœ…               | âœ… + export         |
| Full cycle pipeline    | âŒ                |  âŒ               | âœ…                  |

ðŸ“– Core Methods
Failure Correction
pythonsdk.fix_failure(failed_input: dict, config, k_variants=20) â†’ dict
Preventive Datasets
pythonsdk.pipeline_identify_seeds(dataset, config, top_k_percent=0.1) â†’ dict
sdk.pipeline_generate_critical(config, original_dataset, **kwargs) â†’ dict
Labeled Data Generation
pythonsdk.distill_train(config, samples=1000) â†’ dict  # Generate labeled dataset
sdk.distill_export() â†’ dict  # Export for training (Enterprise)

ðŸ“ˆ ROI Metrics
Failure Correction

â± Time to fix: Hours â†’ <30s
ðŸ§© Downtime reduction: 80â€“95%

Preventive Datasets

ðŸ“‰ Data volume reduction: 70â€“90%
ðŸ§  Model robustness: +40â€“60%
ðŸ’° Infrastructure savings: ~85%

Labeled Data Generation

ðŸ“Š Data generation speed: 10-50Ã— faster than manual labeling
ðŸŽ¯ Training efficiency: 70-90% less data needed
ðŸ’¸ Cost savings: ~90% vs. human labeling


ðŸ“ž Support

Docs: https://dsfuptech.cloud
Issues: contacto@dsfuptech.cloud
Enterprise: contacto@dsfuptech.cloud


ðŸ“„ License
Proprietary â€” Â© 2025 Jaime Alexander Jimenez, operating as "Uptech"
DSF AML SDK and DSF Quantum SDK are licensed software products.
Community tier for non-commercial use. Professional and Enterprise require valid license key.
