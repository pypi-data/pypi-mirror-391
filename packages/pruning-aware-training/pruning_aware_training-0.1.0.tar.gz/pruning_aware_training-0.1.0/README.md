# Pruning Aware Training

[![Build Status](https://img.shields.io/github/actions/workflow/status/<your-username>/<your-repo-name>/tests.yml?branch=main&label=build)](https://github.com/<your-username>/<your-repo-name>/actions)
[![PyPI](https://img.shields.io/pypi/v/pruning-aware-training?color=blue)](https://pypi.org/project/pruning-aware-training/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://<your-docs-link>)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.xxxxx.svg)](https://doi.org/10.5281/zenodo.xxxxx)

---

**Pruning Aware Training (PAT)** is an open-source, easy-to-integrate framework for **structured (channel) pruning** in PyTorch.  
It enables researchers and practitioners to accelerate deep neural networks by removing redundant channels with minimal loss in task performance.

---

## ğŸŒŸ Key Features

- **Structured Channel Pruning:**  
  Removes whole channels (filters) for real acceleration and model compression.

- **Config-Driven Workflows:**  
  Define pruning behaviour entirely through JSON/YAML configuration files â€” reproducible and shareable.

- **Pruning-Aware Regularization:**  
  Apply channel regularization during training to promote sparsity before pruning.

- **MAC-Aware Scheduling:**  
  Automatically track and prune toward a target compute budget (e.g., 70% MAC reduction).

- **Plug-and-Play Integration:**  
  Works directly with standard PyTorch training loops and models â€” no need to modify architectures.

---

## ğŸ¯ Objectives

- **Reduce Model Size:**  
  Identify and prune redundant parameters while maintaining accuracy.

- **Accelerate Inference:**  
  Lower computational cost for efficient deployment on edge devices, servers, or mobile platforms.

- **Ensure Reproducibility:**  
  Every pruning run is configuration-driven and fully logged for transparent experiments.

---

## ğŸ§© Repository Structure

```

PruningAwareTraining/
â”‚
â”œâ”€â”€ torch_pruning             # Core code
â”œâ”€â”€â”€â”€â”€â”€ pruning_utils.py      # Core orchestration module for initialization, regularization, and pruning
â”œâ”€â”€â”€â”€â”€â”€ pruner/               # Importance metrics and pruning algorithms (Taylor, magnitude, Hessian)
â”œâ”€â”€â”€â”€â”€â”€ load_pruned_model.py  # Allowing fluent save and load masked and pruned models
â”œâ”€â”€ tests/                    # Unit and integration tests for correctness and stability
â”œâ”€â”€ examples/                 # Ready-to-run scripts for different model architectures
â””â”€â”€ reproduce/                # Documantation, Reference experiments and configuration files

````

Full documentation and examples are in  
[`Documentation/README.md`](Documentation/README.md).

---

## âš™ï¸ Installation

### Option 1 â€” Pip (recommended)
```bash
pip install git+https://github.com/AvrahamRaviv/PruningAwareTraining.git
````

### Option 2 â€” Local Development

```bash
git clone https://github.com/AvrahamRaviv/PruningAwareTraining.git
cd <your-repo-name>
pip install -e .
```

---

## ğŸš€ Quick Start

```python
from torch_pruning.pruning_utils import Pruning

# 1. Initialize the pruner
pruner = Pruning(model, output_dir="checkpoints", device=device)

# 2. Apply pruning-aware regularization after each backward pass
loss.backward()
pruner.channel_regularize(model)

# 3. Prune at scheduled epochs
for epoch in range(num_epochs):
    pruner.prune(model, epoch)
```

---

## ğŸ§ª Example Config (JSON)

```json
{
  "start_epoch": 5,
  "end_epoch": 50,
  "interval": 5,
  "global_sparsity": 0.5,
  "mac_target": 0.7,
  "layers_to_prune": ["conv1", "layer2", "layer3"]
}
```

---
## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
Please open a pull request or report issues via the [GitHub Issues](../../issues) page.

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ§­ Acknowledgements

Built upon [Torch-Pruning](https://github.com/VainF/Torch-Pruning) and inspired by its DepGraph architecture.
Special thanks to collaborators and the research community for shaping this work.

---

