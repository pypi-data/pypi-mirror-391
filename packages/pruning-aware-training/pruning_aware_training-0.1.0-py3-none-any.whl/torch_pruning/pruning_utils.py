import os
import json
from functools import partial
from typing import Any, Dict, Optional, List

import torch
import torch.nn as nn
import torch_pruning as tp
import re


class Pruning:
    """
    High-level pruning interface combining channel and slice pruning.
    """

    def __init__(self, model: nn.Module, config_folder: str, forward_fn: Optional[Any] = None,
                 log: Optional[Any] = None, device: Optional[torch.device] = None) -> None:
        try:
            config_path = os.path.join(config_folder, "pruning_config.json")
            with open(config_path, "r") as f:
                sparsity_args = json.load(f)
            channel_sa = sparsity_args['channel_sparsity_args']
            slice_sa = sparsity_args['slice_sparsity_args']
            if log is not None:
                log.info("=> Init pruner module")
            else:
                print("=> Init pruner module")
        except Exception as e:
            if os.path.exists(os.path.join(config_folder, "pruning_config.json")):
                print("There is pruning_config.json in output folder, but it is failed with the following error:")
                print(e)
            else:
                print("There is no pruning_config.json in output folder")
            channel_sa, slice_sa = None, None
            if log is not None:
                log.info("=> Unable to find a valid pruning configuration.")
            else:
                print("=> Unable to find a valid pruning configuration.")

        self.channel_pruner = channel_pruning(channel_sa, model, config_folder, forward_fn, log, device)
        self.slice_pruner = slice_pruning(slice_sa, model, log)
        # Synchronize slice block size and channel mask dictionary between pruners.
        self.channel_pruner.slice_block_size = self.slice_pruner.block_size
        self.slice_pruner.channel_mask_dict = self.channel_pruner.channel_mask_dict

    def channel_regularize(self, model: nn.Module) -> None:
        """Apply channel regularization to the model."""
        self.channel_pruner.regularize(model)

    def slice_regularize(self, model: nn.Module) -> torch.Tensor:
        """Apply slice regularization to the model."""
        return self.slice_pruner.regularize(model)

    def prune(self, model: nn.Module, epoch: int, log: Optional[Any] = None,
              mask_only: bool = True) -> None:
        """Perform pruning using both channel and slice pruners."""
        self.channel_pruner.prune(model, epoch, log=log, mask_only=mask_only)
        self.slice_pruner.prune(model, epoch, log=log)


class channel_pruning:
    """
    Implements channel pruning using a chosen pruning method.
    """
    def __init__(self, channel_sparsity_args: Optional[Dict[str, Any]], model: nn.Module,
                 config_folder: str, forward_fn: Any, log: Optional[Any] = None,
                 device: Optional[torch.device] = None) -> None:
        self.channel_mask_dict: Dict[str, torch.Tensor] = {}
        if channel_sparsity_args is None:
            if log is not None:
                log.info("=> Unable to find a valid channel pruning configuration.")
            else:
                print("=> Unable to find a valid channel pruning configuration.")
            self.prune_channels = False
            self.prune_channels_at_init = False
            self.reach_mac_target = False
            return

        self.prune_channels = channel_sparsity_args.get('is_prune', True)
        self.channel_sparsity_args = channel_sparsity_args
        self.device = device
        self.infer = self.channel_sparsity_args['infer']

        # Example inputs for forward pass estimation.
        self.example_inputs = build_inputs(channel_sparsity_args, device=device)
        if self.infer:
            self.example_inputs = (self.example_inputs[0][:, :1], self.example_inputs[0][:, :1])
        self.current_epoch: int = 0
        self.current_step = None
        self.current_pr: float = 0.0
        self.ignored_layers: List[Any] = []
        self.start_epoch: int = self.channel_sparsity_args['start_epoch']
        self.end_epoch: int = self.channel_sparsity_args['end_epoch']
        self.epoch_rate: int = self.channel_sparsity_args['epoch_rate']
        self.global_prune_rate: float = self.channel_sparsity_args['global_prune_rate']
        self.layers_to_prune = self.channel_sparsity_args['layers']
        self.mac_target: float = self.channel_sparsity_args.get('mac_target', 0.0)
        self.reach_mac_target = self.channel_sparsity_args.get("reach_mac_target", False)
        self.log_str = self.channel_sparsity_args.get("log_str", None)
        self.prune_channels_at_init = self.channel_sparsity_args['prune_channels_at_init'] or self.infer

        self.channels_pruner_args = {
            "pruning_method": self.channel_sparsity_args['pruning_method'],
            "global_pruning": self.channel_sparsity_args['global_pruning'],
            "round_to": self.channel_sparsity_args['block_size'],
            "reg": self.channel_sparsity_args['regularize']['reg'],
            "mac_reg": self.channel_sparsity_args['regularize']['mac_reg'],
            "gamma_reg":  self.channel_sparsity_args['regularize'].get("gamma", 1),
            "alpha_shrinkage_reg":  self.channel_sparsity_args['regularize'].get("alpha_shrinkage_reg", 4),
            "max_pruning_rate": self.channel_sparsity_args['max_pruning_rate'],
            "MAC_params": self.channel_sparsity_args['MAC_params'],
            "isomorphic": self.channel_sparsity_args.get("isomorphic", False)
        }
        self.pruner = None
        self.regularization = None
        self.model_forward_fn = forward_fn
        self.init_channel_mask: Dict[Any, Any] = {}
        self.MACs_per_layer: Dict[str, List[float]] = {}
        self.channels_pruner_args["current_round_to"] = 1
        _ = self.measure_macs_masked_model(model)
        self.config_folder = config_folder
        self.max_imp_current_step = torch.tensor(0.0, device=device)
        self.slice_block_size = None
        self.prune_at_target = self.channel_sparsity_args.get("prune_at_target", False)
        self.reset_optimizer = False
        self.verbose = self.channel_sparsity_args.get("verbose", 1)
        self.init_channel_pruner(model, log, print_layers=True)

    def init_channel_pruner(self, model, log=None, print_layers=False):
        # set layers to pruned and their pruning rate
        pruning_ratio_dict = self.set_layers_to_prune(model)

        # set pruning method
        if self.channels_pruner_args['pruning_method'] == 'BNScalePruner':
            imp = tp.importance.BNScaleImportance()
            pruner_entry = partial(tp.pruner.BNScalePruner, group_lasso=True)
        elif self.channels_pruner_args['pruning_method'] == 'GroupNormPruner':
            imp = tp.importance.GroupNormImportance(p=2, gamma=self.channels_pruner_args["gamma_reg"])
            pruner_entry = partial(tp.pruner.GroupNormPruner)
        elif self.channels_pruner_args['pruning_method'] == 'MACAwareImportance':
            L_MACs = {k: v[0] for k, v in self.MACs_per_layer.items()}
            imp = MACAwareImportance(p=2, layers_mac=L_MACs,
                                     params=self.channels_pruner_args["MAC_params"],
                                     current_max=self.max_imp_current_step)
            pruner_entry = partial(tp.pruner.GroupNormPruner)
        else:
            raise NameError(f'Unsupported pruner method. {self.channels_pruner_args["pruning_method"]}')

        # init pruner
        grad_d = {}
        for n, m in model.named_parameters():
           grad_d[n] = m.requires_grad
        self.pruner = pruner_entry(
            model,
            example_inputs=self.example_inputs,
            importance=imp,
            ignored_layers=self.ignored_layers,
            pruning_ratio=self.current_pr,
            pruning_ratio_dict=pruning_ratio_dict if not self.channels_pruner_args["global_pruning"] else None,
            global_pruning=self.channels_pruner_args["global_pruning"],
            round_to=self.channels_pruner_args["current_round_to"],
            reg=self.channels_pruner_args["reg"],
            max_pruning_ratio=self.channels_pruner_args["max_pruning_rate"],
            forward_fn=self.model_forward_fn,
            isomorphic=self.channels_pruner_args["isomorphic"]
        )
        for n, m in model.named_parameters():
            m.requires_grad = grad_d[n]

        # init regularizer
        self.pruner.update_regularizer()

        if self.verbose > 0:
            if log is not None:
                log.info(f"Epoch {self.current_epoch}, pruning progress:")
                log.info(f"Pruning from epoch {self.start_epoch} to epoch {self.end_epoch}, with a current pruning rate of {self.current_pr:.3f}.")
                log.info(f"Total target: {self.global_prune_rate}, Pruning algorithm: {self.channels_pruner_args['pruning_method']}.")
                if self.channels_pruner_args["round_to"] > 1:
                    log.info(f"Target round_to: {self.channels_pruner_args['round_to']}, Current round_to: {self.channels_pruner_args['current_round_to']}")
                if print_layers:
                    num_groups = 0
                    source_convs = []
                    log.info("*************")
                    for group in self.pruner.DG.get_all_groups(ignored_layers=self.pruner.ignored_layers,
                                                               root_module_types=self.pruner.root_module_types):
                        log.info(f"group number {num_groups + 1}:")
                        log.info(f"Source conv: {group[0].dep.source.name}")
                        source_convs.append(group[0].dep.source.name.split(" ")[0])
                        if any([isinstance(_gt.dep.layer, torch.nn.Conv2d) for _gt in group]):
                            log.info(f"Dependencies:")
                            for _g in group:
                                if isinstance(_g.dep.layer, torch.nn.Conv2d):
                                    log.info(str(_g.dep)[str(_g.dep).index("=>") + 2:].strip())
                        log.info("*************\n")
                        num_groups += 1
                    log.info(f"There are {num_groups} groups of layers, with the following source convs:\n{source_convs}")
            else:
                print(f"Epoch {self.current_epoch}, pruning progress:")
                print(f"Pruning from epoch {self.start_epoch} to epoch {self.end_epoch}, with a current pruning rate of {self.current_pr:.3f}.")
                print(f"Total target: {self.global_prune_rate}, Pruning algorithm: {self.channels_pruner_args['pruning_method']}.")
                if self.channels_pruner_args["round_to"] > 1:
                    print(f"Target round_to: {self.channels_pruner_args['round_to']}, Current round_to: {self.channels_pruner_args['current_round_to']}")
                if print_layers:
                    num_groups = 0
                    source_convs = []
                    print("*************")
                    for group in self.pruner.DG.get_all_groups(ignored_layers=self.pruner.ignored_layers,
                                                               root_module_types=self.pruner.root_module_types):
                        print(f"group number {num_groups + 1}:")
                        print(f"Source conv: {group[0].dep.source.name}")
                        source_convs.append(group[0].dep.source.name.split(" ")[0])
                        if any([isinstance(_gt.dep.layer, torch.nn.Conv2d) for _gt in group]):
                            print(f"Dependencies:")
                            for _g in group:
                                if isinstance(_g.dep.layer, torch.nn.Conv2d):
                                    print(str(_g.dep)[str(_g.dep).index("=>") + 2:].strip())
                        print("*************\n")
                        num_groups += 1
                    print(f"There are {num_groups} groups of layers, with the following source convs:\n{source_convs}")

    def prune(self, model, epoch, log=None, mask_only=True, step=None):
        """
            Prune the model
            We are supporting two modes: 1. Physically prune channels. 2. Mask with zeros.
            It is handaled by mask_only flag.
        """
        if not self.prune_channels:
            self.update_channel_mask_dict(model)
            if self.reach_mac_target and self.verbose > 0:
                if log is not None:
                    log.info(f" Model already reach {self.mac_target} MACs reduction")
                    log.info(f" Pruning statistics:")
                    for line in self.log_str.split("\n")[:-1]:
                        log.info(f" {line}")
                else:
                    print(f" Model already reach {self.mac_target} MACs reduction")
                    print(f" Pruning statistics:")
                    for line in self.log_str.split("\n")[:-1]:
                        print(f" {line}")
            return
        self.current_epoch = epoch
        self.log_str = ""
        if self.start_epoch <= epoch:
            self.current_step = step
            self.init_channel_pruner(model, log)
            self.update_max_imp()
            for group in self.pruner.step(interactive=True):
                # print(group)
                dep, idxs = group[0]
                dep_str = str(dep)
                if len(idxs) > 0:
                    mask_only = mask_only and not self.prune_channels_at_init and not self.reach_mac_target
                    pom = ["Mask", "masked"] if mask_only else ["Prune", "pruned"]
                    idxs_ratio_str = f"{len(idxs)} / {dep.target.module.weight.shape[0]}"
                    log_str = f"{pom[0]} {idxs_ratio_str} channels {dep_str[dep_str.find('on'): dep_str.find('(') - 1]}."
                    if self.verbose > 1:
                        log_str += f" Indices of {pom[1]} channels are: {idxs}."
                    if log is not None:
                        log.info(f" {log_str}")
                    else:
                        print(f" {log_str}")
                    self.log_str += f"{log_str}\n"

                    if mask_only:
                        self.mask_group(group)
                    else:
                        group.prune(idxs[:len(idxs) - (len(idxs) % self.slice_block_size)])
        elif self.verbose > 0 and self.channels_pruner_args["reg"] > 0:
            if log is not None:
                log.info(f" Epoch {self.current_epoch}, regularization phase with alpha = {self.channels_pruner_args['reg']}")
            else:
                print(f" Epoch {self.current_epoch}, regularization phase with alpha = {self.channels_pruner_args['reg']}")

        if self.prune_channels_at_init or not mask_only or self.reach_mac_target:
            self.prune_channels = False

        self.update_channel_mask_dict(model)

        # log MACs
        current_macs, total_macs = self.measure_macs_masked_model(model)
        if mask_only or self.prune_channels_at_init:
            if log is not None:
                log.info(f" Current MACs are {current_macs / total_macs:.3f}% of original model")
            else:
                print(f" Current MACs are {current_macs / total_macs:.3f}% of original model")
            if current_macs / total_macs < self.mac_target and self.prune_at_target:
                self.reach_mac_target = True
                # update the config file: 1. reach_mac_target, 2. logs of pruning
                config_path = os.path.join(self.config_folder, "pruning_config.json")
                with open(config_path, "r") as f:
                    sparsity_args = json.load(f)
                    import time
                    time.sleep(5)
                sparsity_args['channel_sparsity_args']['reach_mac_target'] = True
                if self.prune_at_target:
                    self.log_str = self.log_str.replace("Mask", "Prune")
                    self.reset_optimizer = True
                sparsity_args['channel_sparsity_args']['log_str'] = f"reach_mac_target at epoch {self.current_epoch}\n" + self.log_str
                with open(config_path, 'w') as file:
                    json.dump(sparsity_args, file, indent=4)

        else:
            if log is not None:
                log.info(f" Model already reach {self.mac_target} MACs reduction")
            else:
                print(f" Model already reach {self.mac_target} MACs reduction")

    def regularize(self, model):
        if not self.prune_channels or self.channels_pruner_args["reg"] == 0 or self.current_epoch > self.end_epoch:
            return
        self.update_max_imp()
        self.pruner.regularize(model, alpha=2 ** self.channels_pruner_args["alpha_shrinkage_reg"])

    def calc_prune_rate(self):
        """
        Calculate the pruning rate by interpolating between the coarse epoch-level values
        using the current step progress within the epoch.
        """
        if self.current_epoch < self.start_epoch:
            self.current_pr = 0
        elif self.current_epoch < self.end_epoch:
            # Calculate total coarse updates (epochs where pruning is updated)
            num_coarse_steps = sum([1 for i in range(self.start_epoch, self.end_epoch)
                                   if i % self.epoch_rate == 0])
            # Count how many coarse updates have occurred up to the beginning of the current epoch
            current_coarse_index = sum([1 for i in range(self.start_epoch, self.current_epoch)
                                       if i % self.epoch_rate == 0])
            # Coarse pruning values at the current and next update points
            prev_pr = self.global_prune_rate * current_coarse_index / num_coarse_steps
            next_pr = self.global_prune_rate * (current_coarse_index + 1) / num_coarse_steps

            if self.current_step is None:
                self.current_pr = next_pr
            else:
                # Interpolate using the current step progress within the epoch
                step_fraction = self.current_step[0] / self.current_step[1] if self.current_step[1] > 0 else 0
                self.current_pr = prev_pr + step_fraction * (next_pr - prev_pr)
        else:
            self.current_pr = self.global_prune_rate

    def set_layers_to_prune(self, model):
        """
            Function that setting layers to prune and their coresponding pruning rate
            As of now it is kind of hard coded, will be replaced by self.channel_sparsity_args['cp_layers'] style (as in Slice Pruning)
        """
        self.calc_prune_rate()
        ltp = self.layers_to_prune
        pruning_ratio_dict = {}

        for name, m in model.named_modules():
            # Ignore ConvTranspose2d (currently unsupported)
            if isinstance(m, torch.nn.ConvTranspose2d):
                self.ignored_layers.append(m)
                continue
            # Ignore Linear layers, such as classification head
            if isinstance(m, torch.nn.Linear):
                self.ignored_layers.append(m)
                continue
            # Pruning only Conv2d (MHA is currently not supported)
            if not isinstance(m, torch.nn.Conv2d):
                continue
            # Ignore layers with single output channels
            if m.out_channels == 1:
                continue
            # Check if the current layer should be pruned or ignored
            if name in ltp:
                pruning_ratio_dict[m] = self.current_pr
            else:
                self.ignored_layers.append(m)

        return pruning_ratio_dict

    @staticmethod
    def mask_group(group):
        for dep, idxs in group:
            target_layer = dep.target.module
            pruning_fn = dep.handler
            if not isinstance(target_layer, (torch.nn.Conv2d, torch.nn.ReLU, torch.nn.PReLU, torch.nn.BatchNorm2d)):
                continue
            mask = torch.ones_like(dep.target.module.weight)
            has_bias = False
            if pruning_fn in [tp.prune_conv_in_channels, tp.prune_linear_in_channels]:
                mask[:, idxs] = 0
            elif pruning_fn in [tp.prune_conv_out_channels, tp.prune_linear_out_channels]:
                mask[idxs] = 0
                if target_layer.bias is not None:
                    has_bias = True
            elif pruning_fn in [tp.prune_depthwise_conv_in_channels, tp.prune_depthwise_conv_out_channels]:
                target_layer.weight.data[idxs] *= 0
                if target_layer.bias is not None:
                    target_layer.bias.data[idxs] *= 0
            elif pruning_fn in [tp.prune_batchnorm_out_channels]:
                mask[idxs] = 0
                if target_layer.bias is not None:
                    has_bias = True
            else:
                continue
            target_layer.weight.data *= mask
            if has_bias:
                bias_mask = torch.ones_like(dep.target.module.bias)
                bias_mask[idxs] = 0
                target_layer.bias.data *= bias_mask

    def update_channel_mask_dict(self, model):
        for name, param in model.named_modules():
            if isinstance(param, nn.Conv2d):
                name = name[len("module."):] if name.startswith("module.") else name
                pruned_channel_indices = torch.where(torch.sum(param.weight, dim=(1, 2, 3)) == 0)[0]
                r, _ = divmod(pruned_channel_indices.shape[0], self.slice_block_size)
                if r == 0:
                    self.channel_mask_dict[name + '.weight'] = pruned_channel_indices[:r]
                else:
                    self.channel_mask_dict[name + '.weight'] = pruned_channel_indices[-r * self.slice_block_size:]

    def measure_macs_masked_model(self, model):
        # Get per-layer MACs (and params) as dictionaries
        macs_dict = tp.utils.op_counter.count_ops_and_params(model, self.example_inputs, layer_wise=True)[2]

        total_adjusted_macs = 0.0
        total_macs = 0.0
        ltp = self.set_layers_to_prune(model)

        for name, module in model.named_modules():
            if module in macs_dict and isinstance(module, torch.nn.Conv2d) and module in ltp:
                macs_layer = macs_dict[module]
                weight = module.weight
                out_channels, in_channels, _, _ = weight.shape

                # Count fully-zeroed input channels:
                zeros_input = (torch.sum(weight, dim=(0, 2, 3)) == 0).sum().item()
                # Count fully-zeroed output channels:
                zeros_output = (torch.sum(weight, dim=(1, 2, 3)) == 0).sum().item()

                active_in_ratio = (in_channels - zeros_input) / in_channels
                active_out_ratio = (out_channels - zeros_output) / out_channels

                ratio = active_in_ratio * active_out_ratio
                if ratio == 1.0:
                    continue
                total_adjusted_macs += macs_layer * ratio
                total_macs += macs_layer

                self.MACs_per_layer[name] = [macs_layer, macs_layer * ratio]

        # update current round to
        target_round_to = self.channels_pruner_args["round_to"]
        if target_round_to > 1:
            current_mac_reduction = 1 - total_adjusted_macs / total_macs
            self.channels_pruner_args["current_round_to"] = min(target_round_to, max(1, int(((current_mac_reduction / (1 - self.mac_target)) * target_round_to + 1))))

        if total_macs == 0:
            total_adjusted_macs, total_macs = 1., 1.
        return total_adjusted_macs, total_macs

    def update_max_imp(self):
        if self.channels_pruner_args['pruning_method'] != 'MACAwareImportance':
            return
        self.pruner.importance.current_max = torch.ones_like(self.pruner.importance.current_max)
        for group in self.pruner.DG.get_all_groups(ignored_layers=self.pruner.ignored_layers,
                                                   root_module_types=self.pruner.root_module_types):
            if self.pruner._check_pruning_ratio(group):
                imp = self.pruner.importance(group, act_only=True)
                self.pruner.importance.current_max = torch.max(self.pruner.importance.current_max, imp.max())


class slice_pruning:
    def __init__(self, slice_sparsity_args, model, log=None):
        if slice_sparsity_args is None:
            if log is not None:
                log.info("=> Unable to find a valid slice pruning configuration.")
            else:
                print("=> Unable to find a valid slice pruning configuration.")
            self.prune_slices = False
            self.prune_slices_at_init = False
            self.block_size = 8
            return

        self.current_epoch = 0
        self.current_pr = 0
        self.slice_sparsity_args = slice_sparsity_args
        if 'is_prune' in slice_sparsity_args.keys():
            self.prune_slices = self.slice_sparsity_args['is_prune']
        else:
            self.prune_slices = True
        self.start_epoch = self.slice_sparsity_args['start_epoch']
        self.end_epoch = self.slice_sparsity_args['end_epoch']
        self.epoch_rate = self.slice_sparsity_args['epoch_rate']
        self.block_size = slice_sparsity_args['block_size']
        self.prune_rate = slice_sparsity_args['prune_rate']
        self.reg = slice_sparsity_args['reg']
        self.slice_sparsity_args['layers'] = {name + '.weight': self.prune_rate for name, m in model.named_modules() if isinstance(m, nn.Conv2d)}
        self.channel_mask_dict = {}

    def extract_slices(self, name: str, w: torch.nn.parameter.Parameter):
        c_out, c_in, y, x = w.shape  # c_out = number of filters, c_in = number of channels
        B = c_out // self.block_size  # number of blocks
        S = B * c_in * y * x  # number of slices
        # cacluate input and output zeros channels
        input_cm = torch.where(w.sum(dim=(0, 2, 3)) == 0)[0].to('cpu')
        B_indices = torch.arange(S).view(B, c_in, y, x)
        icm_indices = B_indices[:, input_cm, :, :].contiguous().view(-1)
        channel_mask = self.channel_mask_dict[name] if self.channel_mask_dict[name].shape[0] > 0 else None
        if channel_mask is not None:
            channel_mask = channel_mask.to('cpu')
            # unpruned filters
            eff_filters = torch.tensor([i for i in range(c_out) if i not in channel_mask])
            # effective number of blocks
            eff_B = eff_filters.shape[0] // self.block_size
            # indices of unpruned filters
            eff_filter_indices = torch.cat([torch.tensor([b + a for b in range(0, self.block_size * eff_B, eff_B)]) for a in range(eff_B)])
            filter_indices = torch.cat((eff_filters[eff_filter_indices], channel_mask))
            ocm_indices = torch.tensor([i for i in range(int(S * (1 - channel_mask.shape[0] / c_out)), S)])
        else:
            filter_indices = torch.cat([torch.tensor([b + a for b in range(0, self.block_size * B, B)]) for a in range(B)])
            ocm_indices = torch.tensor([], dtype=torch.int64)
        # reordering filters by SNP itterations
        w = w[filter_indices]
        # keep indices of original ordering
        revert_fi = torch.argsort(filter_indices)
        # split the layer into blocks
        Blocks = w.view(B, self.block_size, c_in, y, x)  # [B, A, c_in, y, x]
        # split each block into slices
        Slices = Blocks.permute(0, 2, 3, 4, 1).contiguous()  # [B, c_in, y, x, A]
        Slices = Slices.view(S, self.block_size)  # [S, A]
        # calculate indices of first index of each column
        fc_indices = torch.tensor([s for s in range(S) if s % y == 0])
        # calculate indices of zero input channels
        return Slices, revert_fi, fc_indices, ocm_indices, icm_indices

    def calc_prune_rate(self):
        if self.slice_sparsity_args['pruning_gradually'] and self.current_epoch < self.end_epoch:  # would be move outside of loop
            num_steps = sum([1 for i in range(self.start_epoch, self.end_epoch) if
                             i % self.epoch_rate == 0])
            curr_step = sum([1 for i in range(self.start_epoch, self.current_epoch + 1) if
                             i % self.epoch_rate == 0])
            self.current_pr = self.prune_rate * curr_step / num_steps
        else:
            self.current_pr = self.prune_rate

    def regularize(self, model):
        if not self.prune_slices:
            return torch.tensor(0).to('cuda')
        assert self.slice_sparsity_args["pruning_mode"] == "Prune", "sparsity loss is available only on pruned stage"
        SP_loss = 0
        # run over the relevant layers, as defined in the config
        for name, param in model.named_parameters():
            if name.startswith("module."):  # parallel GPUs automaticaly added this prefix
                name = name[len("module."):]

            if name in self.slice_sparsity_args['layers'].keys():
                Slices, _, fc_indices, _, _ = self.extract_slices(name, param)
                # disable pruning on first index of each column (for loss it's equal to set them as zeros)
                if self.slice_sparsity_args['disable_first_index']:
                    Slices[fc_indices] = 0
                if "L2_norm" in self.slice_sparsity_args['pruning_method']:
                    # calculate the L2 norm of each slice
                    L2_norm_slices = torch.norm(Slices, dim=1)
                    # calculate alpha
                    alpha = (1 / (L2_norm_slices.detach() + 1e-8)).view(-1, 1)
                    Slices = Slices * alpha
                    # calculate L2-norm as a loss
                    L_loss = torch.norm(Slices, dim=1)
                    SP_loss += torch.sum(L_loss)
        if not self.prune_slices or self.slice_sparsity_args["reg"] == 0 or self.current_epoch > self.end_epoch:
            SP_loss = torch.zeros_like(SP_loss)
        return SP_loss * self.reg  # * self.current_pr / self.prune_rate

    def prune(self, model, epoch, log=None):
        self.current_epoch = epoch
        if not self.prune_slices:
            return
        if self.slice_sparsity_args["pruning_mode"] == "Unprune":
            if log is not None:
                log.info("Slice pruning disabled in Unprune mode")
            return

        pruning = (self.start_epoch <= self.current_epoch and self.current_epoch % self.epoch_rate == 0) or self.current_epoch >= self.end_epoch

        if not pruning:
            if log is not None:
                log.info("Slice pruning disabled in current epoch")
            return
        else:
            self.calc_prune_rate()
            if log is not None:
                log.info(f"Epoch {self.current_epoch}, slice pruning progress:")
                log.info(f"Pruning from epoch {self.start_epoch} to epoch {self.end_epoch}, with a current pruning rate of {self.current_pr}.")
                log.info(f"Total target: {self.prune_rate}.")
            # loop over layers and pruned them
            for name, param in model.named_parameters():
                name = name[len("module."):] if name.startswith("module.") else name

                # slice pruning
                if name in self.slice_sparsity_args['layers'].keys():
                    # extract slices and sort them by their L2-norm
                    Slices, revert_indices, fc_indices, ocm_indices, icm_indices = self.extract_slices(name, param)
                    L2_norm_slices = torch.norm(Slices, dim=1)
                    # disable pruning on first index of each column (for loss it's equal to set them as zeros)
                    if self.slice_sparsity_args['disable_first_index']:
                        L2_norm_slices[fc_indices] = torch.inf
                    # disable pruning on slices which already pruned during channel pruning
                    L2_norm_slices[ocm_indices] = torch.inf
                    L2_norm_slices[icm_indices] = torch.inf

                    # Sort slices by their L2-norm
                    sorted_slices_norms, _ = L2_norm_slices.flatten().sort()
                    # Determine the threshold based on p%
                    slices_threshold_index = int(self.current_pr * len(sorted_slices_norms))
                    slices_threshold = sorted_slices_norms[slices_threshold_index]
                    # Create a mask to zero out entries below the threshold
                    slices_mask = L2_norm_slices <= slices_threshold
                    Slices[slices_mask] = 0
                    # prune the layer
                    c_out, c_in, _, _ = param.shape
                    B = c_out // self.block_size  # number of blocks
                    pruned_layer = Slices.view(B, c_in, 3, 3, self.block_size).permute(0, 4, 1, 2, 3).contiguous()
                    # revert ordering of filters
                    pruned_layer = pruned_layer.view(c_out, c_in, 3, 3)[revert_indices]
                    param.data = pruned_layer
                    if log is not None:
                        log_str = f"Mask {slices_mask.sum()} / {Slices.shape[0]} slices on {name}"
                        log.info(f" {log_str}")

        if log is not None:
            log.info(f" Current slice sparsity: {self.calc_current_sparsity(model)}")

    def calc_current_sparsity(self, model):
        with torch.no_grad():
            num_slices = 0
            num_zero_slices = 0
            for name, param in model.named_parameters():
                if name.startswith("module."):  # parallel GPUs automaticaly added this prefix
                    name = name[len("module."):]
                if name in self.slice_sparsity_args['layers'].keys():
                    # icm = torch.where(param.sum(dim=(0, 2, 3)) == 0)
                    # ocm = torch.where(param.sum(dim=(1, 2, 3)) == 0)
                    # param[icm] = torch.ones_like(param[icm])
                    # param[ocm] = torch.zeros_like(param[ocm])
                    Slices, _, _, _, _ = self.extract_slices(name, param)
                    num_zero_slices += torch.sum(torch.sum(Slices, dim=1) == 0)
                    num_slices += Slices.shape[0]
            return num_zero_slices / num_slices


class MACAwareImportance(tp.importance.GroupNormImportance):
    def __init__(self, p=2, layers_mac=None, params=None, current_max=None):
        """
        MAC-aware importance calculation.
        Args:
            p (int): Norm degree for the original importance score.
        """
        super().__init__(p=p)
        if params is None:
            params = {"type": "Sum", "alpha": 0.9, "beta": 2, "use_macs": True}
        self.use_macs = params["use_macs"]  # Enable or disable MAC logic
        assert layers_mac is not None, "layers mac must be given"
        self.layers_mac = layers_mac
        self.min_layer_mac = min([v for v in self.layers_mac.values()])
        self.max_layer_mac = max([v for v in self.layers_mac.values()])
        self.type, self.alpha, self.beta = params["type"], params["alpha"], params["beta"]
        self.current_max = current_max

    def __call__(self, group, act_only=False):
        # 1. Compute original importance score (e.g., activation-based)
        activation_importance = super().__call__(group)

        if not self.use_macs or act_only:
            return activation_importance

        # 2. Combine both metrics
        dep, _ = group[0]
        layer_name = dep.target.name[:dep.target.name.index(" ")]
        layer_name = layer_name[len("module."):] if layer_name.startswith("module.") else layer_name
        layer_mac = self.layers_mac[layer_name]
        layer_mac_norm = (layer_mac - self.min_layer_mac) / (self.max_layer_mac - self.min_layer_mac)
        ratio_mac_min = (self.min_layer_mac / layer_mac) ** (1 / self.beta)
        # ratio_mac_max = (layer_mac / self.max_layer_mac) ** (1 / self.beta)

        # type 1: multiply by Min(self.layer_mac) / Layer_mac
        if self.type == "Mul":
            combined_importance = activation_importance * ratio_mac_min
        # type 2: balance using alpha
        elif self.type == "Sum":
            combined_importance = self.alpha * activation_importance / self.current_max + (1 - self.alpha) * (1 - layer_mac_norm)
        else:
            combined_importance = activation_importance

        return combined_importance

    @torch.no_grad()
    def regularize(self, alpha=2 ** 4, bias=False):
        for i, group in enumerate(self._groups):
            ch_groups = self._get_channel_groups(group)
            imp = self.estimate_importance(group).sqrt()
            if torch.any(torch.isnan(imp)):  # avoid nan
                continue
            gamma = alpha ** ((imp.max() - imp) / (imp.max() - imp.min()))

            # Update Gradient
            for i, (dep, idxs) in enumerate(group):
                layer = dep.target.module
                # if isinstance(layer, torch.nn.ConvTranspose2d):
                #     continue
                prune_fn = dep.handler
                if prune_fn in [
                    tp.pruner.function.prune_conv_out_channels,
                    tp.pruner.function.prune_linear_out_channels,
                ]:
                    if layer.weight.grad is None: continue

                    root_idxs = group[i].root_idxs
                    _gamma = torch.index_select(gamma, 0, torch.tensor(root_idxs, device=gamma.device))

                    w = layer.weight.data[idxs]
                    g = w * _gamma.view(-1, *([1] * (
                                len(w.shape) - 1)))  # / group_norm.view( -1, *([1]*(len(w.shape)-1)) ) * group_size #group_size #* gamma.view( -1, *([1]*(len(w.shape)-1)) )
                    layer.weight.grad.data[idxs] += self.reg * g

                    if bias and layer.bias is not None:
                        b = layer.bias.data[idxs]
                        g = b * _gamma
                        layer.bias.grad.data[idxs] += self.reg * g

                elif prune_fn in [
                    tp.pruner.function.prune_conv_in_channels,
                    tp.pruner.function.prune_linear_in_channels,
                ]:
                    if layer.weight.grad is None: continue
                    gn = imp
                    if hasattr(dep.target, 'index_transform') and isinstance(dep.target.index_transform,
                                                                             tp._helpers._FlattenIndexMapping):
                        gn = imp.repeat_interleave(w.shape[1] // imp.shape[0])

                    # regularize input channels
                    if prune_fn == tp.pruner.function.prune_conv_in_channels and layer.groups > 1:
                        gamma = gamma[:len(idxs) // ch_groups]
                        idxs = idxs[:len(idxs) // ch_groups]

                    root_idxs = group[i].root_idxs
                    _gamma = torch.index_select(gamma, 0, torch.tensor(root_idxs, device=gamma.device))

                    w = layer.weight.data[:, idxs]
                    g = w * _gamma.view(1, -1, *([1] * (
                                len(w.shape) - 2)))  # / gn.view( 1, -1, *([1]*(len(w.shape)-2)) ) * group_size #* gamma.view( 1, -1, *([1]*(len(w.shape)-2))  )
                    layer.weight.grad.data[:, idxs] += self.reg * g
        self.cnt += 1


def build_inputs(cfg, device):
    """
    Build dummy input tensors from a config specification.

    Config options:
    - "input_shape": either a single shape [B, C, H, W] or a list of shapes [[...], [...], ...]
    - "container": optional, if set to "tuple" returns a tuple of tensors, otherwise returns a list.

    Returns:
    - torch.Tensor if single shape
    - list[torch.Tensor] if multiple shapes
    - tuple[torch.Tensor, ...] if multiple shapes and container="tuple"
    """
    if "input_shape" not in cfg:
        # Require config to define "input_shape"
        raise ValueError("Config must contain 'inputs'.")

    raw = cfg["input_shape"]
    container = cfg.get("container", None)

    # Case 1: single tensor, shape given as [B, C, H, W]
    if isinstance(raw, (list, tuple)) and all(isinstance(x, int) for x in raw):
        shape = tuple(int(x) for x in raw)
        return torch.zeros(shape, device=device)

    # Case 2: multiple tensors, shape list like [[...], [...], ...]
    if isinstance(raw, (list, tuple)) and all(isinstance(s, (list, tuple)) for s in raw):
        shapes = [tuple(int(x) for x in s) for s in raw]
        tensors = [torch.zeros(s, device=device) for s in shapes]
        if container == "tuple":
            # Return tensors as tuple if requested
            return tuple(tensors)
        return tensors  # default: return as list

    # Invalid input format
    raise ValueError(
        "'inputs' must be [B,C,H,W] or a list of such shapes; "
        "use 'container': 'tuple' to return a tuple.")