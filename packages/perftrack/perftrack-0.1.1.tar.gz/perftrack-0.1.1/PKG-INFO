Metadata-Version: 2.4
Name: perftrack
Version: 0.1.1
Summary: Lightweight CLI tool to detect performance regressions by comparing local benchmark snapshots.
Author-email: Pravin <p-r-a-v-i-n@users.noreply.github.com>
License-Expression: MIT
Project-URL: Homepage, https://github.com/p-r-a-v-i-n/perftrack
Project-URL: Issues, https://github.com/p-r-a-v-i-n/perftrack/issues
Keywords: benchmark,cli,performance,regression
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: psutil<8,>=7.1.3
Provides-Extra: dev
Requires-Dist: pre-commit; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Dynamic: license-file

# PerfTrack

PerfTrack is a command-line tool that helps detect performance regressions in open-source projects and CI pipelines. It requires no external services and tracks performance locally.

Designed for Python projects (Django, Pandas, FastAPI, ML libraries), internal tooling, and personal benchmarking workflows

## Why PerfTrack?
Performance regressions often go unnoticed because:

- CI timing is noisy and unreliable for benchmarking
- Tools like ASV or pytest-benchmark require setup and maintenance
- There is no simple utility to store a baseline and fail CI when performance drops

PerfTrack stores local performance snapshots and compares them against future runs, both locally and in CI.

## What It Measures
- Wall-clock time
- CPU time
- Peak RSS memory usage

Baseline cached locally â†’ regression check in CI.

## Installation

```bash
pip install perftrack

```

## Quick start

```bash
# run a command and record performance
perftrack run "python script.py"

# set the latest run as baseline
perftrack baseline set-latest

# later compare new results with baseline
perftrack compare --fail-on-regression

# Generate Simple HTML report
perftrack report

```
Replace "python script.py" with whatever you want to benchmark (scripts, test suites, build steps, ML training, etc.)

## Directory
Perftrack store results under:

```bash
.perftrack/
  baseline.json
  latest.json

```

## CI Regression Example
PerfTrack guarantees that performance regressions are caught before merge.
Here is a ([pull request](https://github.com/p-r-a-v-i-n/perftrack-examples/pull/7)) where we intentionally slowed down code to demonstrate PerfTrack in CI.

<img width="741" height="209" alt="perftrack" src="https://github.com/user-attachments/assets/25eca2c6-0226-4608-9614-a2eb0102e67e" />
