from typing import Dict, List, Tuple

import torch
from torch.nn.utils.rnn import pad_sequence

from phonemize import Prediction
from phonemize.model.model import load_checkpoint
from phonemize.model.utils import _get_len_util_stop
from phonemize.preprocessing.text import Preprocessor
from phonemize.preprocessing.utils import _batchify, _product


class Predictor:

    """Performs model predictions on a batch of inputs."""

    def __init__(self, model: torch.nn.Module, preprocessor: Preprocessor) -> None:
        """
        Initializes a Predictor object with a trained transformer model and a preprocessor.
        """

        self.model = model
        self.text_tokenizer = preprocessor.text_tokenizer
        self.phoneme_tokenizer = preprocessor.phoneme_tokenizer

    def __call__(self, words: List[str], lang: str, batch_size: int = 8) -> List[Prediction]:
        """Predict phonemes for a list of words.

        Args:
            words: list of words to predict.
            lang: language code.
            batch_size: inference batch size.

        Returns:
            List[Prediction]
        """

        predictions: Dict[str, Tuple[List[int], List[float]]] = {}
        valid_texts = set()

        # handle words that result in an empty input to the model
        for word in words:
            input_tokens = self.text_tokenizer(sentence=word, language=lang)
            decoded = self.text_tokenizer.decode(
                sequence=input_tokens, remove_special_tokens=True
            )
            if len(decoded) == 0:
                predictions[word] = ([], [])
            else:
                valid_texts.add(word)

        valid_texts = sorted(list(valid_texts), key=lambda x: len(x))
        batch_pred = self._predict_batch(texts=valid_texts, batch_size=batch_size, language=lang)
        predictions.update(batch_pred)

        output: List[Prediction] = []
        for word in words:
            tokens, probs = predictions[word]
            out_phons = self.phoneme_tokenizer.decode(
                sequence=tokens, remove_special_tokens=True
            )
            out_phons_tokens = self.phoneme_tokenizer.decode(
                sequence=tokens, remove_special_tokens=False
            )
            output.append(
                Prediction(
                    word=word,
                    phonemes=''.join(out_phons),
                    phoneme_tokens=out_phons_tokens,
                    confidence=_product(probs),
                    token_probs=probs,
                )
            )

        return output

    def _predict_batch(
        self,
        texts: List[str],
        batch_size: int,
        language: str,
    ) -> Dict[str, Tuple[List[int], List[float]]]:
        """Return mapping: word -> (phoneme token ids, token probabilities)."""

        predictions: Dict[str, Tuple[List[int], List[float]]] = {}
        text_batches = _batchify(texts, batch_size)
        for text_batch in text_batches:
            input_batch, lens_batch = [], []
            for text in text_batch:
                input_tokens = self.text_tokenizer(text, language)
                input_batch.append(torch.tensor(input_tokens))
                lens_batch.append(torch.tensor(len(input_tokens)))

            input_batch = pad_sequence(sequences=input_batch, batch_first=True, padding_value=0)
            lens_batch = torch.stack(lens_batch)
            start_indx = self.phoneme_tokenizer._get_start_index(language)
            start_inds = torch.tensor([start_indx] * input_batch.size(0))
            start_inds = start_inds.to(input_batch.device)
            batch = {
                'text': input_batch,
                'text_len': lens_batch,
                'start_index': start_inds,
            }
            with torch.no_grad():
                output_batch, probs_batch = self.model.generate(batch)
            output_batch, probs_batch = output_batch.cpu(), probs_batch.cpu()
            for text, output, probs in zip(text_batch, output_batch, probs_batch):
                seq_len = _get_len_util_stop(output, self.phoneme_tokenizer.end_index)
                predictions[text] = (output[:seq_len].tolist(), probs[:seq_len].tolist())

        return predictions

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = 'cpu') -> 'Predictor':
        """Initialize predictor from a checkpoint (.pt file)."""
        model, checkpoint = load_checkpoint(checkpoint_path, device=device)

        preprocessor = checkpoint['preprocessor']
        return Predictor(model=model, preprocessor=preprocessor)
    