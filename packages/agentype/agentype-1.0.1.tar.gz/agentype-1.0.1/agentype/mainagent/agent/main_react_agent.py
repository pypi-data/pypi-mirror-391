#!/usr/bin/env python3
"""
agentype - Main React Agentæ¨¡å—
Author: cuilei
Version: 1.0
"""

import gc
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any

# å¯¼å…¥ promptsï¼ˆä¿æŒä¸å…¶ä»– Agent ä¸€è‡´çš„æ¥å£ï¼‰
from agentype.mainagent.config.prompts import (
    SUPPORTED_LANGUAGES,
    DEFAULT_LANGUAGE,
    get_system_prompt_template,
    get_fallback_prompt_template,
    get_user_query_templates,
)

# Tokenç»Ÿè®¡æ¨¡å—
from agentype.common.token_statistics import TokenStatistics, TokenReporter, merge_token_stats
from agentype.common.log_token_parser import LogTokenParser

# åŸºç¡€ç»„ä»¶
from agentype.mainagent.config.settings import ConfigManager
from agentype.mainagent.clients.mcp_client import MCPClient
from agentype.mainagent.utils.content_processor import ContentProcessor
from agentype.mainagent.utils.parser import ReactParser
from agentype.mainagent.utils.validator import ValidationUtils
from agentype.mainagent.utils.i18n import _, set_language as _set_i18n_language
from agentype.mainagent.utils.path_manager import path_manager
from agentype.mainagent.utils.output_logger import OutputLogger
from agentype.mainagent.tools.cluster_tools import check_cluster_completion
from agentype.mainagent.tools.file_paths_tools import load_file_paths_bundle
# å¯¼å…¥å…±äº«æ¨¡å—
from agentype.common.llm_client import LLMClient
from agentype.common.llm_logger import LLMLogger


class MainReactAgent:
    """MainAgent React å®ç°ï¼ˆå¯¹é½å…¶å®ƒ Agent çš„å†™æ³•ï¼‰

    å­ Agent è§†ä¸º MainAgent MCP æœåŠ¡å™¨æä¾›çš„å·¥å…·å°è£…ï¼Œ
    ä¸åœ¨æ­¤ç›´æ¥åŠ è½½å­ Agentã€‚é€šè¿‡è°ƒç”¨è‡ªèº« MCP å·¥å…·å®Œæˆæ•°æ®å¤„ç†ã€
    åŸºå› åˆ†æä¸ç»†èƒæ³¨é‡Šã€‚
    """

    def __init__(self,
                 config: Optional[ConfigManager] = None,
                 server_script: str = None,
                 max_content_length: int = 10000,
                 enable_summarization: bool = True,
                 max_iterations: int = 30,
                 context_summary_threshold: int = 50,  # ğŸ†• ä»30æé«˜åˆ°50ï¼Œç»™LLMæ›´å¤šè¿­ä»£ç©ºé—´å®Œæˆæ‰€æœ‰é˜¶æ®µ
                 max_context_length: int = 150000,  # tokené˜ˆå€¼ä¿æŒä¸å˜
                 enable_llm_logging: bool = True,
                 log_dir: str = None,
                 language: str = "zh",
                 enable_streaming: bool = True,
                 api_timeout: int = 300,
                 console_output: bool = True,
                 file_output: bool = True):

        # é…ç½®ä¸ MCP å®¢æˆ·ç«¯
        self.config = config or ConfigManager.from_env()
        if server_script is None:
            server_script = str(path_manager.get_mcp_server_path())
        # â­ ä¼ å…¥ config ç»™ MCPClientï¼Œç”¨äºæ··åˆé…ç½®ä¼ é€’ï¼ˆç¯å¢ƒå˜é‡ + å‘½ä»¤è¡Œå‚æ•°ï¼‰
        self.mcp_client = MCPClient(server_script, config=self.config)

        # React ç»„ä»¶
        self.content_processor = ContentProcessor(max_content_length, enable_summarization)
        self.parser = ReactParser()
        self.validator = ValidationUtils()

        # è¯­è¨€ä¸ LLM æ—¥å¿—
        self.language = language if language in SUPPORTED_LANGUAGES else DEFAULT_LANGUAGE
        # åŒæ­¥ i18n è¯­è¨€ç¯å¢ƒ
        try:
            _set_i18n_language(self.language)
        except Exception:
            pass
        self.enable_llm_logging = enable_llm_logging
        # å¦‚æœæ²¡æœ‰æŒ‡å®šlog_dirï¼Œä½¿ç”¨ ConfigManager çš„æ—¥å¿—ç›®å½•
        if log_dir is None:
            from pathlib import Path
            log_dir = str(Path(self.config.log_dir) / "llm" / "MainAgent")
        self.llm_logger = LLMLogger(log_dir) if enable_llm_logging else None

        # Tokenç»Ÿè®¡æŠ¥å‘Šå™¨
        self.token_reporter = TokenReporter(language=self.language)

        # åˆå§‹åŒ–è¾“å‡ºæ—¥å¿—å™¨
        if console_output or file_output:
            self.console_logger = OutputLogger(
                log_prefix="celltypeMainagent",
                console_output=console_output,
                file_output=file_output,
                log_dir=str(self.config.log_dir)
            )
        else:
            self.console_logger = None

        # ğŸŒŸ åˆå§‹åŒ–å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰
        self.llm_client = LLMClient(
            config=self.config,
            logger_callbacks={
                'info': self._log_info,
                'success': self._log_success,
                'warning': self._log_warning,
                'error': self._log_error
            }
        )

        # è¿è¡Œæ—¶
        self.available_tools: List[Dict] = []
        self.max_iterations = max_iterations
        self.context_summary_threshold = context_summary_threshold
        self.max_context_length = max_context_length
        self.enable_streaming = enable_streaming
        self.api_timeout = api_timeout

        # ğŸ†• æµç¨‹å®Œæˆåº¦æ£€æŸ¥çŠ¶æ€è¿½è¸ª
        self.workflow_completion_reminder_sent = False  # è®°å½•æ˜¯å¦å·²å‘é€è¿‡å®Œæˆåº¦æé†’

    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Main Agent"""
        self._log_info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– MainAgentâ€¦")
        if not await self.mcp_client.start_server():
            self._log_error("âŒ MainAgent MCP æœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
            return False
        self.available_tools = await self.mcp_client.list_tools()
        self._log_success(f"âœ… MainAgent MCP å·²å¯åŠ¨ï¼Œå·¥å…·æ•°: {len(self.available_tools)}")
        return True

    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            await self.mcp_client.stop_server()
            if self.llm_logger:
                await self.llm_logger.close()
            gc.collect()
            self._log_success("âœ… MainAgent èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            self._log_warning(f"âš ï¸ èµ„æºæ¸…ç†æ—¶å‡ºç°å¼‚å¸¸: {e}")

    def _log_info(self, message: str) -> None:
        """è¾“å‡ºä¿¡æ¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.info(message)
        else:
            print(message)

    def _log_success(self, message: str) -> None:
        """è¾“å‡ºæˆåŠŸæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.success(message)
        else:
            print(message)

    def _log_error(self, message: str) -> None:
        """è¾“å‡ºé”™è¯¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.error(message)
        else:
            print(message)

    def _log_warning(self, message: str) -> None:
        """è¾“å‡ºè­¦å‘Šæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.warning(message)
        else:
            print(message)

    def _log_header(self, message: str) -> None:
        """è¾“å‡ºæ ‡é¢˜æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.header(message)
        else:
            print(message)

    def _log_separator(self, char: str = "=", length: int = 60) -> None:
        """è¾“å‡ºåˆ†éš”çº¿"""
        if self.console_logger:
            self.console_logger.separator(char, length)
        else:
            print(char * length)

    def set_language(self, language: str):
        """è®¾ç½®è¯­è¨€"""
        if language in SUPPORTED_LANGUAGES:
            self.language = language
            self._log_info(f"ğŸŒ è¯­è¨€å·²è®¾ç½®ä¸º: {language}")
            # åŒæ­¥ i18n è¯­è¨€ç¯å¢ƒ
            try:
                _set_i18n_language(language)
            except Exception:
                pass
        else:
            self._log_error(f"âŒ ä¸æ”¯æŒçš„è¯­è¨€: {language}ï¼Œæ”¯æŒçš„è¯­è¨€: {SUPPORTED_LANGUAGES}")

    def _get_system_prompt(self) -> str:
        """è·å– React ç³»ç»Ÿ prompt"""
        try:
            template = get_system_prompt_template(self.language)
            tool_list = "\n".join([f"{i+1}. {t.get('name', 'Unknown')}: {t.get('description', '')}" for i, t in enumerate(self.available_tools)])
            cache_status = "âœ“ MCP æœåŠ¡å™¨å·²å¯åŠ¨" if self.language == 'zh' else "âœ“ MCP Server Started"
            import os
            try:
                files = [f for f in os.listdir('.') if os.path.isfile(f)]
                file_list = ', '.join(files[:10])
            except Exception:
                file_list = "main_react_agent.py, mcp_server.py, prompts.py"
            return template.format(
                tool_list=tool_list,
                operating_system="Linux",
                cache_status=cache_status,
                file_list=file_list
            )
        except Exception as e:
            self._log_warning(f"âš ï¸ ä½¿ç”¨ç³»ç»Ÿæç¤ºæ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ¨¡æ¿: {e}")
            fallback_template = get_fallback_prompt_template(self.language)
            tool_names = ', '.join([t.get('name', '') for t in self.available_tools])
            return fallback_template.format(tool_names=tool_names)


    async def _call_openai(self, messages: List[Dict], timeout: int = 270, stream: bool = False, request_type: str = "main") -> str:
        """è°ƒç”¨ OpenAI API - ä½¿ç”¨å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰"""
        return await self.llm_client.call_api(
            messages=messages,
            timeout=timeout,
            stream=stream,
            request_type=request_type,
            llm_logger=self.llm_logger,
            console_logger=self.console_logger
        )
    async def _retry_llm_call_with_correction(self, messages: List[Dict], validation_result: Dict, max_retries: int = 10, timeout: int = 270) -> str:
        """é‡æ–°è°ƒç”¨ LLM å¹¶æä¾›ä¿®æ­£æŒ‡å¯¼"""
        retry_count = 0
        response = ""

        while retry_count < max_retries:
            retry_count += 1
            self._log_warning(_("retry.llm_retry", retry=retry_count, max_retries=max_retries))
            self._log_info(_("retry.last_response_issues", issues=', '.join(validation_result['issues'])))

            # æ„å»ºä¿®æ­£æç¤º
            correction_prompt = ValidationUtils.build_correction_prompt(validation_result, self.available_tools, self.language)

            # æ·»åŠ ä¿®æ­£æç¤ºåˆ°æ¶ˆæ¯å†å²
            correction_messages = messages.copy()
            correction_messages.append({"role": "user", "content": correction_prompt})

            # é‡æ–°è°ƒç”¨ LLMï¼Œä¿æŒä¸ä¸»è°ƒç”¨ç›¸åŒçš„æµå¼è¾“å‡ºè®¾ç½®
            response = await self._call_openai(correction_messages, timeout, stream=self.enable_streaming, request_type="retry")

            # ä¸ºé‡è¯•è°ƒç”¨æ·»åŠ é¢å¤–çš„æ—¥å¿—æ ‡è®°
            if self.llm_logger:
                self._log_info(_("retry.retry_logged", count=retry_count))

            # éªŒè¯æ–°å“åº”æ ¼å¼
            new_validation = ValidationUtils.validate_response_format(response)

            # ğŸš¨ é¢å¤–æ£€æŸ¥ï¼šé‡è¯•å“åº”ä¸­æ˜¯å¦ä»åŒ…å«è·³è¿‡è¡Œä¸º
            skip_patterns = [
                r"è·³è¿‡å‰©ä½™", r"æ—¶é—´å…³ç³»", r"è·³è¿‡.*?ç°‡", r"çœç•¥.*?åˆ†æ",
                r"ç›´æ¥è¿›å…¥åç»­", r"ç»“æŸå¤„ç†", r"è·³è¿‡.*?cluster",
                r"ç”±äº.*?è·³è¿‡", r"å°†è·³è¿‡.*?çš„è¯¦ç»†åˆ†æ"
            ]
            still_trying_to_skip = any(re.search(pattern, response, re.IGNORECASE) for pattern in skip_patterns)

            if still_trying_to_skip and '</final_answer>' in response:
                self._log_warning(f"ğŸš¨ é‡è¯•ç¬¬{retry_count}æ¬¡ä»æ£€æµ‹åˆ°è·³è¿‡è¡Œä¸ºï¼Œç»§ç»­é‡è¯•")
                from agentype.prompts import get_prompt_manager

                manager = get_prompt_manager(self.language)
                skip_message = manager.get_agent_specific_prompt('mainagent', 'SKIP_DETECTION_RETRY_MESSAGE')

                new_validation = {
                    "valid": False,
                    "issues": skip_message['issues']
                }
                validation_result = new_validation
                continue

            if new_validation['valid']:
                self._log_success(_("retry.retry_success"))
                return response
            else:
                self._log_warning(_("retry.retry_failed", count=retry_count, issues=', '.join(new_validation['issues'])))
                validation_result = new_validation

        self._log_error(_("retry.max_retries_reached"))
        return response

    async def _summarize_context(self, messages: List[Dict]) -> List[Dict]:
        """æ”¹è¿›çš„ä¸Šä¸‹æ–‡æ€»ç»“ï¼Œä¿ç•™å…³é”®å·¥å…·è°ƒç”¨ç»“æœä»¥é˜²æ­¢çŠ¶æ€ä¸¢å¤±

        Args:
            messages: å½“å‰çš„å¯¹è¯å†å²

        Returns:
            æ€»ç»“åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        try:
            self._log_info(_("context.too_long"))

            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€åˆçš„ç”¨æˆ·æŸ¥è¯¢
            system_message = messages[0] if messages and messages[0]["role"] == "system" else None
            initial_query = messages[1] if len(messages) > 1 and messages[1]["role"] == "user" else None

            # ğŸ†• è¯†åˆ«å…³é”®å·¥å…·è°ƒç”¨ - è¿™äº›ä¸åº”è¯¥è¢«æ€»ç»“ä¸¢å¤±
            critical_tools = ['load_cluster_types', 'save_cluster_type', 'get_all_cluster_ids',
                            'check_cluster_completion', 'extract_cluster_genes']
            critical_messages = []

            # æŸ¥æ‰¾ä¹‹å‰çš„æ€»ç»“æ¶ˆæ¯
            existing_summary = None
            summary_index = -1
            for i, msg in enumerate(messages):
                if msg.get("role") == "assistant" and "[ä¸Šä¸‹æ–‡æ€»ç»“]" in msg.get("content", ""):
                    existing_summary = msg
                    summary_index = i
                    break

            # ç¡®å®šéœ€è¦æ€»ç»“çš„å¯¹è¯å†å²èŒƒå›´ (ğŸ†• ä¿ç•™æœ€è¿‘5ä¸ªæ¶ˆæ¯)
            if existing_summary and summary_index > 1:
                # å¦‚æœå·²æœ‰æ€»ç»“ï¼Œä»æ€»ç»“åå¼€å§‹åˆ°å€’æ•°5è½®
                conversation_to_summarize = messages[summary_index+1:-5] if len(messages) > summary_index + 6 else messages[summary_index+1:]
                recent_messages = messages[-5:] if len(messages) > summary_index + 6 else []
            else:
                # å¦‚æœæ²¡æœ‰æ€»ç»“ï¼Œä»ç¬¬3æ¡æ¶ˆæ¯å¼€å§‹ï¼ˆè·³è¿‡ç³»ç»Ÿæ¶ˆæ¯å’Œåˆå§‹æŸ¥è¯¢ï¼‰
                conversation_to_summarize = messages[2:-5] if len(messages) > 7 else messages[2:]
                recent_messages = messages[-5:] if len(messages) > 7 else []

            # ğŸ†• ä»è¦æ€»ç»“çš„æ¶ˆæ¯ä¸­æå–å…³é”®å·¥å…·è°ƒç”¨ç»“æœ
            for msg in conversation_to_summarize:
                content = msg.get("content", "")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®å·¥å…·çš„è°ƒç”¨æˆ–observation
                if any(tool in content for tool in critical_tools):
                    # ä¿ç•™å®Œæ•´çš„å·¥å…·è°ƒç”¨å’Œå…¶observation
                    critical_messages.append(msg)

            if not conversation_to_summarize:
                self._log_info(_("context.nothing_to_summarize"))
                return messages

            # æ„å»ºæ€»ç»“prompt
            conversation_text = ""
            for msg in conversation_to_summarize:
                role = msg["role"]
                content = msg["content"]
                conversation_text += f"\n{role.upper()}: {content}\n"

            # æ ¹æ®æ˜¯å¦å­˜åœ¨ä¹‹å‰çš„æ€»ç»“æ¥æ„å»ºä¸åŒçš„prompt
            from agentype.prompts import get_prompt_manager
            manager = get_prompt_manager(self.language)

            if existing_summary:
                existing_summary_content = existing_summary.get("content", "").replace("[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„å¤„ç†è¿›å±•ï¼š", "").strip()
                template = manager.get_agent_specific_prompt('mainagent', 'CONTEXT_SUMMARY_INCREMENTAL_TEMPLATE')
                summarization_prompt = template.format(
                    existing_summary=existing_summary_content,
                    conversation=conversation_text
                )
            else:
                template = manager.get_agent_specific_prompt('mainagent', 'CONTEXT_SUMMARY_INITIAL_TEMPLATE')
                summarization_prompt = template.format(
                    conversation=conversation_text
                )

            # è°ƒç”¨OpenAIè¿›è¡Œæ€»ç»“ï¼Œä¿æŒä¸ä¸»è°ƒç”¨ç›¸åŒçš„æµå¼è¾“å‡ºè®¾ç½®
            summary_messages = [{"role": "user", "content": summarization_prompt}]
            summary = await self._call_openai(summary_messages, timeout=self.api_timeout, stream=self.enable_streaming, request_type="summary")

            # è®°å½•æ€»ç»“è¯·æ±‚æ—¥å¿—
            if self.llm_logger:
                self._log_info(_("context.summary_logged"))

            # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
            new_messages = []

            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            if system_message:
                new_messages.append(system_message)

            # ä¿ç•™åˆå§‹æŸ¥è¯¢
            if initial_query:
                new_messages.append(initial_query)

            # æ·»åŠ æ›´æ–°çš„æ€»ç»“æ¶ˆæ¯ï¼ˆæ›¿æ¢ä¹‹å‰çš„æ€»ç»“ï¼‰
            summary_message = {
                "role": "assistant",
                "content": f"[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„å¤„ç†è¿›å±•ï¼š{summary}"
            }
            new_messages.append(summary_message)

            # ğŸ†• æ’å…¥ä¿ç•™çš„å…³é”®å·¥å…·è°ƒç”¨æ¶ˆæ¯ (æœ€è¿‘5ä¸ª)
            if critical_messages:
                self._log_info(f"ğŸ’¾ ä¿ç•™ {len(critical_messages)} ä¸ªå…³é”®å·¥å…·è°ƒç”¨ç»“æœ")
                new_messages.extend(critical_messages[-5:])

            # æ·»åŠ æœ€è¿‘çš„æ¶ˆæ¯ä»¥ä¿æŒè¿ç»­æ€§
            new_messages.extend(recent_messages)

            self._log_success(_("context.summary_completed"))
            self._log_info(_("context.original_count", count=len(messages)))
            self._log_info(_("context.summarized_count", count=len(new_messages)))
            self._log_info(_("context.summary_length", length=len(summary)))

            return new_messages

        except Exception as e:
            error_msg = _("context.summary_failed", error=str(e))
            self._log_error(f"âŒ {error_msg}")

            # æ€»ç»“å¤±è´¥æ—¶ï¼Œç®€å•åœ°ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            if len(messages) > 8:
                self._log_warning(_("context.fallback_strategy"))
                return messages[-8:]

            return messages

    def _build_user_query(self, user_request: str, extra: Optional[Dict[str, Any]] = None) -> str:
        """æ„å»ºç»Ÿä¸€ç”¨æˆ·é—®é¢˜æ–‡æœ¬ï¼Œå¯é™„åŠ é¢å¤–ä¸Šä¸‹æ–‡å‚æ•°"""
        templates = get_user_query_templates(self.language)
        # è‹¥æä¾›äº†ç»“æ„åŒ–çš„ data/tissue/species ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ unified æ¨¡æ¿ï¼Œä¾¿äºLLMç†è§£å’Œæ­£ç¡®è°ƒç”¨å·¥å…·
        if extra:
            data_path = extra.get("data_path") or extra.get("rds_path") or extra.get("h5ad_path") or extra.get("h5_path")
            tissue = extra.get("tissue_type") or extra.get("tissue_description")
            species = extra.get("species")
            if data_path or tissue or species:
                unified_tpl = templates.get("unified")
                if unified_tpl:
                    return unified_tpl.format(
                        data_path=data_path or "",
                        tissue=tissue or "",
                        species=species or ""
                    )

        # å›é€€åˆ°é€šç”¨å·¥ä½œæµæ¨¡æ¿ï¼Œå¹¶åœ¨æœ«å°¾é™„ä¸ŠåŸå§‹é™„åŠ å‚æ•°JSON
        tpl = templates.get("full_workflow") or "è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡è¿è¡Œå®Œæ•´å·¥ä½œæµï¼š{task_description}"
        base = tpl.format(task_description=user_request)
        if extra:
            try:
                extra_json = json.dumps(extra, ensure_ascii=False)
                return base + f"\n\né™„åŠ å‚æ•°: {extra_json}"
            except Exception:
                return base
        return base

    async def process_request(self, user_request: str, input_data: Optional[Dict[str, Any]] = None, tissue_type: Optional[str] = None, cluster_column: str = "seurat_clusters") -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·è¯·æ±‚ï¼šç›´æ¥è¿›å…¥ LLM React å¾ªç¯"""
        try:
            self._log_info(f"ğŸ“‹ å¤„ç†ç”¨æˆ·è¯·æ±‚(React): {user_request}")
            return await self.process_with_llm_react(user_request, input_data=input_data, tissue_type=tissue_type, cluster_column=cluster_column)
        except Exception as e:
            return {"success": False, "error": f"è¯·æ±‚å¤„ç†å¼‚å¸¸: {e}"}

    async def process_with_llm_react(self, user_input: Optional[str] = None, input_data: Optional[Any] = None, tissue_type: Optional[str] = None, cluster_column: str = "seurat_clusters", species: Optional[str] = None) -> Dict:
        """åŸºäº LLM çš„ React æ¨ç†ä¸ MCP å·¥å…·è°ƒç”¨å¾ªç¯"""
        # è§„èŒƒåŒ–è¾“å…¥
        extra: Optional[Dict[str, Any]] = None
        if isinstance(input_data, str):
            extra = {"data_path": input_data}
        elif isinstance(input_data, dict):
            # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…å¤–éƒ¨å¼•ç”¨è¢«æ„å¤–ä¿®æ”¹
            extra = dict(input_data)
        else:
            extra = None

        # ç»Ÿä¸€ç»„ç»‡ç±»å‹å…¥å‚ï¼ˆæ”¯æŒå¤šç§é”®å + æ˜¾å¼å‚æ•°ï¼‰
        # æ”¯æŒçš„é”®åï¼štissue_type / tissue / tissue_description / ç»„ç»‡ / ç»„ç»‡ç±»å‹ / organ
        def _get_tissue_from_extra(d: Dict[str, Any]) -> Optional[str]:
            for key in (
                "tissue_type", "tissue", "tissue_description", "ç»„ç»‡", "ç»„ç»‡ç±»å‹", "organ"
            ):
                if key in d and d[key]:
                    try:
                        val = str(d[key]).strip()
                        if val:
                            return val
                    except Exception:
                        pass
            return None

        # è§„èŒƒåŒ– tissue åˆ° extra ä¸­
        if extra is None:
            extra = {}

        normalized_tissue = tissue_type or _get_tissue_from_extra(extra)
        if normalized_tissue:
            extra["tissue_type"] = normalized_tissue

        # ğŸŒŸ æ–°å¢ï¼šæ·»åŠ  species åˆ° extra
        if species:
            extra["species"] = species
            # ä¸éƒ¨åˆ†å·¥å…·çš„å‚æ•°åä¿æŒå…¼å®¹ï¼ˆenhanced_cell_annotation ä½¿ç”¨ tissue_descriptionï¼‰
            extra.setdefault("tissue_description", normalized_tissue)

        # è§„èŒƒåŒ– cluster_column åˆ° extra ä¸­
        if cluster_column:
            extra["cluster_column"] = cluster_column

        if not user_input:
            has_data_path = extra and any(k in extra for k in ("data_path", "rds_path", "h5ad_path", "h5_path"))
            has_tissue = extra and any(k in extra for k in ("tissue_type", "tissue_description"))
            if has_data_path and has_tissue:
                user_input = "è¯·åœ¨æŒ‡å®šç»„ç»‡èƒŒæ™¯ä¸‹å®Œæˆç»†èƒç±»å‹åˆ†æä¸æ³¨é‡Šï¼Œå¹¶ç»™å‡ºæœ€ç»ˆç»“æœ"
            elif has_data_path:
                user_input = "è¯·å¯¹æä¾›çš„æ•°æ®è¿›è¡Œç»†èƒç±»å‹åˆ†æä¸æ³¨é‡Šï¼Œå¹¶ç»™å‡ºæœ€ç»ˆç»“æœ"
            else:
                user_input = "æ‰§è¡Œå®Œæ•´çš„ç»†èƒç±»å‹åˆ†ææµç¨‹"

        system_prompt = self._get_system_prompt()
        user_query = self._build_user_query(user_input, extra=extra)
        messages: List[Dict[str, str]] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"<question>{user_query}</question>"},
        ]

        analysis_log: List[Dict[str, Any]] = []
        iteration = 0
        final_answer = None

        while True:
            iteration += 1

            # é•¿ä¸Šä¸‹æ–‡æ§åˆ¶
            total_chars = sum(len(m.get("content", "")) for m in messages)
            if total_chars > self.max_context_length or iteration == self.context_summary_threshold:
                messages = await self._summarize_context(messages)

            response = await self._call_openai(messages, timeout=self.api_timeout, stream=self.enable_streaming)
            self._log_info(f"ğŸ“ AIå“åº”é•¿åº¦: {len(response)}")

            # éªŒè¯å“åº”æ ¼å¼
            validation = self.validator.validate_response_format(response)

            if not validation['valid']:
                self._log_error(f"âŒ æ ¼å¼éªŒè¯å¤±è´¥: {', '.join(validation['issues'])}")
                response = await self._retry_llm_call_with_correction(messages, validation, 10, self.api_timeout)
            else:
                self._log_success("âœ… æ ¼å¼éªŒè¯é€šè¿‡")

            analysis_log.append({
                "iteration": iteration,
                "response": response,
                "validation": validation,
                "type": "ai_response"
            })

            # ğŸš¨ é¢„æ£€æŸ¥ï¼šæ£€æµ‹å¯èƒ½çš„è·³è¿‡è¡Œä¸º
            skip_patterns = [
                r"è·³è¿‡å‰©ä½™",
                r"æ—¶é—´å…³ç³»",
                r"è·³è¿‡.*?ç°‡",
                r"çœç•¥.*?åˆ†æ",
                r"ç›´æ¥è¿›å…¥åç»­",
                r"ç»“æŸå¤„ç†",
                r"è·³è¿‡.*?cluster",
                r"ç”±äº.*?è·³è¿‡",
                r"å°†è·³è¿‡.*?çš„è¯¦ç»†åˆ†æ"
            ]
            skip_detected = any(re.search(pattern, response, re.IGNORECASE) for pattern in skip_patterns)

            if skip_detected and '</final_answer>' in response:
                self._log_warning("ğŸš¨ æ£€æµ‹åˆ°å°è¯•è·³è¿‡å¾ªç¯çš„è¡Œä¸ºï¼Œå¼ºåˆ¶è¦æ±‚ç»§ç»­å¤„ç†")
                from agentype.prompts import get_prompt_manager

                manager = get_prompt_manager(self.language)
                skip_message = manager.get_agent_specific_prompt('mainagent', 'SKIP_DETECTION_INITIAL_MESSAGE')

                skip_validation_result = {
                    "valid": False,
                    "issues": skip_message['issues']
                }
                response = await self._retry_llm_call_with_correction(messages, skip_validation_result, 10, self.api_timeout)
                analysis_log.append({
                    "iteration": iteration,
                    "response": response,
                    "validation": {"valid": False, "issues": ["æ£€æµ‹åˆ°è·³è¿‡è¡Œä¸ºï¼Œå·²å¼ºåˆ¶é‡è¯•"]},
                    "type": "ai_response_retry_for_skip_prevention"
                })

            # æ£€æŸ¥æ˜¯å¦åŒ…å« final_answer
            if '</final_answer>' in response:
                final_result = ReactParser.extract_final_answer(response)
                if final_result:
                    # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿å…¨å±€é…ç½®å­˜åœ¨
                    from agentype.mainagent.tools.file_paths_tools import set_global_config, _GLOBAL_CONFIG
                    if _GLOBAL_CONFIG is None:
                        # å¦‚æœå…¨å±€é…ç½®ä¸¢å¤±ï¼Œé‡æ–°è®¾ç½®
                        set_global_config(self.config)
                        self._log_warning("âš ï¸ æ£€æµ‹åˆ°å…¨å±€é…ç½®ä¸¢å¤±ï¼Œå·²é‡æ–°åˆå§‹åŒ–")

                    # ç›´æ¥ä» bundle è¯»å–æ–‡ä»¶è·¯å¾„
                    self._log_info("ğŸ“¦ ä» bundle è¯»å–æ–‡ä»¶è·¯å¾„ä¿¡æ¯...")
                    bundle_result = load_file_paths_bundle()

                    if bundle_result.get("success"):
                        # ä»æ‰å¹³åŒ–ç»“æ„ä¸­æå–æ‰€æœ‰è·¯å¾„å­—æ®µï¼ˆæ’é™¤å…ƒæ•°æ®å­—æ®µï¼‰
                        metadata_keys = {"success", "session_id", "timestamp", "metadata", "cluster_mapping"}
                        extracted_paths = {k: v for k, v in bundle_result.items() if k not in metadata_keys and v}

                        # ğŸ†• æµç¨‹å®Œæˆåº¦éªŒè¯ï¼šæ£€æŸ¥å¿…è¦çš„è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        required_outputs = ['annotated_rds', 'annotated_h5ad']  # Phase 6 å’Œ Phase 7 çš„è¾“å‡º
                        missing_outputs = [key for key in required_outputs if not extracted_paths.get(key)]

                        if missing_outputs:
                            # æ£€æµ‹åˆ°ç¼ºå¤±çš„è¾“å‡ºæ–‡ä»¶
                            if not self.workflow_completion_reminder_sent:
                                # ç¬¬ä¸€æ¬¡æ£€æµ‹åˆ°ç¼ºå¤±ï¼Œå‘é€æé†’
                                self._log_warning(f"âš ï¸ æ£€æµ‹åˆ°å·¥ä½œæµæœªå®Œå…¨å®Œæˆï¼Œç¼ºå°‘: {', '.join(missing_outputs)}")
                                self._log_info("ğŸ“¢ æé†’ LLM ç»§ç»­æ‰§è¡Œå‰©ä½™é˜¶æ®µ...")

                                # ğŸ†• ä»promptè·å–å·¥ä½œæµå®Œæˆåº¦éªŒè¯æ¶ˆæ¯
                                from agentype.prompts import get_prompt_manager

                                manager = get_prompt_manager(self.language)
                                workflow_message = manager.get_agent_specific_prompt(
                                    'mainagent',
                                    'WORKFLOW_COMPLETION_INITIAL_MESSAGE'
                                )

                                # æ ¼å¼åŒ– issues åˆ—è¡¨ï¼Œå¡«å……ç¼ºå¤±çš„è¾“å‡ºæ–‡ä»¶
                                formatted_issues = [
                                    issue.format(missing_outputs=', '.join(missing_outputs))
                                    if '{missing_outputs}' in issue
                                    else issue
                                    for issue in workflow_message['issues']
                                ]

                                validation_result = {
                                    "valid": False,
                                    "issues": formatted_issues
                                }

                                # è®¾ç½®æ ‡å¿—ä½ï¼Œè®°å½•å·²å‘é€æé†’
                                self.workflow_completion_reminder_sent = True

                                # é‡æ–°è°ƒç”¨ LLMï¼Œè¦æ±‚ç»§ç»­æ‰§è¡Œ
                                response = await self._retry_llm_call_with_correction(
                                    messages, validation_result, 10, self.api_timeout
                                )
                                analysis_log.append({
                                    "iteration": iteration,
                                    "response": response,
                                    "validation": {"valid": False, "issues": ["å·¥ä½œæµæœªå®Œæˆï¼Œå·²å‘é€æé†’"]},
                                    "type": "ai_response_retry_for_workflow_completion"
                                })
                                continue  # ç»§ç»­å¾ªç¯ï¼Œä¸ break
                            else:
                                # ç¬¬äºŒæ¬¡ä»ç„¶ç¼ºå¤±ï¼Œæ¥å—ç»“æŸ
                                self._log_warning(f"âš ï¸ å·²æé†’ LLM ä½†ä»è¾“å‡º final_answerï¼Œæ¥å—å½“å‰ç»“æœ")
                                self._log_info(f"ğŸ“ æ³¨æ„ï¼šç¼ºå°‘è¾“å‡ºæ–‡ä»¶ {', '.join(missing_outputs)}")
                                # ç»§ç»­æ‰§è¡Œï¼Œå…è®¸é€€å‡º

                        self._log_success(f"âœ… åˆ†æå®Œæˆï¼Œç»“æœ: {final_result}")
                        self._log_info(f"ğŸ“ å·²ä» bundle è¯»å–åˆ°è·¯å¾„ä¿¡æ¯")

                        # ç›´æ¥ä½¿ç”¨ final_resultï¼Œä¸æ·»åŠ  token ç»Ÿè®¡æŠ¥å‘Š
                        final_answer = final_result
                        break
                    else:
                        # Bundle è¯»å–å¤±è´¥
                        self._log_error("âŒ æ— æ³•ä» bundle è¯»å–æ–‡ä»¶è·¯å¾„")
                        self._log_error(f"   Bundle è¿”å›: {bundle_result}")
                        validation_result = {
                            "valid": False,
                            "issues": [
                                "ç³»ç»Ÿæ— æ³•ä» bundle è¯»å–æ–‡ä»¶è·¯å¾„ä¿¡æ¯",
                                "å¯èƒ½åŸå› ï¼šå·¥å…·æœªæ­£ç¡®ä¿å­˜è·¯å¾„åˆ° bundle",
                                "è¯·æ£€æŸ¥ä¹‹å‰çš„å·¥å…·è°ƒç”¨æ˜¯å¦æˆåŠŸæ‰§è¡Œ"
                            ]
                        }
                        response = await self._retry_llm_call_with_correction(messages, validation_result, 10, self.api_timeout)
                        analysis_log.append({
                            "iteration": iteration,
                            "response": response,
                            "validation": {"valid": False, "issues": ["Bundleè·¯å¾„è¯»å–å¤±è´¥ï¼Œå·²é‡è¯•"]},
                            "type": "ai_response_retry_for_bundle_failure"
                        })
                        continue
                else:
                    self._log_warning("âš ï¸ å‘ç°final_answeræ ‡ç­¾ä½†æ— æ³•æå–å†…å®¹")

            # æå–å¹¶æ‰§è¡Œ action
            action = ReactParser.extract_action(response, self.available_tools)
            if action:
                # æ£€æŸ¥æ˜¯å¦æœ‰è§£æé”™è¯¯
                if 'error' in action:
                    error_msg = action.get('message', action.get('error'))
                    self._log_warning(f"Actionè§£æé”™è¯¯: {error_msg}")
                    # æ„é€ observationè®©LLMçŸ¥é“é—®é¢˜
                    error_observation = f"<observation>{{\"error\": \"{error_msg}\"}}</observation>"
                    messages.append({"role": "user", "content": error_observation})
                    continue

                function_name = action['function']
                parameters_str = action['parameters']
                parameters = ReactParser.parse_parameters(parameters_str)

                self._log_info(f"ğŸ”§ è°ƒç”¨å·¥å…·: {function_name}, å‚æ•°: {parameters}")

                # æ‰§è¡Œ MCP å·¥å…·è°ƒç”¨
                tool_result = await self.mcp_client.call_tool(function_name, parameters)

                if tool_result and tool_result.get("success"):
                    raw_content = tool_result.get("content", "")
                    self._log_success(f"âœ… å·¥å…·è°ƒç”¨æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(raw_content)}")

                    # å¤„ç†å†…å®¹é•¿åº¦
                    result_content = await self.content_processor.process_tool_result_content(
                        raw_content, self._call_openai, self.language,
                        tool_name=function_name, tool_params=parameters, mcp_client=self.mcp_client
                    )
                else:
                    result_content = json.dumps({
                        "error": tool_result.get("error", "å·¥å…·è°ƒç”¨å¤±è´¥") if tool_result else "æ— å“åº”",
                        "success": False
                    }, ensure_ascii=False)
                    self._log_error("âŒ å·¥å…·è°ƒç”¨å¤±è´¥")

                # è®°å½•åˆ†ææ—¥å¿—
                analysis_log.append({
                    "iteration": iteration,
                    "action": action,
                    "result": result_content,
                    "type": "tool_call"
                })

                # ğŸ’° å·¥å…·è°ƒç”¨åæ”¶é›†tokenç»Ÿè®¡
                try:
                    tool_token_summary = await self._collect_all_token_stats()
                    simple_report = tool_token_summary.get('simple_report', '')

                    if simple_report:
                        self._log_info(f"ğŸ’° {simple_report}")
                except Exception as e:
                    self._log_warning(f"âš ï¸ Tokenç»Ÿè®¡å¤±è´¥: {e}")

                # ğŸ” è¿›åº¦ç›‘æ§ï¼šç‰¹å®šå·¥å…·è°ƒç”¨åæ£€æŸ¥ç°‡å®Œæˆåº¦
                progress_reminder = ""
                if function_name in ["enhanced_gene_analysis", "save_cluster_type"]:
                    try:
                        # å°è¯•ä»ä¹‹å‰çš„åˆ†ææ—¥å¿—ä¸­æ‰¾åˆ°markeråŸºå› æ–‡ä»¶è·¯å¾„
                        marker_genes_path = None
                        for log_entry in reversed(analysis_log):
                            if log_entry.get("type") == "tool_call" and "marker_genes_json" in str(log_entry.get("result", "")):
                                result_str = str(log_entry.get("result", ""))
                                if ".agentype_cache" in result_str and "cluster_marker_genes" in result_str:
                                    # ç®€å•æå–è·¯å¾„
                                    path_match = re.search(r'/[^\s"]+cluster_marker_genes[^\s"]*\.json', result_str)
                                    if path_match:
                                        marker_genes_path = path_match.group()
                                        break

                        if marker_genes_path:
                            completion_status = check_cluster_completion(marker_genes_path)
                            if completion_status.get("success", False):
                                completed = completion_status.get("completed_clusters", 0)
                                total = completion_status.get("total_clusters", 0)
                                rate = completion_status.get("completion_rate", 0)

                                from agentype.prompts import get_prompt_manager

                                manager = get_prompt_manager(self.language)

                                if not completion_status.get("all_completed", False):
                                    incomplete = completion_status.get("incomplete_clusters", [])
                                    reminder_template = manager.get_agent_specific_prompt('mainagent', 'PROGRESS_REMINDER_TEMPLATE')
                                    progress_reminder = reminder_template.format(
                                        completed=completed,
                                        total=total,
                                        incomplete_clusters=', '.join(incomplete[:5])
                                    )
                                else:
                                    complete_template = manager.get_agent_specific_prompt('mainagent', 'CLUSTER_PROGRESS_COMPLETE_MESSAGE')
                                    next_action = manager.get_agent_specific_prompt('mainagent', 'CLUSTER_PROGRESS_ACTION_GENERAL_NEXT')
                                    completion_message = complete_template.format(
                                        total_clusters=total,
                                        next_action=next_action
                                    )
                                    progress_reminder = f"\n\n{completion_message}"

                                self._log_info(progress_reminder)
                    except Exception as e:
                        self._log_warning(f"âš ï¸ è¿›åº¦ç›‘æ§æ£€æŸ¥å¤±è´¥: {e}")

                # æ„é€  observation å¹¶æ·»åŠ åˆ°å¯¹è¯
                observation = f"<observation>{result_content}{progress_reminder}</observation>"
                messages.append({"role": "assistant", "content": response})
                messages.append({"role": "user", "content": observation})
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ä¸”æ²¡æœ‰final_answerï¼Œå¼ºåˆ¶è¦æ±‚LLMæä¾›final_answer
                self._log_warning("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨ä¸”æ— final_answerï¼Œè¦æ±‚æä¾›æœ€ç»ˆç­”æ¡ˆ")
                validation_result = {
                    "valid": False,
                    "issues": ["å¿…é¡»æä¾›<final_answer>æ ‡ç­¾å’Œæ–‡ä»¶è·¯å¾„ä¿¡æ¯æ¥å®Œæˆåˆ†æ"]
                }
                response = await self._retry_llm_call_with_correction(messages, validation_result, 10, self.api_timeout)
                # é‡æ–°åˆ†ææ–°çš„å“åº”
                analysis_log.append({
                    "iteration": iteration,
                    "response": response,
                    "validation": {"valid": False, "issues": ["ç¼ºå°‘final_answerï¼Œå·²é‡è¯•"]},
                    "type": "ai_response_retry_for_final_answer"
                })
                continue

        # ä½¿ç”¨æ–°çš„ä¼˜å…ˆçº§æ–¹æ³•è§£ææ–‡ä»¶è·¯å¾„
        # å¦‚æœå¾ªç¯æ­£å¸¸ç»“æŸï¼ˆæœ‰final_answerï¼‰ï¼Œåº”è¯¥å·²ç»æå–äº†paths
        extracted_paths = {}
        final_cluster_completion_status = None

        # æŸ¥æ‰¾æœ€åä¸€ä¸ªåŒ…å«final_answerçš„AIå“åº”
        last_final_answer_response = None
        for log_entry in reversed(analysis_log):
            if log_entry.get('type') == 'ai_response':
                response_content = log_entry.get('response', '')
                if ReactParser.has_final_answer(response_content):
                    last_final_answer_response = response_content
                    break

        # æŸ¥æ‰¾æœ€åä¸€æ¬¡ç°‡å®Œæˆåº¦æ£€æŸ¥ç»“æœ
        for log_entry in reversed(analysis_log):
            if log_entry.get('type') == 'cluster_completion_verification':
                final_cluster_completion_status = log_entry.get('cluster_completion_check')
                break

        if last_final_answer_response:
            # ä» bundle è¯»å–è·¯å¾„ä¿¡æ¯
            bundle_result = load_file_paths_bundle()
            if bundle_result.get("success"):
                # ä»æ‰å¹³åŒ–ç»“æ„ä¸­æå–æ‰€æœ‰è·¯å¾„å­—æ®µï¼ˆæ’é™¤å…ƒæ•°æ®å­—æ®µï¼‰
                metadata_keys = {"success", "session_id", "timestamp", "metadata", "cluster_mapping"}
                extracted_paths = {k: v for k, v in bundle_result.items() if k not in metadata_keys}
                valid_paths = {k: v for k, v in extracted_paths.items() if v}
                if valid_paths:
                    self._log_header("\nğŸ“ æœ€ç»ˆæ–‡ä»¶è·¯å¾„ (ä» bundle è¯»å–):")
                    for file_type, path in valid_paths.items():
                        self._log_info(f"   {file_type}: {path}")
            else:
                self._log_warning("âš ï¸ æ— æ³•ä» bundle è¯»å–æ–‡ä»¶è·¯å¾„")
        else:
            self._log_warning("âš ï¸ æœªæ‰¾åˆ°åŒ…å«final_answerçš„å“åº”")

        self._log_header(f"\nğŸ“Š åˆ†ææ€»ç»“:")
        self._log_info(f"æ€»è¿­ä»£æ¬¡æ•°: {iteration}")
        self._log_info(f"æœ€ç»ˆç»“æœ: {final_answer}")
        self._log_info(f"å·¥å…·è°ƒç”¨æ¬¡æ•°: {len([log for log in analysis_log if log['type'] == 'tool_call'])}")

        # æ‰“å°ç°‡å®Œæˆåº¦ä¿¡æ¯
        if final_cluster_completion_status:
            self._log_header(f"ğŸ” ç°‡æ³¨é‡Šå®Œæˆæƒ…å†µ:")
            if final_cluster_completion_status.get("success", False):
                completed = final_cluster_completion_status.get("completed_clusters", 0)
                total = final_cluster_completion_status.get("total_clusters", 0)
                rate = final_cluster_completion_status.get("completion_rate", 0)
                self._log_success(f"   âœ… å·²å®Œæˆ: {completed}/{total} ({rate:.1%})")

                if final_cluster_completion_status.get("all_completed", False):
                    self._log_success(f"   ğŸ‰ æ‰€æœ‰ç°‡å·²å®Œæˆæ³¨é‡Š")
                else:
                    incomplete = final_cluster_completion_status.get("incomplete_clusters", [])
                    self._log_warning(f"   âš ï¸  æœªå®Œæˆ: {', '.join(incomplete[:3])}{'...' if len(incomplete) > 3 else ''}")
            else:
                self._log_error(f"   âŒ æ£€æŸ¥å¤±è´¥: {final_cluster_completion_status.get('error', 'æœªçŸ¥é”™è¯¯')}")
        else:
            self._log_info(f"   â„¹ï¸  æœªè¿›è¡Œç°‡å®Œæˆåº¦æ£€æŸ¥")

        if iteration >= self.context_summary_threshold:
            self._log_info(f"ä¸Šä¸‹æ–‡æ€»ç»“: å·²åœ¨ç¬¬{self.context_summary_threshold}æ¬¡è¿­ä»£æ—¶è§¦å‘")

        # æ‰“å° LLM æ—¥å¿—ç»Ÿè®¡
        if self.llm_logger:
            try:
                log_summary = self.llm_logger.get_log_summary()
                self._log_info(f"LLMç»Ÿè®¡: æ€»è¯·æ±‚{log_summary.get('total_requests', 0)}æ¬¡")
                self._log_info(f"æˆåŠŸ/å¤±è´¥: {log_summary.get('success_count', 0)}/{log_summary.get('error_count', 0)}")
                self._log_info(f"æ—¥å¿—æ–‡ä»¶: {log_summary.get('log_file', 'N/A')}")
            except Exception as e:
                self._log_warning(f"è·å–LLMç»Ÿè®¡å¤±è´¥: {e}")

        # åˆ¤æ–­æœ€ç»ˆæˆåŠŸæ¡ä»¶
        all_clusters_completed = True
        if final_cluster_completion_status:
            all_clusters_completed = final_cluster_completion_status.get("all_completed", False)
            if final_cluster_completion_status.get("success", True):
                # å¦‚æœæ£€æŸ¥æˆåŠŸï¼Œä½¿ç”¨æ£€æŸ¥ç»“æœ
                pass
            else:
                # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤ä¸ºå·²å®Œæˆï¼ˆé¿å…é˜»æ­¢æµç¨‹ï¼‰
                all_clusters_completed = True

        final_success = final_answer is not None and bool(extracted_paths) and all_clusters_completed

        # æ”¶é›†æ‰€æœ‰Agentçš„tokenç»Ÿè®¡
        token_summary = await self._collect_all_token_stats()

        # è·å–å½“å‰ session_id
        from agentype.mainagent.config.session_config import get_session_id
        current_session_id = get_session_id()

        # æ„å»ºå®Œæ•´çš„è¿”å›ç»“æœ
        result_data = {
            "session_id": current_session_id,  # ä¼šè¯IDï¼Œç”¨äºè¿½è¸ªå’Œæ—¥å¿—
            "language": self.language,  # å½“å‰ä½¿ç”¨çš„è¯­è¨€ï¼ˆzh/enï¼‰
            "api_base": self.config.openai_api_base,  # API URL
            "model": self.config.openai_model,  # ä½¿ç”¨çš„æ¨¡å‹
            "input_data": user_input,
            "final_result": final_answer,
            "output_file_paths": extracted_paths,  # è§£æå‡ºçš„æ–‡ä»¶è·¯å¾„
            "total_iterations": iteration,
            "context_summarized": iteration >= self.context_summary_threshold,
            "context_summary_threshold": self.context_summary_threshold,
            "analysis_log": analysis_log,
            "llm_log_summary": self.llm_logger.get_log_summary() if self.llm_logger else None,
            "cluster_completion_status": final_cluster_completion_status,
            "all_clusters_completed": all_clusters_completed,
            "token_stats": token_summary,
            "success": final_success
        }

        # ===== ä¿å­˜ JSON ç»“æœåˆ°æ–‡ä»¶ =====
        try:
            import json
            from datetime import datetime
            # ä½¿ç”¨ results/celltypeMainagent ç›®å½•
            results_dir = self.config.get_results_dir()

            # æ„å»ºæ–‡ä»¶è·¯å¾„: outputs/results/celltypeMainagent/result_{session_id}.json
            result_filename = f"result_{current_session_id}.json"
            result_file_path = results_dir / result_filename

            # ä¿å­˜ JSON æ–‡ä»¶
            with open(result_file_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)

            # æ·»åŠ ä¿å­˜ä¿¡æ¯åˆ°è¿”å›ç»“æœ
            result_data["result_file"] = str(result_file_path)
            result_data["result_saved_at"] = datetime.now().isoformat()

            self._log_success(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {result_file_path}")

        except Exception as e:
            self._log_warning(f"âš ï¸ ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            # ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            result_data["result_file"] = None
            result_data["result_save_error"] = str(e)

        # è¿”å›å®Œæ•´ç»“æœï¼ˆåŒ…å«ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼‰
        return result_data

    async def _collect_all_token_stats(self) -> Dict[str, Any]:
        """æ”¶é›†æ‰€æœ‰Agentçš„tokenç»Ÿè®¡ä¿¡æ¯

        ç­–ç•¥ï¼š
        æ‰€æœ‰ Agent (åŒ…æ‹¬ MainAgent) ç»Ÿä¸€é€šè¿‡æ—¥å¿—æ–‡ä»¶è§£æç»Ÿè®¡
        """
        try:
            # è·å–å½“å‰ session_id
            from agentype.mainagent.config.session_config import get_session_id
            current_session_id = get_session_id()

            # åˆå§‹åŒ–æ—¥å¿—è§£æå™¨
            # ä½¿ç”¨é…ç½®çš„æ—¥å¿—ç›®å½•
            from pathlib import Path
            log_base_dir = str(Path(self.config.log_dir) / "llm")

            self._log_info(f"ğŸ“‚ ä»æ—¥å¿—ç›®å½•è§£ææ‰€æœ‰ Agent ç»Ÿè®¡: {log_base_dir}")

            # å®šä¹‰æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ Agent (åŒ…æ‹¬ MainAgent)
            agents_to_query = ["MainAgent", "SubAgent", "DataAgent", "AppAgent"]
            all_agent_stats = {}

            try:
                log_parser = LogTokenParser(log_base_dir)

                # è§£ææ¯ä¸ª Agent çš„æ—¥å¿—
                for agent_name in agents_to_query:
                    try:
                        stats = log_parser.parse_agent_logs(agent_name, current_session_id)
                        all_agent_stats[agent_name] = stats

                        if stats.total_tokens > 0:
                            prompt_str = f"{stats.prompt_tokens:,}"
                            completion_str = f"{stats.completion_tokens:,}"
                            total_str = f"{stats.total_tokens:,}"
                            self._log_success(
                                f"âœ… {agent_name}: {total_str} tokens "
                                f"(è¾“å…¥: {prompt_str}, è¾“å‡º: {completion_str}) "
                                f"({stats.request_count} æ¬¡è¯·æ±‚)"
                            )
                        else:
                            self._log_info(f"ğŸ“­ {agent_name}: æš‚æ—  token æ¶ˆè€—")

                    except Exception as e:
                        self._log_warning(f"âš ï¸ è§£æ {agent_name} æ—¥å¿—å¤±è´¥: {e}")
                        # åˆ›å»ºç©ºç»Ÿè®¡å¯¹è±¡ä½œä¸ºå ä½ç¬¦
                        all_agent_stats[agent_name] = TokenStatistics(agent_name=agent_name)

            except Exception as e:
                self._log_warning(f"âš ï¸ åˆå§‹åŒ–æ—¥å¿—è§£æå™¨å¤±è´¥: {e}")
                # é™çº§ï¼šæ‰€æœ‰ Agent ä½¿ç”¨ç©ºç»Ÿè®¡
                for agent_name in agents_to_query:
                    all_agent_stats[agent_name] = TokenStatistics(agent_name=agent_name)

            # åˆ†ç¦» MainAgent å’Œå­ Agent
            main_agent_stats = all_agent_stats.get("MainAgent", TokenStatistics(agent_name="MainAgent"))
            sub_agent_stats = {k: v for k, v in all_agent_stats.items() if k != "MainAgent"}

            # åˆå¹¶æ‰€æœ‰tokenç»Ÿè®¡
            total_stats = merge_token_stats(list(all_agent_stats.values()))
            total_stats.agent_name = "Total"

            self._log_success(f"ğŸ“Š æ€» Token ç»Ÿè®¡: {total_stats.total_tokens:,} tokens")

            # ç”ŸæˆæŠ¥å‘Šï¼Œä» total_stats ä¸­è·å– api_baseï¼ˆç”±æ—¥å¿—è§£æå™¨æå–ï¼‰
            # ä¼˜å…ˆä½¿ç”¨ total_stats.api_baseï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨ config
            api_base = total_stats.api_base if total_stats.api_base else self.config.openai_api_base
            simple_report = self.token_reporter.generate_simple_report(total_stats, api_base=api_base)
            detailed_report = self.token_reporter.generate_detailed_report(total_stats, sub_agent_stats, api_base=api_base)

            return {
                "main_agent": main_agent_stats.get_summary(api_base=api_base),
                "sub_agents": {name: stats.get_summary(api_base=api_base) for name, stats in sub_agent_stats.items()},
                "total": total_stats.get_summary(api_base=api_base),
                "simple_report": simple_report,
                "detailed_report": detailed_report
            }

        except Exception as e:
            self._log_error(f"âŒ æ”¶é›†tokenç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "main_agent": {"error": str(e)},
                "sub_agents": {},
                "total": {"error": str(e)},
                "simple_report": f"ç»Ÿè®¡å¤±è´¥: {e}",
                "detailed_report": f"ç»Ÿè®¡å¤±è´¥: {e}",
                "error": str(e)
            }

    # å·²å»é™¤æ—§çš„å…¼å®¹æ€§å·¥ä½œæµå…¥å£ä¸è§£æå™¨åŒ…è£…ï¼Œç»Ÿä¸€ç”± React é©±åŠ¨
