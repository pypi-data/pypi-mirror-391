#!/usr/bin/env python3
"""
agentype - Data Processor MCP Server
Author: cuilei
Version: 1.0
"""

import sys
import asyncio
import json
import os
from typing import Optional, Dict, Any
from pathlib import Path

# å¯¼å…¥é¡¹ç›®æ¨¡å—

# å¯¼å…¥å›½é™…åŒ–æ”¯æŒ
try:
    from agentype.dataagent.utils.i18n import _
except ImportError:
    # å¦‚æœå›½é™…åŒ–æ¨¡å—ä¸å­˜åœ¨ï¼Œæä¾›ç®€å•çš„å ä½ç¬¦
    def _(key, **kwargs):
        return key.format(**kwargs) if kwargs else key

try:
    from mcp.server.fastmcp import FastMCP
    # ç›´æ¥å¯¼å…¥æ ¸å¿ƒå¤„ç†å‡½æ•°
    from agentype.dataagent.tools.data_converters import (
        run_r_findallmarkers,
        run_r_sce_to_h5,
        convert_r_markers_csv_to_json,
        easyscfpy_h5_to_json,
        scanpy_path_to_json,
        convert_scanpy_file_to_h5
    )
    # å¯¼å…¥é…ç½®å‡½æ•°ç”¨äºæ„å»ºå®é™…è¾“å‡ºè·¯å¾„
    from agentype.config import get_session_id_for_filename
    # å¯¼å…¥è·¯å¾„ç®¡ç†å·¥å…·
    from agentype.mainagent.tools.file_paths_tools import (
        save_file_paths_bundle as _save_file_paths_bundle,
        load_file_paths_bundle as _load_file_paths_bundle,
        load_and_validate_bundle as _load_and_validate_bundle
    )
except ImportError as e:
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    current_file = Path(__file__).resolve()
    # project_root ä¸å†éœ€è¦

    print(f"å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥MCPåŒ…å®‰è£…ï¼špip install mcp")
    print(f"å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}")
    #
    print(f"Pythonè·¯å¾„: {sys.path[:3]}")
    sys.exit(1)

# ç¼“å­˜ç›®å½•ï¼ˆå°†åœ¨ __main__ å—ä¸­åˆå§‹åŒ–ï¼Œä½¿ç”¨ ConfigManager æä¾›çš„è·¯å¾„ï¼‰
cache_dir = None

# å¯¼å…¥è·¯å¾„ç®¡ç†å™¨
try:
    from agentype.dataagent.utils.path_manager import normalize_path, get_absolute_paths
except ImportError:
    # ç®€å•çš„å¤‡ç”¨å®ç°
    def normalize_path(file_path):
        return str(Path(file_path).resolve()) if file_path else ""
    def get_absolute_paths(**kwargs):
        return {k: normalize_path(v) for k, v in kwargs.items()}

# åˆå§‹åŒ–FastMCPæœåŠ¡å™¨
mcp = FastMCP("celltype-data-processor", log_level="INFO")

# ç¼“å­˜ç›®å½•é…ç½®ï¼ˆåœ¨ __main__ å—ä¸­åˆå§‹åŒ–ï¼‰
CACHE_DIR = None

# é…ç½®å¯¹è±¡ï¼ˆåœ¨ __main__ å—ä¸­åˆå§‹åŒ–ï¼‰
_CONFIG = None

def ensure_cache_dir() -> str:
    """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
    os.makedirs(CACHE_DIR, exist_ok=True)
    return CACHE_DIR

@mcp.tool()
async def run_r_findallmarkers(
    seurat_file: Optional[str] = None,
    pval_threshold: float = 0.05
) -> str:
    """è¿è¡ŒR FindAllMarkersåˆ†æï¼Œè·å–å„èšç±»çš„æ ‡è®°åŸºå› ï¼ˆè‡ªåŠ¨åŒ–ç‰ˆæœ¬ï¼‰

    æ­¤å·¥å…·å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†markeråŸºå› åˆ†ææµç¨‹ï¼š
    1. è‡ªåŠ¨ä»bundleè¯»å–Seuratæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆrds_fileï¼‰
    2. è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨session_idå‘½å
    3. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜marker_genes_jsonè·¯å¾„

    Args:
        seurat_file: Seurat RDSæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä»bundleè¯»å–ï¼‰
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå·²åºŸå¼ƒï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
        pval_threshold: på€¼é˜ˆå€¼ï¼Œé»˜è®¤0.05

    Returns:
        JSONæ ¼å¼çš„åˆ†æç»“æœ
    """
    try:
        ensure_cache_dir()

        # 1. ä½¿ç”¨æ™ºèƒ½fallbackè‡ªåŠ¨è·å–Seuratæ–‡ä»¶è·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_get_input_path
            seurat_file = auto_get_input_path(
                manual_path=seurat_file,
                bundle_keys=['rds_file', 'sce_h5', 'h5_file'],
                tool_name='run_r_findallmarkers'
            )
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": f"è‡ªåŠ¨è·å–Seuratè·¯å¾„å¤±è´¥: {e}"
            }, ensure_ascii=False, indent=2)

        print(f"ğŸ”¬ è¿è¡ŒFindAllMarkersåˆ†æ: {seurat_file}")

        # åœ¨è°ƒç”¨å‰ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        session_id = get_session_id_for_filename()
        results_dir = _CONFIG.get_results_dir()
        actual_output_file = str(results_dir / f'cluster_markers_{session_id}.json')

        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œå™¨è°ƒç”¨å¤„ç†å‡½æ•°ï¼Œä¼ é€’ output_file å‚æ•°
        # æ³¨æ„ï¼šåº•å±‚å‡½æ•°ç°åœ¨è¿”å›Dictæˆ–æŠ›å‡ºRuntimeError
        from agentype.dataagent.tools.data_converters import run_r_findallmarkers as original_func

        try:
            result_dict = await asyncio.get_event_loop().run_in_executor(
                None, original_func, seurat_file, pval_threshold, "seurat_clusters", actual_output_file
            )

            # ä»Dictä¸­æå–markeråŸºå› æ•°æ®
            marker_genes = result_dict.get("marker_genes", {})

        except RuntimeError as e:
            return json.dumps({
                "success": False,
                "error": f"FindAllMarkersåˆ†æå¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)

        # ä¸ºMCPä¼ è¾“åˆ›å»ºç²¾ç®€ç‰ˆæœ¬ï¼ˆæ¯ä¸ªåˆ†ç°‡åªè¿”å›å‰10ä¸ªåŸºå› ï¼‰
        preview_marker_genes = {}
        for cluster, genes in marker_genes.items():
            preview_marker_genes[cluster] = genes[:10] if len(genes) > 10 else genes

        result = {
            "success": True,
            "input_file": seurat_file,
            "output_file": actual_output_file,
            "pval_threshold": pval_threshold,
            "marker_genes_preview": preview_marker_genes,
            "cluster_count": len(marker_genes),
            "total_genes": sum(len(genes) for genes in marker_genes.values()),
            "note": "å®Œæ•´çš„markeråŸºå› æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè¿™é‡Œåªæ˜¾ç¤ºæ¯ä¸ªåˆ†ç°‡çš„å‰10ä¸ªåŸºå› é¢„è§ˆ"
        }

        # 2. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜marker_genes_jsonè·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_update_bundle
            auto_update_bundle('marker_genes_json', actual_output_file)
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°bundleå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"è¿è¡ŒFindAllMarkersåˆ†ææ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def run_r_sce_to_h5(
    seurat_file: Optional[str] = None
) -> str:
    """å°†R SCEå¯¹è±¡è½¬æ¢ä¸ºH5æ ¼å¼ï¼ˆè‡ªåŠ¨åŒ–ç‰ˆæœ¬ï¼‰

    æ­¤å·¥å…·å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†SCEè½¬H5æµç¨‹ï¼š
    1. è‡ªåŠ¨ä»bundleè¯»å–Seuratæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆrds_fileï¼‰
    2. è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨session_idå‘½å
    3. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜sce_h5è·¯å¾„

    Args:
        seurat_file: Seurat RDSæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä»bundleè¯»å–ï¼‰
        output_file: è¾“å‡ºH5æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå·²åºŸå¼ƒï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰

    Returns:
        JSONæ ¼å¼çš„è½¬æ¢ç»“æœ
    """
    try:
        ensure_cache_dir()

        # 1. ä½¿ç”¨æ™ºèƒ½fallbackè‡ªåŠ¨è·å–Seuratæ–‡ä»¶è·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_get_input_path
            seurat_file = auto_get_input_path(
                manual_path=seurat_file,
                bundle_keys=['rds_file', 'h5_file'],
                tool_name='run_r_sce_to_h5'
            )
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": f"è‡ªåŠ¨è·å–Seuratè·¯å¾„å¤±è´¥: {e}"
            }, ensure_ascii=False, indent=2)

        print(f"ğŸ“ è½¬æ¢SCEä¸ºH5æ ¼å¼: {seurat_file}")

        # åœ¨è°ƒç”¨å‰ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        session_id = get_session_id_for_filename()
        results_dir = _CONFIG.get_results_dir()
        actual_output_file = str(results_dir / f'sce_{session_id}.h5')

        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œå™¨è°ƒç”¨å¤„ç†å‡½æ•°ï¼Œä¼ é€’ output_file å‚æ•°
        # æ³¨æ„ï¼šåº•å±‚å‡½æ•°ç°åœ¨è¿”å›Dictæˆ–æŠ›å‡ºRuntimeError
        from agentype.dataagent.tools.data_converters import run_r_sce_to_h5 as original_func

        try:
            h5_result = await asyncio.get_event_loop().run_in_executor(
                None, original_func, seurat_file, actual_output_file
            )

            # ä»Dictä¸­æå–æ•°æ®
            result = {
                "success": h5_result.get("success", True),
                "input_file": h5_result.get("input_file", seurat_file),
                "output_file": h5_result.get("output_file", actual_output_file),
                "file_size": h5_result.get("file_size", 0),
                "message": h5_result.get("message", "è½¬æ¢æˆåŠŸ")
            }

            # 2. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜sce_h5è·¯å¾„
            try:
                from agentype.mainagent.tools.file_paths_tools import auto_update_bundle
                auto_update_bundle('sce_h5', actual_output_file)
            except Exception as e:
                print(f"âš ï¸ æ›´æ–°bundleå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        except RuntimeError as e:
            return json.dumps({
                "success": False,
                "error": f"SCEè½¬H5æ ¼å¼å¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)

        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"è½¬æ¢SCEä¸ºH5æ ¼å¼æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def convert_r_markers_csv_to_json(
    csv_file: str,
    pval_threshold: float = 0.05
) -> str:
    """è½¬æ¢R FindAllMarkers CSVç»“æœä¸ºJSONæ ¼å¼
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        pval_threshold: på€¼é˜ˆå€¼ï¼Œé»˜è®¤0.05
        
    Returns:
        JSONæ ¼å¼çš„è½¬æ¢ç»“æœ
    """
    try:
        ensure_cache_dir()

        print(f"ğŸ”„ è½¬æ¢CSVä¸ºJSONæ ¼å¼: {csv_file}")

        # åœ¨è°ƒç”¨å‰ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        session_id = get_session_id_for_filename()
        results_dir = _CONFIG.get_results_dir()
        actual_output_file = str(results_dir / f'cluster_marker_genes_{session_id}.json')

        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œå™¨è°ƒç”¨å¤„ç†å‡½æ•°ï¼Œä¼ é€’ output_file å‚æ•°
        # æ³¨æ„ï¼šåº•å±‚å‡½æ•°ç°åœ¨è¿”å›Dictæˆ–æŠ›å‡ºRuntimeError
        from agentype.dataagent.tools.data_converters import convert_r_markers_csv_to_json as original_func

        try:
            result_dict = await asyncio.get_event_loop().run_in_executor(
                None, original_func, csv_file, pval_threshold, actual_output_file
            )

            # ä»Dictä¸­æå–markeråŸºå› æ•°æ®
            marker_genes = result_dict.get("marker_genes", {})

        except RuntimeError as e:
            return json.dumps({
                "success": False,
                "error": f"CSVè½¬JSONå¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)

        # ä¸ºMCPä¼ è¾“åˆ›å»ºç²¾ç®€ç‰ˆæœ¬ï¼ˆæ¯ä¸ªåˆ†ç°‡åªè¿”å›å‰10ä¸ªåŸºå› ï¼‰
        preview_marker_genes = {}
        for cluster, genes in marker_genes.items():
            preview_marker_genes[cluster] = genes[:10] if len(genes) > 10 else genes

        # è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜marker_genes_jsonè·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_update_bundle
            auto_update_bundle('marker_genes_json', actual_output_file)
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°bundleå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        result = {
            "success": True,
            "input_file": csv_file,
            "output_file": actual_output_file,
            "pval_threshold": pval_threshold,
            "marker_genes_preview": preview_marker_genes,
            "cluster_count": len(marker_genes),
            "total_genes": sum(len(genes) for genes in marker_genes.values()),
            "note": "å®Œæ•´çš„markeråŸºå› æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè¿™é‡Œåªæ˜¾ç¤ºæ¯ä¸ªåˆ†ç°‡çš„å‰10ä¸ªåŸºå› é¢„è§ˆ"
        }

        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"è½¬æ¢CSVä¸ºJSONæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def easyscfpy_h5_to_json(
    h5_file: Optional[str] = None,
    pval_threshold: float = 0.05
) -> str:
    """å°†easySCF H5æ–‡ä»¶è½¬æ¢ä¸ºJSONæ ¼å¼ï¼ˆè‡ªåŠ¨åŒ–ç‰ˆæœ¬ï¼‰

    æ­¤å·¥å…·å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†H5è½¬JSONæµç¨‹ï¼š
    1. è‡ªåŠ¨ä»bundleè¯»å–H5æ•°æ®æ–‡ä»¶è·¯å¾„
    2. è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨session_idå‘½å
    3. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜marker_genes_jsonè·¯å¾„

    Args:
        h5_file: H5æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä»bundleè¯»å–ï¼‰
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå·²åºŸå¼ƒï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
        pval_threshold: på€¼é˜ˆå€¼ï¼Œé»˜è®¤0.05

    Returns:
        JSONæ ¼å¼çš„è½¬æ¢ç»“æœ
    """
    try:
        ensure_cache_dir()

        # 1. ä½¿ç”¨æ™ºèƒ½fallbackè‡ªåŠ¨è·å–H5æ–‡ä»¶è·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_get_input_path
            h5_file = auto_get_input_path(
                manual_path=h5_file,
                bundle_keys=['sce_h5', 'scanpy_h5', 'h5_file'],
                tool_name='easyscfpy_h5_to_json'
            )
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": f"è‡ªåŠ¨è·å–H5è·¯å¾„å¤±è´¥: {e}"
            }, ensure_ascii=False, indent=2)

        print(f"ğŸ”„ è½¬æ¢easySCF H5ä¸ºJSONæ ¼å¼: {h5_file}")

        # åœ¨è°ƒç”¨å‰ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        session_id = get_session_id_for_filename()
        results_dir = _CONFIG.get_results_dir()
        actual_output_file = str(results_dir / f'cluster_marker_genes_{session_id}.json')

        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œå™¨è°ƒç”¨å¤„ç†å‡½æ•°ï¼Œä¼ é€’ output_file å‚æ•°
        # æ³¨æ„ï¼šåº•å±‚å‡½æ•°ç°åœ¨è¿”å›Dictæˆ–æŠ›å‡ºRuntimeError
        from agentype.dataagent.tools.data_converters import easyscfpy_h5_to_json as original_func

        try:
            result_dict = await asyncio.get_event_loop().run_in_executor(
                None, original_func, h5_file, pval_threshold, None, actual_output_file
            )

            # ä»Dictä¸­æå–markeråŸºå› æ•°æ®
            marker_genes = result_dict.get("marker_genes", {})

        except RuntimeError as e:
            return json.dumps({
                "success": False,
                "error": f"easySCF H5è½¬JSONå¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)

        # ä¸ºMCPä¼ è¾“åˆ›å»ºç²¾ç®€ç‰ˆæœ¬ï¼ˆæ¯ä¸ªåˆ†ç°‡åªè¿”å›å‰10ä¸ªåŸºå› ï¼‰
        preview_marker_genes = {}
        for cluster, genes in marker_genes.items():
            preview_marker_genes[cluster] = genes[:10] if len(genes) > 10 else genes

        result = {
            "success": True,
            "input_file": h5_file,
            "output_file": actual_output_file,
            "pval_threshold": pval_threshold,
            "marker_genes_preview": preview_marker_genes,
            "cluster_count": len(marker_genes),
            "total_genes": sum(len(genes) for genes in marker_genes.values()),
            "note": "å®Œæ•´çš„markeråŸºå› æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè¿™é‡Œåªæ˜¾ç¤ºæ¯ä¸ªåˆ†ç°‡çš„å‰10ä¸ªåŸºå› é¢„è§ˆ"
        }

        # 2. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜marker_genes_jsonè·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_update_bundle
            auto_update_bundle('marker_genes_json', actual_output_file)
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°bundleå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"è½¬æ¢easySCF H5ä¸ºJSONæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        }, ensure_ascii=False, indent=2)


@mcp.tool()
async def scanpy_path_to_json(
    scanpy_path: Optional[str] = None,
    pval_threshold: float = 0.05
) -> str:
    """å°†scanpyæ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºJSONæ ¼å¼ï¼ˆè‡ªåŠ¨åŒ–ç‰ˆæœ¬ï¼‰

    æ­¤å·¥å…·å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†scanpyè½¬JSONæµç¨‹ï¼š
    1. è‡ªåŠ¨ä»bundleè¯»å–scanpyæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆh5ad_fileï¼Œé™çº§h5_fileï¼‰
    2. è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨session_idå‘½å
    3. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜marker_genes_jsonè·¯å¾„

    Args:
        scanpy_path: scanpyæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä»bundleè¯»å–ï¼‰
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå·²åºŸå¼ƒï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
        pval_threshold: på€¼é˜ˆå€¼ï¼Œé»˜è®¤0.05

    Returns:
        JSONæ ¼å¼çš„è½¬æ¢ç»“æœ
    """
    try:
        ensure_cache_dir()

        # 1. ä½¿ç”¨æ™ºèƒ½fallbackè‡ªåŠ¨è·å–scanpyæ–‡ä»¶è·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_get_input_path
            scanpy_path = auto_get_input_path(
                manual_path=scanpy_path,
                bundle_keys=['h5ad_file', 'scanpy_h5', 'sce_h5', 'h5_file'],
                tool_name='scanpy_path_to_json'
            )
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": f"è‡ªåŠ¨è·å–scanpyè·¯å¾„å¤±è´¥: {e}"
            }, ensure_ascii=False, indent=2)

        print(f"ğŸ”¬ è½¬æ¢scanpyæ–‡ä»¶ä¸ºJSONæ ¼å¼: {scanpy_path}")

        # åœ¨è°ƒç”¨å‰ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        session_id = get_session_id_for_filename()
        results_dir = _CONFIG.get_results_dir()
        actual_output_file = str(results_dir / f'cluster_marker_genes_{session_id}.json')

        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œå™¨è°ƒç”¨å¤„ç†å‡½æ•°ï¼Œä¼ é€’ output_file å‚æ•°
        # æ³¨æ„ï¼šåº•å±‚å‡½æ•°ç°åœ¨è¿”å›Dictæˆ–æŠ›å‡ºRuntimeError
        from agentype.dataagent.tools.data_converters import scanpy_path_to_json as original_func

        try:
            result_dict = await asyncio.get_event_loop().run_in_executor(
                None, original_func, scanpy_path, pval_threshold, None, actual_output_file
            )

            # ä»Dictä¸­æå–markeråŸºå› æ•°æ®
            marker_genes = result_dict.get("marker_genes", {})

        except RuntimeError as e:
            return json.dumps({
                "success": False,
                "error": f"scanpyæ–‡ä»¶è½¬JSONå¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)

        # ä¸ºMCPä¼ è¾“åˆ›å»ºç²¾ç®€ç‰ˆæœ¬ï¼ˆæ¯ä¸ªåˆ†ç°‡åªè¿”å›å‰10ä¸ªåŸºå› ï¼‰
        preview_marker_genes = {}
        for cluster, genes in marker_genes.items():
            preview_marker_genes[cluster] = genes[:10] if len(genes) > 10 else genes

        result = {
            "success": True,
            "input_file": scanpy_path,
            "output_file": actual_output_file,
            "pval_threshold": pval_threshold,
            "marker_genes_preview": preview_marker_genes,
            "cluster_count": len(marker_genes),
            "total_genes": sum(len(genes) for genes in marker_genes.values()),
            "note": "å®Œæ•´çš„markeråŸºå› æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè¿™é‡Œåªæ˜¾ç¤ºæ¯ä¸ªåˆ†ç°‡çš„å‰10ä¸ªåŸºå› é¢„è§ˆ"
        }

        # 2. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜marker_genes_jsonè·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_update_bundle
            auto_update_bundle('marker_genes_json', actual_output_file)
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°bundleå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"è½¬æ¢scanpyæ–‡ä»¶ä¸ºJSONæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def convert_scanpy_file_to_h5(
    input_file: Optional[str] = None
) -> str:
    """å°†scanpyæ•°æ®æ–‡ä»¶è½¬æ¢ä¸ºeasySCF H5æ ¼å¼ï¼ˆè‡ªåŠ¨åŒ–ç‰ˆæœ¬ï¼‰

    æ­¤å·¥å…·å®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†scanpyè½¬H5æµç¨‹ï¼š
    1. è‡ªåŠ¨ä»bundleè¯»å–scanpyæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆh5ad_fileï¼Œé™çº§h5_fileï¼‰
    2. è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨session_idå‘½å
    3. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜scanpy_h5è·¯å¾„

    Args:
        input_file: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä»bundleè¯»å–ï¼‰
        h5_file: è¾“å‡ºH5æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå·²åºŸå¼ƒï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰

    Returns:
        JSONæ ¼å¼çš„è½¬æ¢ç»“æœ
    """
    try:
        ensure_cache_dir()

        # 1. ä½¿ç”¨æ™ºèƒ½fallbackè‡ªåŠ¨è·å–è¾“å…¥æ–‡ä»¶è·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_get_input_path
            input_file = auto_get_input_path(
                manual_path=input_file,
                bundle_keys=['h5ad_file', 'h5_file'],
                tool_name='convert_scanpy_file_to_h5'
            )
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": f"è‡ªåŠ¨è·å–è¾“å…¥æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}"
            }, ensure_ascii=False, indent=2)

        print(f"ğŸ”„ è½¬æ¢scanpyæ–‡ä»¶ä¸ºH5æ ¼å¼: {input_file}")

        # åœ¨è°ƒç”¨å‰ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        session_id = get_session_id_for_filename()
        results_dir = _CONFIG.get_results_dir()
        actual_output_file = str(results_dir / f'data_{session_id}.h5')

        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œå™¨è°ƒç”¨å¤„ç†å‡½æ•°ï¼Œä¼ é€’ output_file å‚æ•°
        # æ³¨æ„ï¼šåº•å±‚å‡½æ•°ç°åœ¨è¿”å›Dictæˆ–æŠ›å‡ºRuntimeError
        from agentype.dataagent.tools.data_converters import convert_scanpy_file_to_h5 as original_func

        try:
            result_dict = await asyncio.get_event_loop().run_in_executor(
                None, original_func, input_file, actual_output_file
            )

            # ä»Dictä¸­æå–æ•°æ®
            file_size = result_dict.get("file_size", 0)
            message = result_dict.get("message", "è½¬æ¢æˆåŠŸ")

            result = {
                "success": True,
                "message": message,
                "input_file": input_file,
                "output_file": actual_output_file,
                "file_size": f"{file_size / 1024 / 1024:.1f} MB" if file_size > 1024*1024 else f"{file_size / 1024:.1f} KB" if file_size > 1024 else f"{file_size} bytes"
            }

        except RuntimeError as e:
            return json.dumps({
                "success": False,
                "error": f"scanpyæ–‡ä»¶è½¬H5å¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)

        # 2. è‡ªåŠ¨æ›´æ–°bundleï¼Œä¿å­˜scanpy_h5è·¯å¾„
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_update_bundle
            auto_update_bundle('scanpy_h5', actual_output_file)
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°bundleå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"è½¬æ¢scanpyæ–‡ä»¶ä¸ºH5æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def validate_json_only(marker_genes_json: str) -> str:
    """ä¸“é—¨éªŒè¯JSONæ–‡ä»¶æ ¼å¼å’Œå†…å®¹

    Args:
        marker_genes_json: MarkeråŸºå› JSONæ–‡ä»¶è·¯å¾„

    Returns:
        JSONæ ¼å¼çš„éªŒè¯ç»“æœ
    """
    try:
        if not os.path.exists(marker_genes_json):
            return json.dumps({
                "success": False,
                "error": f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {marker_genes_json}",
                "valid": False
            }, ensure_ascii=False, indent=2)

        file_path_obj = Path(marker_genes_json)

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if file_path_obj.suffix.lower() != '.json':
            return json.dumps({
                "success": False,
                "error": f"æ–‡ä»¶ä¸æ˜¯JSONæ ¼å¼: {marker_genes_json}",
                "valid": False,
                "file_extension": file_path_obj.suffix
            }, ensure_ascii=False, indent=2)

        # è¯»å–å¹¶éªŒè¯JSONå†…å®¹
        try:
            with open(marker_genes_json, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
        except json.JSONDecodeError as e:
            return json.dumps({
                "success": False,
                "error": f"JSONæ ¼å¼é”™è¯¯: {str(e)}",
                "valid": False,
                "json_decode_error": str(e)
            }, ensure_ascii=False, indent=2)
        
        file_size = file_path_obj.stat().st_size
        
        # åˆ†æJSONå†…å®¹ç»“æ„
        content_info = {
            "data_type": type(json_data).__name__,
            "size_bytes": file_size,
            "size_readable": f"{file_size / 1024:.1f} KB" if file_size > 1024 else f"{file_size} bytes"
        }
        
        # å¦‚æœæ˜¯å­—å…¸ï¼Œåˆ†æé”®å€¼
        if isinstance(json_data, dict):
            content_info.update({
                "keys": list(json_data.keys()),
                "key_count": len(json_data.keys()),
                "is_marker_genes": "cluster" in str(json_data).lower() or "gene" in str(json_data).lower()
            })
        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆ†æå…ƒç´ 
        elif isinstance(json_data, list):
            content_info.update({
                "list_length": len(json_data),
                "first_element_type": type(json_data[0]).__name__ if json_data else "empty",
                "is_marker_genes": len(json_data) > 0 and any("gene" in str(item).lower() for item in json_data[:3])
            })
        
        # ç‰¹æ®Šæ£€æŸ¥ï¼šæ˜¯å¦æ˜¯markeråŸºå› æ–‡ä»¶
        marker_gene_indicators = [
            "cluster", "gene", "marker", "p_val", "avg_log", "pct"
        ]
        content_str = str(json_data).lower()
        marker_score = sum(1 for indicator in marker_gene_indicators if indicator in content_str)
        
        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæœ‰clusteré”®ï¼Œå¾ˆå¯èƒ½æ˜¯markeråŸºå› 
        cluster_pattern = any("cluster" in str(k).lower() for k in (json_data.keys() if isinstance(json_data, dict) else []))
        content_info["likely_marker_genes"] = marker_score >= 2 or cluster_pattern
        
        result = {
            "success": True,
            "valid": True,
            "file_path": str(file_path_obj),
            "file_name": file_path_obj.name,
            "json_validation": "passed",
            "content_analysis": content_info,
            "processing_recommendation": "ready_for_use" if content_info.get("likely_marker_genes", False) else "general_json"
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"JSONéªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}",
            "valid": False
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def validate_file_type(file_path: str) -> str:
    """è¯†åˆ«å’ŒéªŒè¯æ–‡ä»¶ç±»å‹
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        JSONæ ¼å¼çš„æ–‡ä»¶ç±»å‹ä¿¡æ¯
    """
    try:
        # ä½¿ç”¨å½“å‰ä¼šè¯çš„session_idä¿æŒä¸€è‡´æ€§
        current_timestamp = get_session_id_for_filename()
        print(f"ğŸ•’ ä½¿ç”¨å½“å‰ä¼šè¯session_id: {current_timestamp}")
        
        if not os.path.exists(file_path):
            return json.dumps({
                "success": False,
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                "file_type": "unknown",
                "timestamp": current_timestamp
            }, ensure_ascii=False, indent=2)
        
        file_path_obj = Path(file_path)
        file_size = file_path_obj.stat().st_size
        file_extension = file_path_obj.suffix.lower()
        
        # åŸºäºæ‰©å±•ååˆ¤æ–­æ–‡ä»¶ç±»å‹
        file_type_map = {
            '.rds': 'rds',
            '.h5': 'h5',
            '.h5ad': 'h5ad',
            '.csv': 'csv',
            '.json': 'json'
        }
        
        file_type = file_type_map.get(file_extension, 'unknown')
        
        result = {
            "success": True,
            "file_path": str(file_path_obj),
            "file_name": file_path_obj.name,
            "file_size": file_size,
            "file_extension": file_extension,
            "file_type": file_type,
            "valid": file_type != 'unknown',
            "timestamp": current_timestamp
        }
        
        # å¯¹JSONæ–‡ä»¶è¿›è¡Œé¢å¤–éªŒè¯å’Œæ·±åº¦åˆ†æ
        if file_type == 'json':
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                result["json_valid"] = True
                result["json_keys"] = list(json_data.keys()) if isinstance(json_data, dict) else None

                # æ·±åº¦åˆ†æï¼šæ£€æµ‹æ˜¯å¦ä¸º marker åŸºå› æ–‡ä»¶
                marker_gene_indicators = [
                    "cluster", "gene", "marker", "p_val", "avg_log", "pct"
                ]
                content_str = str(json_data).lower()
                marker_score = sum(1 for indicator in marker_gene_indicators if indicator in content_str)

                # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæœ‰ cluster é”®ï¼Œå¾ˆå¯èƒ½æ˜¯ marker åŸºå› 
                cluster_pattern = any("cluster" in str(k).lower() for k in (json_data.keys() if isinstance(json_data, dict) else []))
                result["is_marker_genes"] = marker_score >= 2 or cluster_pattern
                result["marker_confidence"] = "high" if marker_score >= 3 or cluster_pattern else "medium" if marker_score >= 2 else "low"

            except json.JSONDecodeError as e:
                result["json_valid"] = False
                result["json_error"] = str(e)
                result["is_marker_genes"] = False

        # å¯¹CSVæ–‡ä»¶è¿›è¡Œå†…å®¹åˆ†æ
        if file_type == 'csv':
            try:
                import csv
                with open(file_path, 'r', encoding='utf-8') as f:
                    # è¯»å–å‰å‡ è¡Œæ¥æ£€æµ‹åˆ—å
                    reader = csv.reader(f)
                    header = next(reader, None)

                    if header:
                        # è½¬æ¢ä¸ºå°å†™ä¾¿äºæ¯”è¾ƒ
                        header_lower = [col.lower() for col in header]

                        # FindAllMarkers å¸¸è§åˆ—å
                        marker_columns = ['cluster', 'gene', 'p_val', 'avg_log', 'pct.1', 'pct.2', 'p_val_adj']

                        # æ£€æµ‹åŒ¹é…çš„åˆ—æ•°
                        matches = sum(1 for col in marker_columns if any(col in h for h in header_lower))

                        result["csv_columns"] = header
                        result["csv_column_count"] = len(header)
                        result["is_marker_csv"] = matches >= 3  # è‡³å°‘åŒ¹é…3ä¸ªç‰¹å¾åˆ—
                        result["marker_column_matches"] = matches
                        result["marker_confidence"] = "high" if matches >= 5 else "medium" if matches >= 3 else "low"
                    else:
                        result["csv_columns"] = []
                        result["is_marker_csv"] = False
                        result["csv_error"] = "CSVæ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–å¤´éƒ¨"

            except Exception as e:
                result["csv_error"] = f"CSVåˆ†æå¤±è´¥: {str(e)}"
                result["is_marker_csv"] = False

        # è‡ªåŠ¨æ›´æ–° Bundle
        try:
            from agentype.mainagent.tools.file_paths_tools import auto_update_bundle

            # æ–‡ä»¶ç±»å‹åˆ° bundle å­—æ®µçš„æ˜ å°„
            file_type_to_bundle_key = {
                'rds': 'rds_file',
                'h5': 'h5_file',
                'h5ad': 'h5ad_file',
                'csv': 'marker_genes_csv',
                'json': 'marker_genes_json'
            }

            bundle_key = None
            should_update = False

            # æ ¹æ®æ–‡ä»¶ç±»å‹å’Œå†…å®¹åˆ†æå†³å®šæ˜¯å¦æ›´æ–°
            if file_type == 'json':
                # JSON éœ€è¦å†…å®¹åˆ†æï¼Œåªæœ‰ marker åŸºå›  JSON æ‰æ›´æ–°
                if result.get('is_marker_genes', False):
                    bundle_key = 'marker_genes_json'
                    should_update = True
            elif file_type == 'csv':
                # CSV éœ€è¦å†…å®¹åˆ†æï¼Œåªæœ‰ marker åŸºå›  CSV æ‰æ›´æ–°
                if result.get('is_marker_csv', False):
                    bundle_key = 'marker_genes_csv'
                    should_update = True
            elif file_type in ['rds', 'h5', 'h5ad']:
                # åŸå§‹æ•°æ®æ–‡ä»¶ç›´æ¥æ›´æ–°
                bundle_key = file_type_to_bundle_key[file_type]
                should_update = True

            # æ‰§è¡Œæ›´æ–°
            if should_update and bundle_key:
                auto_update_bundle(bundle_key, str(file_path_obj))
                result['bundle_updated'] = True
                result['bundle_key'] = bundle_key
                result['bundle_update_message'] = f'âœ… å·²è‡ªåŠ¨æ›´æ–°åˆ° bundle.{bundle_key}'
                print(f"âœ… å·²å°†æ–‡ä»¶è‡ªåŠ¨ä¿å­˜åˆ° bundle.{bundle_key}: {file_path_obj}")
            else:
                result['bundle_updated'] = False
                result['bundle_update_message'] = 'æœªæ›´æ–° bundleï¼ˆæ–‡ä»¶ç±»å‹ä¸åŒ¹é…æˆ–å†…å®¹åˆ†ææœªé€šè¿‡ï¼‰'

        except Exception as e:
            print(f"âš ï¸ æ›´æ–°bundleå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")
            result['bundle_updated'] = False
            result['bundle_error'] = str(e)

        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"æ–‡ä»¶ç±»å‹éªŒè¯å¤±è´¥: {str(e)}",
            "file_type": "unknown",
            "timestamp": get_session_id_for_filename()
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def get_token_stats() -> str:
    """è·å–DataAgentçš„tokenæ¶ˆè€—ç»Ÿè®¡ä¿¡æ¯

    Returns:
        str: JSONæ ¼å¼çš„tokenç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # ç”±äºMCPæœåŠ¡å™¨æ˜¯ç‹¬ç«‹è¿›ç¨‹ï¼Œè¿™é‡Œè¿”å›é»˜è®¤çš„ç©ºç»Ÿè®¡
        # å®é™…çš„tokenç»Ÿè®¡éœ€è¦é€šè¿‡agentå®ä¾‹è·å–
        from agentype.common.token_statistics import TokenStatistics

        # åˆ›å»ºä¸€ä¸ªç©ºçš„ç»Ÿè®¡å¯¹è±¡ä½œä¸ºå ä½ç¬¦
        # å®é™…åº”è¯¥é€šè¿‡æŸç§IPCæœºåˆ¶æˆ–å…±äº«å­˜å‚¨è·å–çœŸå®æ•°æ®
        stats = TokenStatistics(agent_name="DataAgent")

        return json.dumps({
            "success": True,
            "data": stats.to_dict(),
            "message": "DataAgent tokenç»Ÿè®¡ (MCPæœåŠ¡å™¨ç‹¬ç«‹è¿›ç¨‹)"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"è·å–tokenç»Ÿè®¡æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        }, ensure_ascii=False, indent=2)


# ========== è·¯å¾„ç®¡ç†å·¥å…· ==========

@mcp.tool()
async def save_file_paths_bundle(
    rds_file: Optional[str] = None,
    h5ad_file: Optional[str] = None,
    h5_file: Optional[str] = None,
    marker_genes_json: Optional[str] = None
) -> str:
    """ä¿å­˜æ•°æ®æ–‡ä»¶è·¯å¾„åˆ°cacheç›®å½•

    æ­¤å·¥å…·ç”¨äºåœ¨æ•°æ®å¤„ç†å®Œæˆåä¿å­˜æ‰€æœ‰å…³é”®æ–‡ä»¶è·¯å¾„ï¼Œä»¥ä¾¿åç»­æ­¥éª¤ä½¿ç”¨ã€‚

    Args:
        rds_file: RDSæ–‡ä»¶è·¯å¾„
        h5ad_file: H5ADæ–‡ä»¶è·¯å¾„
        h5_file: H5æ–‡ä»¶è·¯å¾„ï¼ˆeasySCFæ ¼å¼ï¼‰
        marker_genes_json: MarkeråŸºå› JSONæ–‡ä»¶è·¯å¾„

    Returns:
        JSONæ ¼å¼çš„ä¿å­˜ç»“æœï¼ŒåŒ…å«æˆåŠŸçŠ¶æ€ã€session_idã€ä¿å­˜è·¯å¾„ç­‰ä¿¡æ¯

    ä½¿ç”¨åœºæ™¯ï¼š
        - åœ¨å®Œæˆæ•°æ®å¤„ç†åç«‹å³è°ƒç”¨ï¼Œä¿å­˜ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
        - ç¡®ä¿è·¯å¾„æ ¼å¼æ­£ç¡®ï¼ˆç‰¹åˆ«æ³¨æ„ä¸­æ–‡è·¯å¾„çš„åˆ†éš”ç¬¦ï¼‰
        - å¦‚æœä¿å­˜å¤±è´¥ï¼Œæ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®æ­£è·¯å¾„åé‡è¯•
    """
    try:
        # marker_genes_json æ˜¯ DataAgent çš„å¿…éœ€è¾“å‡º
        if not marker_genes_json or marker_genes_json.strip() == "":
            return json.dumps({
                "success": False,
                "error": "âŒ marker_genes_json æ˜¯å¿…éœ€çš„ï¼DataAgent å¿…é¡»ç”Ÿæˆ marker åŸºå›  JSON æ–‡ä»¶ã€‚",
                "action_required": "è¯·ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€ç”Ÿæˆ JSON æ–‡ä»¶ï¼š",
                "available_methods": [
                    "1. run_r_findallmarkers - å¯¹ RDS/Seurat å¯¹è±¡è¿›è¡Œæ ‡è®°åŸºå› åˆ†æ",
                    "2. easyscfpy_h5_to_json - ä» easySCF H5 æ–‡ä»¶æå–",
                    "3. scanpy_path_to_json - ä» scanpy/AnnData æ–‡ä»¶è½¬æ¢",
                    "4. convert_r_markers_csv_to_json - ä» FindAllMarkers CSV ç»“æœè½¬æ¢"
                ]
            }, ensure_ascii=False, indent=2)

        # æ„å»º metadataï¼ˆè‡ªåŠ¨æ·»åŠ  DataAgent æ ‡è®°ï¼‰
        metadata_dict = {
            "agent": "DataAgent",
            "stage": "data_processing"
        }

        # è°ƒç”¨æ ¸å¿ƒå‡½æ•°ï¼ˆsession_id ç”±åº•å±‚å‡½æ•°è‡ªåŠ¨è·å–ï¼Œä½¿ç”¨é»˜è®¤è¿‡æœŸæ—¶é—´ï¼‰
        result = _save_file_paths_bundle(
            rds_file=rds_file,
            h5ad_file=h5ad_file,
            h5_file=h5_file,
            marker_genes_json=marker_genes_json,
            metadata=metadata_dict
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"ä¿å­˜æ–‡ä»¶è·¯å¾„åŒ…å¼‚å¸¸: {e}"
        }, ensure_ascii=False, indent=2)


@mcp.tool()
async def load_file_paths_bundle() -> str:
    """ä»cacheç›®å½•åŠ è½½å½“å‰ä¼šè¯çš„æ–‡ä»¶è·¯å¾„

    æ­¤å·¥å…·ç”¨äºåŠ è½½ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯ã€‚

    Returns:
        JSONæ ¼å¼çš„è·¯å¾„ä¿¡æ¯ï¼ŒåŒ…å« rds_file, h5ad_file, h5_file, marker_genes_json ç­‰

    ä½¿ç”¨åœºæ™¯ï¼š
        - å½“éœ€è¦è·å–ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶è·¯å¾„æ—¶
        - è·¨æ­¥éª¤ä¼ é€’æ–‡ä»¶è·¯å¾„ä¿¡æ¯
    """
    try:
        result = _load_and_validate_bundle()  # ä½¿ç”¨éªŒè¯ç‰ˆæœ¬æ›´å®‰å…¨
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "error": f"åŠ è½½æ–‡ä»¶è·¯å¾„åŒ…å¼‚å¸¸: {e}"
        }, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    """
    å¯åŠ¨ DataAgent MCP æœåŠ¡å™¨

    é…ç½®é€šè¿‡æ··åˆæ–¹æ¡ˆä¼ é€’ï¼š
    - æ•æ„Ÿä¿¡æ¯ï¼ˆAPI Keyï¼‰é€šè¿‡ç¯å¢ƒå˜é‡ OPENAI_API_KEY
    - éæ•æ„Ÿé…ç½®é€šè¿‡å‘½ä»¤è¡Œå‚æ•°
    """
    import argparse
    import os
    import sys
    from pathlib import Path
    from agentype.mainagent.config.session_config import set_session_id

    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='CellType DataAgent MCP Server')
    parser.add_argument('--api-base', type=str, help='LLM API Base URL (required)')
    parser.add_argument('--model', type=str, default='gpt-4o', help='LLM Model name')
    parser.add_argument('--output-dir', type=str, required=True, help='Output directory (required)')
    parser.add_argument('--language', type=str, default='zh', choices=['zh', 'en'], help='Language')
    parser.add_argument('--enable-streaming', type=str, default='true', help='Enable streaming output')
    parser.add_argument('--enable-thinking', type=str, default='false', help='Enable thinking output')
    parser.add_argument('--session-id', type=str, help='Session ID for tracking')
    args = parser.parse_args()

    # 2. ä»ç¯å¢ƒå˜é‡è¯»å– API Keyï¼ˆå®‰å…¨ï¼‰
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡", file=sys.stderr)
        print("   DataAgent MCP Server éœ€è¦ API Key æ‰èƒ½è¿è¡Œ", file=sys.stderr)
        sys.exit(1)

    # 3. éªŒè¯å¿…éœ€çš„å‘½ä»¤è¡Œå‚æ•°
    if not args.api_base:
        print("âŒ é”™è¯¯: ç¼ºå°‘å¿…éœ€å‚æ•° --api-base", file=sys.stderr)
        sys.exit(1)

    # 4. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir).resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    # 4.5. åˆ›å»ºConfigManagerå¹¶è®¾ç½®ä¸ºæ¨¡å—çº§é…ç½®å¯¹è±¡
    from agentype.dataagent.config.settings import ConfigManager

    enable_thinking = args.enable_thinking.lower() in ('true', '1', 'yes')

    _CONFIG = ConfigManager(
        openai_api_base=args.api_base,
        openai_api_key=api_key,
        openai_model=args.model,
        output_dir=str(output_dir),  # ä¼ é€’output_dirï¼Œè®©ConfigManagerè‡ªåŠ¨æ´¾ç”Ÿcache_dir
        enable_thinking=enable_thinking
    )

    # æ›´æ–°å…¨å±€ CACHE_DIRï¼ˆä» _CONFIG è·å–ï¼‰
    CACHE_DIR = _CONFIG.cache_dir

    print(f"âœ… DataAgent ConfigManager å·²åˆå§‹åŒ–:", file=sys.stderr)
    print(f"   Output Dir: {_CONFIG.output_dir}", file=sys.stderr)
    print(f"   Results Dir: {_CONFIG.results_dir}", file=sys.stderr)
    print(f"   Cache Dir: {_CONFIG.cache_dir}", file=sys.stderr)

    # åˆå§‹åŒ–ç¼“å­˜ï¼ˆä½¿ç”¨ ConfigManagerï¼‰
    from agentype.dataagent.config.cache_config import init_cache
    cache_dir = init_cache(config=_CONFIG)

    # è®¾ç½®å·¥å…·æ¨¡å—çš„å…¨å±€é…ç½®ï¼ˆç”¨äº file_paths_toolsï¼‰
    from agentype.mainagent.tools.file_paths_tools import set_global_config
    set_global_config(_CONFIG)

    # 5. è®¾ç½® session_id
    if args.session_id:
        set_session_id(args.session_id)
        print(f"âœ… DataAgent MCP Server ä½¿ç”¨ session_id: {args.session_id}", file=sys.stderr)

    # 6. æ‰“å°é…ç½®ä¿¡æ¯
    print(f"âœ… DataAgent MCP Server é…ç½®:", file=sys.stderr)
    print(f"   API Base: {args.api_base}", file=sys.stderr)
    print(f"   Model: {args.model}", file=sys.stderr)

    # å¯åŠ¨MCPæœåŠ¡å™¨ (æ ‡å‡†stdioä¼ è¾“)
    print("ğŸš€ å¯åŠ¨CellType DataProcessor MCPæœåŠ¡å™¨...")
    print("ğŸ“‹ åè®®: Model Context Protocol (MCP)")
    print("ğŸ”§ å¯ç”¨å·¥å…·: 12ä¸ª")
    print("ğŸ“Š æ ¸å¿ƒå·¥å…·:")
    print("  1ï¸âƒ£  run_r_findallmarkers - R FindAllMarkersåˆ†æ")
    print("  2ï¸âƒ£  run_r_sce_to_h5 - SCEè½¬H5æ ¼å¼")
    print("  3ï¸âƒ£  convert_r_markers_csv_to_json - CSVè½¬JSON")
    print("  4ï¸âƒ£  easyscfpy_h5_to_json - easySCF H5è½¬JSON")
    print("  5ï¸âƒ£  scanpy_to_json - scanpyå¯¹è±¡è½¬JSON")
    print("  6ï¸âƒ£  scanpy_path_to_json - scanpyæ–‡ä»¶è½¬JSON")
    print("  7ï¸âƒ£  convert_scanpy_file_to_h5 - scanpyæ–‡ä»¶è½¬H5æ ¼å¼")
    print("  8ï¸âƒ£  validate_file_type - æ–‡ä»¶ç±»å‹éªŒè¯")
    print("  9ï¸âƒ£  validate_json_only - JSONæ–‡ä»¶ä¸“é—¨éªŒè¯")
    print("  ğŸ”Ÿ  get_token_stats - è·å–tokenç»Ÿè®¡ä¿¡æ¯")
    print("  1ï¸âƒ£1ï¸âƒ£ save_file_paths_bundle - ä¿å­˜æ–‡ä»¶è·¯å¾„åˆ°cache")
    print("  1ï¸âƒ£2ï¸âƒ£ load_file_paths_bundle - ä»cacheåŠ è½½æ–‡ä»¶è·¯å¾„")
    print("=" * 60)
    mcp.run(transport='stdio')