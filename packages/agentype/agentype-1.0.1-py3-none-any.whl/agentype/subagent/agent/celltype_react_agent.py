#!/usr/bin/env python3
"""
agentype - Celltype React Agentæ¨¡å—
Author: cuilei
Version: 1.0
"""

import asyncio
import gc
import json
import os
import re
import requests
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List

# Tokenç»Ÿè®¡æ¨¡å—
from agentype.common.token_statistics import TokenReporter

# å¯¼å…¥ prompts
from agentype.subagent.config.prompts import (
    SUPPORTED_LANGUAGES,
    DEFAULT_LANGUAGE,
    get_system_prompt_template,
    get_fallback_prompt_template,
    get_user_query_templates,
)

# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from agentype.subagent.config.settings import ConfigManager
from agentype.subagent.clients.mcp_client import MCPClient
from agentype.subagent.utils.content_processor import ContentProcessor
from agentype.subagent.utils.parser import ReactParser
from agentype.subagent.utils.validator import ValidationUtils
from agentype.subagent.utils.i18n import _
from agentype.subagent.utils.path_manager import path_manager
from agentype.subagent.utils.output_logger import OutputLogger
# å¯¼å…¥å…±äº«æ¨¡å—
from agentype.common.llm_client import LLMClient
from agentype.common.llm_logger import LLMLogger
from agentype.common.streaming_filter import StreamingFilter

class CellTypeReactAgent:
    """ç»†èƒç±»å‹åˆ†æ React Agent
    
    è¯¥ç±»æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒï¼Œè´Ÿè´£åè°ƒ MCP å®¢æˆ·ç«¯ã€å†…å®¹å¤„ç†å™¨ã€
    éªŒè¯å·¥å…·ç­‰ç»„ä»¶ï¼Œæ‰§è¡Œå®Œæ•´çš„ç»†èƒç±»å‹åˆ†ææµç¨‹ã€‚
    """
    
    def __init__(self,
                 config: ConfigManager,
                 server_script: str = None,
                 max_content_length: int = 10000,
                 enable_summarization: bool = True,
                 max_iterations: int = 50,
                 context_summary_threshold: int = 15,
                 max_context_length: int = 150000,
                 enable_llm_logging: bool = True,
                 log_dir: str = None,
                 language: str = "zh",
                 enable_streaming: bool = True,
                 api_timeout: int = 300,
                 enable_console_output: bool = True,
                 console_output: bool = None,
                 file_output: bool = None,
                 session_id: str = None):

        # ğŸŒŸ è®¾ç½® session_idï¼ˆå¦‚æœæä¾›ï¼‰
        # æ³¨æ„ï¼šåœ¨MCPæ¨¡å¼ä¸‹ï¼Œsession_idå·²åœ¨MCP Serverå¯åŠ¨æ—¶è®¾ç½®ï¼Œæ­¤å¤„ä¸å†é‡å¤æ‰“å°
        if session_id:
            from agentype.mainagent.config.session_config import set_session_id
            set_session_id(session_id)

        # ç»„ä»¶åˆå§‹åŒ–
        self.config = config
        # å¦‚æœæ²¡æœ‰æŒ‡å®šserver_scriptï¼Œä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–é»˜è®¤è·¯å¾„
        if server_script is None:
            server_script = str(path_manager.get_mcp_server_path())
        self.mcp_client = MCPClient(server_script, config=self.config)
        self.content_processor = ContentProcessor(max_content_length, enable_summarization)
        self.parser = ReactParser()
        self.validator = ValidationUtils()
        
        # è¯­è¨€è®¾ç½®
        self.language = language if language in SUPPORTED_LANGUAGES else DEFAULT_LANGUAGE
        
        # LLM æ—¥å¿—è®°å½•å™¨
        self.enable_llm_logging = enable_llm_logging
        if enable_llm_logging:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šlog_dirï¼Œä½¿ç”¨ ConfigManager çš„æ—¥å¿—ç›®å½•
            if log_dir is None:
                from pathlib import Path
                llm_log_dir = str(Path(self.config.log_dir) / "llm" / "SubAgent")
            else:
                llm_log_dir = str(path_manager.get_llm_log_dir(log_dir))
            self.llm_logger = LLMLogger(llm_log_dir)
        else:
            self.llm_logger = None
        
        # è¿è¡Œæ—¶çŠ¶æ€
        self.available_tools = []
        self.max_iterations = max_iterations
        self.context_summary_threshold = context_summary_threshold
        self.max_context_length = max_context_length
        
        # APIé…ç½®
        self.enable_streaming = enable_streaming
        self.api_timeout = api_timeout

        # åˆå§‹åŒ–æ§åˆ¶å°è¾“å‡ºæ—¥å¿—å™¨ - æ”¯æŒç²¾ç»†åŒ–æ§åˆ¶
        # å‚æ•°ä¼˜å…ˆçº§ï¼šconsole_output/file_output > enable_console_output
        if console_output is not None or file_output is not None:
            # ä½¿ç”¨ç²¾ç»†åŒ–æ§åˆ¶å‚æ•°
            self.console_output = console_output if console_output is not None else True
            self.file_output = file_output if file_output is not None else True
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨enable_console_output
            self.console_output = enable_console_output
            self.file_output = enable_console_output

        # åªæœ‰åœ¨è‡³å°‘ä¸€ä¸ªè¾“å‡ºæ–¹å¼å¯ç”¨æ—¶æ‰åˆ›å»ºlogger
        if self.console_output or self.file_output:
            self.console_logger = OutputLogger(
                log_prefix="celltypeSubagent",
                console_output=self.console_output,
                file_output=self.file_output,
                log_dir=str(self.config.log_dir)
            )
        else:
            self.console_logger = None

        # åˆå§‹åŒ–tokenæŠ¥å‘Šå™¨
        self.token_reporter = TokenReporter(language=self.language)

        # ğŸŒŸ åˆå§‹åŒ–å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰
        self.llm_client = LLMClient(
            config=config,
            logger_callbacks={
                'info': self._log_info,
                'success': self._log_success,
                'warning': self._log_warning,
                'error': self._log_error
            }
        )
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– agent - å¯åŠ¨ MCP æœåŠ¡å™¨å¹¶è·å–å·¥å…·åˆ—è¡¨"""
        # å¯åŠ¨ MCP æœåŠ¡å™¨
        if not await self.mcp_client.start_server():
            self._log_error("âŒ MCP æœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
            return False

        # è·å–å¯ç”¨å·¥å…·
        self.available_tools = await self.mcp_client.list_tools()

        # ğŸŒŸ æ–°å¢ï¼šéªŒè¯å·¥å…·åˆ—è¡¨æ˜¯å¦ä¸ºç©º
        if not self.available_tools:
            self._log_error("âŒ è­¦å‘Šï¼šå¯ç”¨å·¥å…·åˆ—è¡¨ä¸ºç©ºï¼")
            self._log_error("   MCP æœåŠ¡å™¨å¯èƒ½æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œæˆ–è€…æ²¡æœ‰æ³¨å†Œä»»ä½•å·¥å…·")
            return False

        # ğŸŒŸ æ–°å¢ï¼šæ˜¾ç¤ºå·²åŠ è½½çš„å·¥å…·ä¿¡æ¯
        self._log_success(f"âœ… å·²åŠ è½½ {len(self.available_tools)} ä¸ªå·¥å…·")
        tool_names = [t.get('name', 'unknown') for t in self.available_tools]
        self._log_info(f"ğŸ“‹ å·¥å…·åˆ—è¡¨: {', '.join(tool_names)}")

        return True
    
    def set_language(self, language: str):
        """è®¾ç½®åˆ†æè¯­è¨€"""
        if language in SUPPORTED_LANGUAGES:
            self.language = language
            self._log_info(_("agent.language_set", language=language))
        else:
            self._log_warning(_("agent.language_not_supported", language=language, supported=SUPPORTED_LANGUAGES))

    def _get_system_prompt(self) -> str:
        """è·å– React ç³»ç»Ÿ prompt"""
        try:
            work_dir = os.getenv("CELLTYPE_WORK_DIR", str(Path.cwd()))

            print(f"ğŸ” è°ƒè¯•è¾“å‡º: SubAgenté…ç½®ä¿¡æ¯:")
            print(f"   - å·¥ä½œç›®å½•: {work_dir}")

            # ä½¿ç”¨æŒ‡å®šè¯­è¨€çš„æ¨¡æ¿æ ¼å¼åŒ–
            template = get_system_prompt_template(self.language)
            tool_list = "\n".join([f"{i+1}. {tool.get('name', 'Unknown')}: {tool.get('description', '')}"
                                  for i, tool in enumerate(self.available_tools)])

            # æ ¹æ®è¯­è¨€è®¾ç½®ç¼“å­˜çŠ¶æ€
            cache_status = "âœ“ MCP æœåŠ¡å™¨å·²å¯åŠ¨" if self.language == 'zh' else "âœ“ MCP Server Started"

            # è·å–å½“å‰è¿è¡Œç›®å½•çš„æ–‡ä»¶åˆ—è¡¨
            try:
                files = [f for f in os.listdir('.') if os.path.isfile(f)]
                file_list = ', '.join(files)
            except Exception:
                file_list = "celltype_mcp_server.py, prompts.py, celltype_react_agent_mcp.py"

            return template.format(
                tool_list=tool_list,
                operating_system="Linux",
                cache_status=cache_status,
                file_list=file_list
            )
        except Exception as e:
            # fallback åˆ°ç®€åŒ–ç‰ˆæœ¬
            self._log_warning(_("agent.template_fallback", error=e))
            fallback_template = get_fallback_prompt_template(self.language)
            tool_names = ', '.join([tool.get('name', '') for tool in self.available_tools])
            return fallback_template.format(tool_names=tool_names)
    
    async def _call_openai(self, messages: List[Dict], timeout: int = 270, stream: bool = False, request_type: str = "main") -> str:
        """è°ƒç”¨ OpenAI API - ä½¿ç”¨å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰

        åŒ…å«å¯¹ <observation> å¹»è§‰çš„æ£€æµ‹å’Œè‡ªåŠ¨é‡è¯•æœºåˆ¶
        """
        from agentype.common.llm_client import ObservationHallucinationError

        max_hallucination_retries = 3

        for retry in range(max_hallucination_retries):
            try:
                response = await self.llm_client.call_api(
                    messages=messages,
                    timeout=timeout,
                    stream=stream,
                    request_type=request_type,
                    llm_logger=self.llm_logger,
                    console_logger=self.console_logger
                )
                # æˆåŠŸè·å–å“åº”ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                return response

            except ObservationHallucinationError as e:
                self._log_error(f"âŒ LLM äº§ç”Ÿå¹»è§‰ï¼ˆç”Ÿæˆäº† <observation> æ ‡ç­¾ï¼‰- é‡è¯• {retry + 1}/{max_hallucination_retries}")

                if retry < max_hallucination_retries - 1:
                    # è¿˜æœ‰é‡è¯•æœºä¼šï¼Œæ·»åŠ çº æ­£æç¤º
                    from agentype.prompts import get_prompt_manager

                    manager = get_prompt_manager(self.language)
                    correction_content = manager.get_common_prompt('HALLUCINATION_CORRECTION_MESSAGE')

                    correction_message = {
                        "role": "user",
                        "content": correction_content
                    }
                    messages.append(correction_message)
                    self._log_warning("ğŸ”„ æ·»åŠ çº æ­£æç¤ºï¼Œå‡†å¤‡é‡è¯•...")
                    continue
                else:
                    # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
                    self._log_error(f"âŒ LLM æŒç»­äº§ç”Ÿå¹»è§‰ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_hallucination_retries})")
                    raise

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œ
        raise RuntimeError("æœªé¢„æœŸçš„ä»£ç è·¯å¾„")

    async def _retry_llm_call_with_correction(self, messages: List[Dict], validation_result: Dict, max_retries: int = 10, timeout: int = 270) -> str:
        """é‡æ–°è°ƒç”¨ LLM å¹¶æä¾›ä¿®æ­£æŒ‡å¯¼"""
        retry_count = 0
        response = ""

        while retry_count < max_retries:
            retry_count += 1
            self._log_info(_("retry.llm_retry", retry=retry_count, max_retries=max_retries))
            self._log_warning(_("retry.last_response_issues", issues=', '.join(validation_result['issues'])))

            # æ„å»ºä¿®æ­£æç¤º
            correction_prompt = ValidationUtils.build_correction_prompt(validation_result, self.available_tools, self.language)

            # æ·»åŠ ä¿®æ­£æç¤ºåˆ°æ¶ˆæ¯å†å²
            correction_messages = messages.copy()
            correction_messages.append({"role": "user", "content": correction_prompt})

            # é‡æ–°è°ƒç”¨ LLMï¼Œä¿æŒä¸ä¸»è°ƒç”¨ç›¸åŒçš„æµå¼è¾“å‡ºè®¾ç½®
            response = await self._call_openai(correction_messages, timeout, stream=self.enable_streaming, request_type="retry")

            # ä¸ºé‡è¯•è°ƒç”¨æ·»åŠ é¢å¤–çš„æ—¥å¿—æ ‡è®°
            if self.llm_logger:
                self._log_info(_("retry.retry_logged", count=retry_count))

            # ğŸŒŸ ä¿®æ”¹ï¼šéªŒè¯æ—¶ä¼ é€’ has_reasoning å‚æ•°
            new_validation = ValidationUtils.validate_response_format(
                response,
                has_reasoning=self.llm_client.has_reasoning()
            )

            if new_validation['valid']:
                self._log_success(_("retry.retry_success"))
                return response
            else:
                self._log_error(_("retry.retry_failed", count=retry_count, issues=', '.join(new_validation['issues'])))
                validation_result = new_validation

        self._log_error(_("retry.max_retries_reached"))
        return response

    async def _summarize_context(self, messages: List[Dict]) -> List[Dict]:
        """å½“å¯¹è¯å†å²è¿‡é•¿æ—¶ï¼Œæ€»ç»“ä¸Šä¸‹æ–‡ä»¥å‡å°‘tokenæ¶ˆè€—

        Args:
            messages: å½“å‰çš„å¯¹è¯å†å²

        Returns:
            æ€»ç»“åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        try:
            self._log_warning(_("context.too_long"))

            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€åˆçš„ç”¨æˆ·æŸ¥è¯¢
            system_message = messages[0] if messages and messages[0]["role"] == "system" else None
            initial_query = messages[1] if len(messages) > 1 and messages[1]["role"] == "user" else None

            # æŸ¥æ‰¾ä¹‹å‰çš„æ€»ç»“æ¶ˆæ¯
            existing_summary = None
            summary_index = -1
            for i, msg in enumerate(messages):
                if msg.get("role") == "assistant" and "[ä¸Šä¸‹æ–‡æ€»ç»“]" in msg.get("content", ""):
                    existing_summary = msg
                    summary_index = i
                    break

            # ç¡®å®šéœ€è¦æ€»ç»“çš„å¯¹è¯å†å²èŒƒå›´
            # éœ€è¦ä¿ç•™çš„é‡è¦å·¥å…·ç»“æœï¼ˆå¦‚get_gene_infoï¼‰
            important_messages = []

            if existing_summary and summary_index > 1:
                # å¦‚æœå·²æœ‰æ€»ç»“ï¼Œä»æ€»ç»“åå¼€å§‹åˆ°å€’æ•°å‡ è½®
                raw_conversation = messages[summary_index+1:-4] if len(messages) > summary_index + 5 else messages[summary_index+1:]
                recent_messages = messages[-4:] if len(messages) > summary_index + 5 else []
            else:
                # å¦‚æœæ²¡æœ‰æ€»ç»“ï¼Œä»ç¬¬3æ¡æ¶ˆæ¯å¼€å§‹ï¼ˆè·³è¿‡ç³»ç»Ÿæ¶ˆæ¯å’Œåˆå§‹æŸ¥è¯¢ï¼‰
                raw_conversation = messages[2:-4] if len(messages) > 6 else messages[2:]
                recent_messages = messages[-4:] if len(messages) > 6 else []

            # ä»raw_conversationä¸­æå–é‡è¦çš„get_gene_infoç»“æœ
            conversation_to_summarize = []
            for i, msg in enumerate(raw_conversation):
                # æ£€æŸ¥æ˜¯å¦æ˜¯get_gene_infoçš„observation
                is_gene_info_result = False
                if msg.get("role") == "user" and "<observation>" in msg.get("content", ""):
                    # æŸ¥æ‰¾å‰ä¸€æ¡æ¶ˆæ¯ï¼Œçœ‹æ˜¯å¦æ˜¯get_gene_infoè°ƒç”¨
                    if i > 0:
                        prev_msg = raw_conversation[i-1]
                        if (prev_msg.get("role") == "assistant" and
                            "get_gene_info" in prev_msg.get("content", "")):
                            is_gene_info_result = True

                if is_gene_info_result:
                    important_messages.append(msg)  # ä¿ç•™ï¼Œä¸æ€»ç»“
                else:
                    conversation_to_summarize.append(msg)  # å¯ä»¥æ€»ç»“

            if not conversation_to_summarize:
                self._log_info(_("context.no_history"))
                return messages

            # æ„å»ºæ€»ç»“prompt
            conversation_text = ""
            for msg in conversation_to_summarize:
                role = msg["role"]
                content = msg["content"]
                conversation_text += f"\n{role.upper()}: {content}\n"

            # æ ¹æ®æ˜¯å¦å­˜åœ¨ä¹‹å‰çš„æ€»ç»“æ¥æ„å»ºä¸åŒçš„prompt
            from agentype.prompts import get_prompt_manager
            manager = get_prompt_manager(self.language)

            if existing_summary:
                existing_summary_content = existing_summary.get("content", "").replace("[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„åˆ†æè¿›å±•ï¼š", "").strip()
                template = manager.get_agent_specific_prompt('subagent', 'CONTEXT_SUMMARY_INCREMENTAL_TEMPLATE')
                summarization_prompt = template.format(
                    existing_summary=existing_summary_content,
                    conversation=conversation_text
                )
            else:
                template = manager.get_agent_specific_prompt('subagent', 'CONTEXT_SUMMARY_INITIAL_TEMPLATE')
                summarization_prompt = template.format(
                    conversation=conversation_text
                )

            # è°ƒç”¨OpenAIè¿›è¡Œæ€»ç»“ï¼Œä¿æŒä¸ä¸»è°ƒç”¨ç›¸åŒçš„æµå¼è¾“å‡ºè®¾ç½®
            summary_messages = [{"role": "user", "content": summarization_prompt}]
            summary = await self._call_openai(summary_messages, timeout=self.api_timeout, stream=self.enable_streaming, request_type="summary")  # ä¸Šä¸‹æ–‡æ€»ç»“ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´

            # è®°å½•æ€»ç»“è¯·æ±‚æ—¥å¿—
            if self.llm_logger:
                self._log_info(_("context.summary_logged"))

            # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
            new_messages = []

            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            if system_message:
                new_messages.append(system_message)

            # ä¿ç•™åˆå§‹æŸ¥è¯¢
            if initial_query:
                new_messages.append(initial_query)

            # æ·»åŠ æ›´æ–°çš„æ€»ç»“æ¶ˆæ¯ï¼ˆæ›¿æ¢ä¹‹å‰çš„æ€»ç»“ï¼‰
            summary_message = {
                "role": "assistant",
                "content": f"[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„åˆ†æè¿›å±•ï¼š{summary}"
            }
            new_messages.append(summary_message)

            # æ·»åŠ ä¿ç•™çš„é‡è¦æ¶ˆæ¯ï¼ˆå¦‚get_gene_infoç»“æœï¼‰
            if important_messages:
                new_messages.extend(important_messages)
                self._log_info(f"ğŸ’ ä¿ç•™äº† {len(important_messages)} æ¡é‡è¦å·¥å…·ç»“æœï¼ˆå¦‚get_gene_infoï¼‰")

            # æ·»åŠ æœ€è¿‘çš„æ¶ˆæ¯ä»¥ä¿æŒè¿ç»­æ€§
            new_messages.extend(recent_messages)

            self._log_success(_("context.summary_completed"))
            self._log_info(_("context.original_messages", count=len(messages)))
            self._log_info(_("context.summarized_messages", count=len(new_messages)))
            self._log_info(_("context.summary_length", length=len(summary)))

            return new_messages

        except Exception as e:
            error_msg = f"ä¸Šä¸‹æ–‡æ€»ç»“å¤±è´¥: {e}"
            self._log_error(f"âŒ {error_msg}")

            # æ€»ç»“å¤±è´¥æ—¶ï¼Œç®€å•åœ°ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            if len(messages) > 10:
                self._log_info(_("context.simple_strategy"))
                return messages[-10:]

            return messages

    async def analyze_celltype(self, gene_list: str, tissue_type: str = None, cell_type: str = None, species: str = None) -> Dict:
        """åˆ†æç»†èƒç±»å‹

        Args:
            gene_list: åŸºå› åˆ—è¡¨ï¼Œé€—å·åˆ†éš”
            tissue_type: ç»„ç»‡ç±»å‹ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚"éª¨é«“"ã€"è¡€æ¶²"ç­‰
            cell_type: ç»†èƒç±»å‹æç¤ºï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºä¼˜å…ˆåˆ¤æ–­ç»†èƒäºšç¾¤
            species: ç‰©ç§ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚"Human"ã€"Mouse"ç­‰

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # ğŸŒŸ ç‰©ç§å¤„ç†é€»è¾‘
        detected_species = species  # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„

        if not detected_species:
            # è‡ªåŠ¨æ£€æµ‹
            self.console_logger.info("ğŸ” æœªæä¾›ç‰©ç§å‚æ•°ï¼ŒSubAgentè‡ªåŠ¨æ£€æµ‹...")
            from agentype.subagent.utils.common import SpeciesDetector

            # ä»åŸºå› åˆ—è¡¨æå–åŸºå› 
            genes = [g.strip() for g in gene_list.split(',') if g.strip()]

            detector = SpeciesDetector()
            detected_species = detector.detect_species_simple(genes)
            self.console_logger.info(f"âœ… æ£€æµ‹åˆ°ç‰©ç§: {detected_species}")
        else:
            self.console_logger.info(f"âœ… ä½¿ç”¨ä¼ å…¥çš„ç‰©ç§å‚æ•°: {detected_species}")
        self._log_header(_("analysis.starting", genes=gene_list))
        if tissue_type:
            self._log_info(_("analysis.tissue_type", tissue=tissue_type))
        if cell_type:
            self._log_info(f"ğŸ§« ç»†èƒç±»å‹æç¤º: {cell_type}")
        
        # å‡†å¤‡ç³»ç»Ÿ prompt å’Œç”¨æˆ·æŸ¥è¯¢
        system_prompt = self._get_system_prompt()
        
        # æ ¹æ®æ˜¯å¦æœ‰ç»„ç»‡ç±»å‹ç”Ÿæˆä¸åŒçš„æŸ¥è¯¢
        query_templates = get_user_query_templates(self.language)
        template_args = {"gene_list": gene_list}
        template_key = None

        if cell_type:
            template_args["cell_type"] = cell_type
            if tissue_type and "with_tissue_and_celltype" in query_templates:
                template_key = "with_tissue_and_celltype"
                template_args["tissue_type"] = tissue_type
            elif "with_celltype" in query_templates:
                template_key = "with_celltype"
            elif tissue_type and "with_tissue" in query_templates:
                template_key = "with_tissue"
                template_args["tissue_type"] = tissue_type
        elif tissue_type and "with_tissue" in query_templates:
            template_key = "with_tissue"
            template_args["tissue_type"] = tissue_type

        if not template_key:
            template_key = "without_tissue"

        template = query_templates.get(template_key) or query_templates["without_tissue"]
        user_query = template.format(**template_args)
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"<question>{user_query}</question>"}
        ]
        
        self._log_info(_("analysis.system_prompt_length", length=len(system_prompt)))
        self._log_info(_("analysis.user_query", query=user_query) + "\n")
        
        analysis_log = []
        iteration = 0
        final_celltype = None
        final_answer_text = None
        
        while iteration < self.max_iterations:
            iteration += 1
            self._log_header(_("analysis.iteration", iteration=iteration))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œä¸Šä¸‹æ–‡æ€»ç»“
            should_summarize = False
            summarize_reason = ""
            
            # æ£€æŸ¥è¿­ä»£æ¬¡æ•°é˜ˆå€¼
            if iteration == self.context_summary_threshold:
                should_summarize = True
                summarize_reason = f"è¾¾åˆ°è¿­ä»£æ¬¡æ•°é˜ˆå€¼ï¼ˆç¬¬{iteration}æ¬¡è¿­ä»£ï¼‰"
                
            # if len(messages) > self.context_summary_threshold:
            #     should_summarize = True
            #     summarize_reason = f"æ¶ˆæ¯æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼ˆ{len(messages)}ï¼‰"
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä¼°ç®—tokenæ•°é‡ï¼‰
            total_content_length = sum(len(msg.get("content", "")) for msg in messages)
            self._log_info(_("analysis.current_messages") + f" {len(messages)}")
            self._log_info(_("analysis.current_context_length") + f" {total_content_length}")
            if total_content_length > self.max_context_length:
                should_summarize = True
                if summarize_reason:
                    summarize_reason += f" ä¸”ä¸Šä¸‹æ–‡è¿‡é•¿ï¼ˆ{total_content_length}å­—ç¬¦ï¼Œé˜ˆå€¼{self.max_context_length}ï¼‰"
                else:
                    summarize_reason = f"ä¸Šä¸‹æ–‡è¿‡é•¿ï¼ˆ{total_content_length}å­—ç¬¦ï¼Œé˜ˆå€¼{self.max_context_length}ï¼‰"
            
            # æ‰§è¡Œæ€»ç»“
            if should_summarize and len(messages) > 6:
                self._log_warning(_("analysis.context_compression", reason=summarize_reason))
                messages = await self._summarize_context(messages)
            
            # ä½¿ç”¨å…¨å±€é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            if self.enable_streaming:
                self._log_info(_("analysis.streaming_enabled", iteration=iteration))
            
            # è°ƒç”¨ OpenAIï¼ˆä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼‰
            response = await self._call_openai(messages, timeout=self.api_timeout, stream=self.enable_streaming)
            self._log_info(_("analysis.ai_response_length", length=len(response)))

            # ğŸŒŸ æ–°å¢ï¼šéªŒè¯å“åº”æ ¼å¼ï¼Œä¼ é€’ has_reasoning å‚æ•°
            # DeepSeek Reasoner æ¨¡å‹æœ‰ reasoning_content æ—¶ï¼Œå…è®¸æ²¡æœ‰ <thought> æ ‡ç­¾
            validation = ValidationUtils.validate_response_format(
                response,
                has_reasoning=self.llm_client.has_reasoning()
            )
            
            if not validation['valid']:
                self._log_warning(_("analysis.format_validation_failed", issues=', '.join(validation['issues'])))
                response = await self._retry_llm_call_with_correction(messages, validation, 50, self.api_timeout)
            else:
                self._log_success(_("analysis.format_validation_passed"))
            
            analysis_log.append({
                "iteration": iteration,
                "response": response,
                "validation": validation,
                "type": "ai_response"
            })
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å« final_answer
            if '</final_answer>' in response:
                # æå–å¹¶ä¿å­˜ final_answer å†…å®¹
                try:
                    match = re.search(r'<final_answer>(.*?)</final_answer>', response, re.DOTALL)
                    if match:
                        final_answer_text = match.group(1).strip()
                except Exception:
                    pass

                final_celltype = ReactParser.extract_celltype(response)
                if final_celltype:
                    self._log_success(_("analysis.analysis_completed", celltype=final_celltype))
                    break
                else:
                    self._log_warning(_("analysis.final_answer_no_celltype"))
            
            # ğŸŒŸ æ–°å¢ï¼šæå–å¹¶æ‰§è¡Œ actionï¼Œæ”¯æŒè¯¦ç»†é”™è¯¯å¤„ç†
            action = ReactParser.extract_action(response, self.available_tools)

            # æ£€æŸ¥ action æå–ç»“æœ
            if action and 'error' in action:
                # Action æå–å¤±è´¥ - è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                self._log_warning(f"âŒ Action æå–å¤±è´¥: {action.get('message', 'æœªçŸ¥é”™è¯¯')}")

                # æ ¹æ®é”™è¯¯ç±»å‹è®°å½•è¯¦ç»†ä¿¡æ¯
                if action['error'] == 'invalid_action_format':
                    self._log_error(f"   åŸå› ï¼šaction æ ¼å¼ä¸æ­£ç¡®")
                    self._log_error(f"   å†…å®¹: {action.get('action_text', '')[:100]}...")
                elif action['error'] == 'invalid_tool_name':
                    self._log_error(f"   åŸå› ï¼šæ— æ•ˆçš„å·¥å…·åç§°")
                    self._log_error(f"   è¯·æ±‚å·¥å…·: {action.get('func_name', 'unknown')}")
                    self._log_error(f"   å¯ç”¨å·¥å…·: {', '.join(action.get('available_tools', []))}")

                # æ£€æŸ¥æ˜¯å¦æœ‰ final_answerï¼Œå¦‚æœæœ‰åˆ™æ­£å¸¸ç»“æŸ
                if '</final_answer>' in response:
                    self._log_info("   å“åº”ä¸­åŒ…å« final_answerï¼Œå¿½ç•¥ action é”™è¯¯")
                    # ä¸æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œä½†ç»§ç»­å¾ªç¯ä»¥å¤„ç† final_answer
                else:
                    self._log_error("   å“åº”ä¸­æ—¢æ— æœ‰æ•ˆ action ä¹Ÿæ—  final_answerï¼Œå¼‚å¸¸é€€å‡º")
                    break

            elif action:
                # Action æå–æˆåŠŸ - æ‰§è¡Œå·¥å…·è°ƒç”¨
                function_name = action['function']
                parameters_str = action['parameters']
                parameters = ReactParser.parse_parameters(parameters_str)

                self._log_info(_("tools.call.calling", name=function_name, params=parameters))

                # æ‰§è¡Œ MCP å·¥å…·è°ƒç”¨
                tool_result = await self.mcp_client.call_tool(function_name, parameters)

                if tool_result and tool_result.get("success"):
                    raw_content = tool_result.get("content", "")
                    self._log_success(_("tools.call.success", length=len(raw_content)))

                    # å¤„ç†å†…å®¹é•¿åº¦
                    result_content = await self.content_processor.process_tool_result_content(
                        raw_content, self._call_openai, self.language,
                        tool_name=function_name, tool_params=parameters, mcp_client=self.mcp_client
                    )
                else:
                    result_content = json.dumps({
                        "error": tool_result.get("error", "å·¥å…·è°ƒç”¨å¤±è´¥") if tool_result else "æ— å“åº”",
                        "success": False
                    }, ensure_ascii=False)
                    self._log_error(_("tools.call.failed"))

                # è®°å½•åˆ†ææ—¥å¿—
                analysis_log.append({
                    "iteration": iteration,
                    "action": action,
                    "result": result_content,
                    "type": "tool_call"
                })

                # æ„é€  observation å¹¶æ·»åŠ åˆ°å¯¹è¯
                observation = f"<observation>{result_content}</observation>"
                messages.append({"role": "assistant", "content": response})
                messages.append({"role": "user", "content": observation})
            else:
                # action ä¸º None - æ²¡æœ‰æ‰¾åˆ° <action> æ ‡ç­¾
                # è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼ˆå“åº”ä¸­æœ‰ final_answer è€Œé actionï¼‰
                self._log_info(_("tools.call.no_action"))
                break
        
        self._log_header(f"\n{_('analysis.summary.title')}")
        self._log_info(_("analysis.summary.total_iterations", iterations=iteration))
        self._log_info(_("analysis.summary.final_celltype", celltype=final_celltype))
        self._log_info(_("analysis.summary.tool_calls", count=len([log for log in analysis_log if log['type'] == 'tool_call'])))
        if iteration >= self.context_summary_threshold:
            self._log_info(_("analysis.summary.context_summary", threshold=self.context_summary_threshold))
        
        # æ‰“å° LLM æ—¥å¿—ç»Ÿè®¡
        if self.llm_logger:
            try:
                log_summary = self.llm_logger.get_log_summary()
                self._log_info(_("analysis.summary.llm_stats", total=log_summary.get('total_requests', 0)))
                self._log_info(_("analysis.summary.success_failure", success=log_summary.get('success_count', 0), failure=log_summary.get('error_count', 0)))
                self._log_info(_("analysis.summary.log_file", file=log_summary.get('log_file', 'N/A')))
            except Exception as e:
                self._log_error(_("analysis.summary.stats_failed", error=e))
        
        return {
            "gene_list": gene_list,
            "tissue_type": tissue_type,
            "final_celltype": final_celltype,
            "final_answer": final_answer_text,
            "total_iterations": iteration,
            # "context_summarized": iteration >= self.context_summary_threshold,
            # "context_summary_threshold": self.context_summary_threshold,
            # "analysis_log": analysis_log,
            # "llm_log_summary": self.llm_logger.get_log_summary() if self.llm_logger else None,
            "success": final_celltype is not None,
            "detected_species": detected_species  # ğŸŒŸ æ–°å¢ï¼šè¿”å›ç‰©ç§ä¿¡æ¯
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # åœæ­¢ MCP æœåŠ¡å™¨
            await self.mcp_client.stop_server()
            
            # å…³é—­ LLM æ—¥å¿—è®°å½•å™¨
            if self.llm_logger:
                await self.llm_logger.close()
            
            # æ¸…ç†å¯¹è±¡å¼•ç”¨
            self.available_tools = []
            
            # ç»™å¼‚æ­¥æ¸…ç†è¿‡ç¨‹å……è¶³æ—¶é—´
            await asyncio.sleep(0.3)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œæ¸…ç†æœªå¼•ç”¨çš„å¯¹è±¡
            gc.collect()
            
            # å†æ¬¡å»¶è¿Ÿç¡®ä¿åƒåœ¾å›æ”¶å®Œæˆ
            await asyncio.sleep(0.1)
            
        except Exception as e:
            self._log_warning(f"âš ï¸ æ¸…ç†èµ„æºæ—¶å‡ºç°å¼‚å¸¸: {e}")
        
        self._log_success("âœ… èµ„æºæ¸…ç†å®Œæˆ")

    def _log_info(self, message: str) -> None:
        """è¾“å‡ºä¿¡æ¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.info(message)
        else:
            print(message)

    def _log_success(self, message: str) -> None:
        """è¾“å‡ºæˆåŠŸæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.success(message)
        else:
            print(message)

    def _log_error(self, message: str) -> None:
        """è¾“å‡ºé”™è¯¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.error(message)
        else:
            print(message)

    def _log_warning(self, message: str) -> None:
        """è¾“å‡ºè­¦å‘Šæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.warning(message)
        else:
            print(message)

    def _log_header(self, message: str) -> None:
        """è¾“å‡ºæ ‡é¢˜æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.header(message)
        else:
            print(message)

    def _log_separator(self, char: str = "=", length: int = 60) -> None:
        """è¾“å‡ºåˆ†éš”çº¿"""
        if self.console_logger:
            self.console_logger.separator(char, length)
        else:
            print(char * length)
