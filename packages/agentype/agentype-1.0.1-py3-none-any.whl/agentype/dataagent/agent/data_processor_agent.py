#!/usr/bin/env python3
"""
agentype - Data Processor Agentæ¨¡å—
Author: cuilei
Version: 1.0
"""

import asyncio
import gc
import json
import os
import requests
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Union, Optional
import sys

# Tokenç»Ÿè®¡æ¨¡å—
from agentype.common.token_statistics import TokenReporter

# AnnData ç›¸å…³å¯¼å…¥
try:
    from anndata import AnnData
    import scanpy as sc
    ANNDATA_AVAILABLE = True
except ImportError:
    AnnData = None
    sc = None
    ANNDATA_AVAILABLE = False

# å¯¼å…¥ prompts
from agentype.dataagent.config.prompts import (
    SUPPORTED_LANGUAGES,
    DEFAULT_LANGUAGE,
    get_system_prompt_template,
    get_fallback_prompt_template,
    get_user_query_templates
)

# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from agentype.dataagent.config.settings import ConfigManager
from agentype.dataagent.clients.mcp_client import MCPClient
from agentype.dataagent.utils.content_processor import ContentProcessor
from agentype.dataagent.utils.parser import ReactParser
from agentype.dataagent.utils.validator import ValidationUtils
from agentype.dataagent.utils.i18n import _
from agentype.dataagent.utils.path_manager import path_manager
from agentype.dataagent.utils.output_logger import OutputLogger
# å¯¼å…¥å…±äº«æ¨¡å—
from agentype.common.llm_client import LLMClient
from agentype.common.llm_logger import LLMLogger
from agentype.common.streaming_filter import StreamingFilter
# å¯¼å…¥æ–‡ä»¶è·¯å¾„å·¥å…·
from agentype.mainagent.tools.file_paths_tools import load_file_paths_bundle

class DataProcessorReactAgent:
    """æ•°æ®å¤„ç† React Agent
    
    è¯¥ç±»æ˜¯æ•°æ®å¤„ç†ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œè´Ÿè´£åè°ƒ MCP å®¢æˆ·ç«¯ã€å†…å®¹å¤„ç†å™¨ã€
    éªŒè¯å·¥å…·ç­‰ç»„ä»¶ï¼Œæ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ã€‚
    """
    
    def __init__(self,
                 config: ConfigManager,
                 server_script: str = None,
                 max_content_length: int = 10000,
                 enable_summarization: bool = True,
                 max_iterations: int = 50,
                 context_summary_threshold: int = 15,
                 max_context_length: int = 150000,
                 enable_llm_logging: bool = True,
                 log_dir: str = None,
                 language: str = "zh",
                 enable_streaming: bool = True,
                 api_timeout: int = 300,
                 enable_console_output: bool = True,
                 console_output: bool = None,
                 file_output: bool = None,
                 session_id: str = None):

        # é…ç½®éªŒè¯
        print(f"ğŸ” è°ƒè¯•è¾“å‡º: DataAgentåˆå§‹åŒ–å¼€å§‹...")

        # ğŸŒŸ è®¾ç½® session_idï¼ˆå¦‚æœæä¾›ï¼‰
        # æ³¨æ„ï¼šåœ¨MCPæ¨¡å¼ä¸‹ï¼Œsession_idå·²åœ¨MCP Serverå¯åŠ¨æ—¶è®¾ç½®ï¼Œæ­¤å¤„ä¸å†é‡å¤æ‰“å°
        if session_id:
            from agentype.mainagent.config.session_config import set_session_id
            set_session_id(session_id)

        if not config:
            raise ValueError("é…ç½®å¯¹è±¡ä¸èƒ½ä¸ºç©º")

        # éªŒè¯å…³é”®é…ç½®é¡¹
        if not hasattr(config, 'openai_api_base') or not config.openai_api_base:
            raise ValueError("ç¼ºå°‘å¿…è¦é…ç½®: openai_api_base")
        if not hasattr(config, 'openai_api_key') or not config.openai_api_key:
            raise ValueError("ç¼ºå°‘å¿…è¦é…ç½®: openai_api_key")

        print(f"âœ… é…ç½®éªŒè¯é€šè¿‡")
        print(f"ğŸ” è°ƒè¯•è¾“å‡º: API Base: {config.openai_api_base}")
        print(f"ğŸ” è°ƒè¯•è¾“å‡º: API Key: {'***å·²è®¾ç½®***' if config.openai_api_key else 'None'}")
        print(f"ğŸ” è°ƒè¯•è¾“å‡º: Model: {getattr(config, 'openai_model', 'default')}")

        # ç»„ä»¶åˆå§‹åŒ–
        self.config = config
        # å¦‚æœæ²¡æœ‰æŒ‡å®šserver_scriptï¼Œä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–é»˜è®¤è·¯å¾„
        if server_script is None:
            server_script = str(path_manager.get_mcp_server_path())
        self.mcp_client = MCPClient(server_script, config=self.config)
        self.content_processor = ContentProcessor(max_content_length, enable_summarization)
        self.parser = ReactParser()
        self.validator = ValidationUtils()
        
        # è¯­è¨€è®¾ç½®
        self.language = language if language in SUPPORTED_LANGUAGES else DEFAULT_LANGUAGE
        
        # LLM æ—¥å¿—è®°å½•å™¨
        self.enable_llm_logging = enable_llm_logging
        if enable_llm_logging:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šlog_dirï¼Œä½¿ç”¨ ConfigManager çš„æ—¥å¿—ç›®å½•
            if log_dir is None:
                from pathlib import Path
                llm_log_dir = str(Path(self.config.log_dir) / "llm" / "DataAgent")
            else:
                llm_log_dir = str(path_manager.get_llm_log_dir(log_dir))
            self.llm_logger = LLMLogger(llm_log_dir)
        else:
            self.llm_logger = None
        
        # è¿è¡Œæ—¶çŠ¶æ€
        self.available_tools = []
        self.max_iterations = max_iterations
        self.context_summary_threshold = context_summary_threshold
        self.max_context_length = max_context_length
        
        # APIé…ç½®
        self.enable_streaming = enable_streaming
        self.api_timeout = api_timeout

        # åˆå§‹åŒ–æ§åˆ¶å°è¾“å‡ºæ—¥å¿—å™¨ - æ”¯æŒç²¾ç»†åŒ–æ§åˆ¶
        # å‚æ•°ä¼˜å…ˆçº§ï¼šconsole_output/file_output > enable_console_output
        if console_output is not None or file_output is not None:
            # ä½¿ç”¨ç²¾ç»†åŒ–æ§åˆ¶å‚æ•°
            self.console_output = console_output if console_output is not None else True
            self.file_output = file_output if file_output is not None else True
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨enable_console_output
            self.console_output = enable_console_output
            self.file_output = enable_console_output

        # åªæœ‰åœ¨è‡³å°‘ä¸€ä¸ªè¾“å‡ºæ–¹å¼å¯ç”¨æ—¶æ‰åˆ›å»ºlogger
        if self.console_output or self.file_output:
            self.console_logger = OutputLogger(
                log_prefix="celltypeDataAgent",
                console_output=self.console_output,
                file_output=self.file_output,
                log_dir=str(self.config.log_dir)
            )
        else:
            self.console_logger = None

        # åˆå§‹åŒ–tokenæŠ¥å‘Šå™¨
        self.token_reporter = TokenReporter(language=self.language)

        # ğŸŒŸ åˆå§‹åŒ–å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰
        self.llm_client = LLMClient(
            config=config,
            logger_callbacks={
                'info': self._log_info,
                'success': self._log_success,
                'warning': self._log_warning,
                'error': self._log_error
            }
        )

    async def initialize(self) -> bool:
        """åˆå§‹åŒ– agent"""
        print(f"ğŸ” è°ƒè¯•è¾“å‡º: DataAgentå¼€å§‹åˆå§‹åŒ–...")

        # å¯åŠ¨ MCP æœåŠ¡å™¨
        print(f"ğŸ” è°ƒè¯•è¾“å‡º: å¯åŠ¨MCPæœåŠ¡å™¨...")
        if not await self.mcp_client.start_server():
            print(f"âŒ MCPæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
            return False
        print(f"âœ… MCPæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")

        # è·å–å¯ç”¨å·¥å…·
        print(f"ğŸ” è°ƒè¯•è¾“å‡º: è·å–å¯ç”¨å·¥å…·åˆ—è¡¨...")
        self.available_tools = await self.mcp_client.list_tools()
        print(f"âœ… è·å–åˆ° {len(self.available_tools)} ä¸ªå·¥å…·")

        # éªŒè¯LLMæ—¥å¿—è®°å½•å™¨æ˜¯å¦æ­£å¸¸
        if self.llm_logger:
            print(f"âœ… LLMæ—¥å¿—è®°å½•å™¨å·²å¯ç”¨: {self.llm_logger.log_dir}")
        else:
            print(f"âš ï¸ LLMæ—¥å¿—è®°å½•å™¨æœªå¯ç”¨")

        print(f"âœ… DataAgentåˆå§‹åŒ–å®Œæˆ")
        return True
    
    def set_language(self, language: str):
        """è®¾ç½®åˆ†æè¯­è¨€"""
        if language in SUPPORTED_LANGUAGES:
            self.language = language
            self._log_success(_("agent.language_set", language=language))
        else:
            self._log_warning(_("agent.language_not_supported", language=language, supported=SUPPORTED_LANGUAGES))

    def _get_system_prompt(self) -> str:
        """è·å–æ•°æ®å¤„ç† React ç³»ç»Ÿ prompt"""
        try:
            # ä½¿ç”¨å®é™…çš„é…ç½®è·¯å¾„ï¼ˆæ˜¾ç¤ºç»å¯¹è·¯å¾„ï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æµ‹ï¼‰
            base_dir = Path(self.config.output_dir)
            work_dir = base_dir
            results_dir = Path(self.config.results_dir)

            # ç”Ÿæˆç¤ºä¾‹æ–‡ä»¶è·¯å¾„
            input_file_example = str(work_dir / "data.rds")
            input_h5_example = str(work_dir / "data.h5")
            marker_genes_json_example = str(results_dir / "cluster_marker_genes.json")

            print(f"ğŸ” è°ƒè¯•è¾“å‡º: DataAgentè·¯å¾„ç¤ºä¾‹ï¼ˆä»…ä¾›å‚è€ƒï¼Œå®é™…è·¯å¾„ç”±MCP Serveré…ç½®å†³å®šï¼‰:")
            print(f"   - å·¥ä½œç›®å½•ç¤ºä¾‹: {work_dir.as_posix()}")
            print(f"   - ç»“æœç›®å½•ç¤ºä¾‹: {results_dir.as_posix()}")
            print(f"   - RDSç¤ºä¾‹: {input_file_example}")
            print(f"   - H5ç¤ºä¾‹: {input_h5_example}")
            print(f"   - marker_genes_jsonç¤ºä¾‹: {marker_genes_json_example}")

            # ä½¿ç”¨æŒ‡å®šè¯­è¨€çš„æ¨¡æ¿æ ¼å¼åŒ–
            template = get_system_prompt_template(self.language)
            tool_list = "\n".join([f"{i+1}. {tool.get('name', 'Unknown')}: {tool.get('description', '')}"
                                  for i, tool in enumerate(self.available_tools)])

            # æ ¹æ®è¯­è¨€è®¾ç½®ç¼“å­˜çŠ¶æ€
            cache_status = "âœ“ MCP æœåŠ¡å™¨å·²å¯åŠ¨" if self.language == 'zh' else "âœ“ MCP Server Started"

            # è·å–å½“å‰è¿è¡Œç›®å½•çš„æ–‡ä»¶åˆ—è¡¨
            try:
                files = [f for f in os.listdir('.') if os.path.isfile(f)]
                file_list = ', '.join(files)
            except Exception:
                file_list = "data_processor_mcp_server.py, data_processor_agent.py"

            # æ ¼å¼åŒ–æ¨¡æ¿ï¼Œä¸€æ¬¡æ€§æ›¿æ¢æ‰€æœ‰å ä½ç¬¦
            return template.format(
                tool_list=tool_list,
                operating_system="Linux",
                cache_status=cache_status,
                file_list=file_list,
                # æ·»åŠ åŠ¨æ€è·¯å¾„å‚æ•°
                results_dir=str(results_dir),
                input_file_example=input_file_example,
                input_h5_example=input_h5_example,
                json_file_example=marker_genes_json_example
            )

        except Exception as e:
            # fallback åˆ°ç®€åŒ–ç‰ˆæœ¬
            self._log_warning(_("agent.template_fallback", error=e))
            fallback_template = get_fallback_prompt_template(self.language)
            tool_names = ', '.join([tool.get('name', '') for tool in self.available_tools])
            return fallback_template.format(tool_names=tool_names)
    
    def _sort_files_by_priority(self, file_list):
        """æŒ‰æ–‡ä»¶ç±»å‹ä¼˜å…ˆçº§æ’åºï¼šrds > h5ad > h5 > csv > json

        Args:
            file_list: æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            æŒ‰ä¼˜å…ˆçº§æ’åºåçš„æ–‡ä»¶åˆ—è¡¨
        """
        priority_map = {
            '.rds': 1,
            '.h5ad': 2,
            '.h5': 3,
            '.csv': 4,
            '.json': 5
        }

        def get_priority(file_path):
            ext = os.path.splitext(str(file_path).lower())[1]
            return priority_map.get(ext, 99)  # æœªçŸ¥æ ¼å¼æ’åœ¨æœ€å

        return sorted(file_list, key=get_priority)

    def _auto_detect_species_from_json(self, marker_genes_json: str) -> Optional[Dict]:
        """
        ä»JSONæ–‡ä»¶è‡ªåŠ¨æ£€æµ‹ç‰©ç§ï¼ˆä¸ä½¿ç”¨LLMï¼‰

        Args:
            marker_genes_json: markeråŸºå› JSONæ–‡ä»¶è·¯å¾„

        Returns:
            ç‰©ç§æ£€æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å«detected_speciesã€confidenceç­‰ä¿¡æ¯
            å¦‚æœæ£€æµ‹å¤±è´¥è¿”å›None
        """
        try:
            from agentype.dataagent.utils.common import SpeciesDetector

            if not marker_genes_json or not os.path.exists(marker_genes_json):
                self._log_warning(f"âš ï¸ JSONæ–‡ä»¶ä¸å­˜åœ¨: {marker_genes_json}")
                return None

            self._log_info(f"ğŸ” æ­£åœ¨ä»JSONæ–‡ä»¶è‡ªåŠ¨æ£€æµ‹ç‰©ç§: {marker_genes_json}")

            # è¯»å–JSONå¹¶æå–åŸºå› 
            with open(marker_genes_json, 'r', encoding='utf-8') as f:
                data = json.load(f)

            gene_names = []
            # æ”¯æŒå¤šç§JSONæ ¼å¼
            if isinstance(data, dict):
                # æ ¼å¼1: {"cell_type1": ["gene1", "gene2"], ...}
                for value in data.values():
                    if isinstance(value, list):
                        gene_names.extend(value)
                # æ ¼å¼2: {"genes": ["gene1", "gene2"]}
                if 'genes' in data and isinstance(data['genes'], list):
                    gene_names = data['genes']
            elif isinstance(data, list):
                # æ ¼å¼3: ["gene1", "gene2", ...]
                if all(isinstance(item, str) for item in data):
                    gene_names = data
                # æ ¼å¼4: [{"gene": "gene1"}, ...]
                elif all(isinstance(item, dict) for item in data):
                    for item in data:
                        if 'gene' in item:
                            gene_names.append(item['gene'])
                        elif 'symbol' in item:
                            gene_names.append(item['symbol'])

            # å»é‡
            gene_names = list(set(g for g in gene_names if g and isinstance(g, str)))

            if not gene_names:
                self._log_warning("âš ï¸ JSONæ–‡ä»¶ä¸­æœªæ‰¾åˆ°åŸºå› ä¿¡æ¯")
                return None

            self._log_info(f"ğŸ“Š ä»JSONæå–åˆ° {len(gene_names)} ä¸ªåŸºå› ")

            # è°ƒç”¨æ£€æµ‹å™¨ï¼ˆé˜ˆå€¼è®¾ç½®ä¸º0.8ï¼Œæœ€å°‘5ä¸ªåŸºå› ï¼‰
            detector = SpeciesDetector(uppercase_threshold=0.8, min_genes_required=5)
            species, info = detector.detect_species_from_genes(gene_names)

            self._log_success(f"âœ… DataAgentè‡ªåŠ¨æ£€æµ‹ç‰©ç§: {species} (ç½®ä¿¡åº¦: {info.get('confidence')})")
            self._log_info(f"   å¤§å†™æ¯”ä¾‹: {info.get('uppercase_ratio'):.3f}, æœ‰æ•ˆåŸºå› æ•°: {info.get('valid_genes')}")

            return {
                "detected_species": species,
                "detection_method": "json_genes_automatic",
                "confidence": info.get("confidence"),
                "gene_count": len(gene_names),
                "uppercase_ratio": info.get("uppercase_ratio"),
                "uppercase_count": info.get("uppercase_count"),
                "threshold": info.get("threshold")
            }
        except Exception as e:
            self._log_warning(f"âš ï¸ è‡ªåŠ¨ç‰©ç§æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            self._log_warning(f"   è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}")
            return None
    
    def _save_anndata_to_h5ad(self, adata: 'AnnData') -> str:
        """å°† AnnData å¯¹è±¡ä¿å­˜ä¸º h5ad æ–‡ä»¶
        
        Args:
            adata: scanpy AnnData å¯¹è±¡
            
        Returns:
            ä¿å­˜çš„ h5ad æ–‡ä»¶è·¯å¾„
        """
        if not ANNDATA_AVAILABLE or AnnData is None:
            raise ImportError("scanpy å’Œ anndata åº“æœªå®‰è£…ï¼Œæ— æ³•å¤„ç† AnnData å¯¹è±¡")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨è·å–ç¼“å­˜ç›®å½•
        from agentype.dataagent.config.cache_config import get_cache_dir
        from agentype.config import get_session_id_for_filename
        cache_dir = get_cache_dir()

        # ä½¿ç”¨ session_id ç”Ÿæˆæ–‡ä»¶å
        session_id = get_session_id_for_filename()
        h5ad_filename = f"adata_{session_id}.h5ad"
        h5ad_path = cache_dir / h5ad_filename
        
        try:
            self._log_info(f"ğŸ“Š AnnData å¯¹è±¡ä¿¡æ¯: {adata.n_obs} ç»†èƒ, {adata.n_vars} åŸºå› ")

            # ä¿å­˜ä¸º h5ad æ–‡ä»¶ï¼Œä½¿ç”¨é€‚åº¦å‹ç¼©
            self._log_info(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ AnnData å¯¹è±¡åˆ°: {h5ad_path}")
            adata.write_h5ad(str(h5ad_path), compression='gzip')

            self._log_success(f"âœ“ AnnData å¯¹è±¡å·²æˆåŠŸä¿å­˜ä¸º h5ad æ–‡ä»¶")
            return str(h5ad_path)

        except Exception as e:
            self._log_error(f"âŒ ä¿å­˜ AnnData å¯¹è±¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    class StreamingFilter:
        """æµå¼è¾“å‡ºå†…å®¹è¿‡æ»¤å™¨"""
        
        def __init__(self):
            self.buffer = ""
            self.state = "normal"  # normal, in_action, in_thought, in_final_answer
            self.pending_output = ""
        
        def filter_chunk(self, new_chunk: str) -> str:
            """è¿‡æ»¤æ–°çš„å†…å®¹å—ï¼Œè¿”å›åº”è¯¥æ˜¾ç¤ºçš„éƒ¨åˆ†"""
            if not new_chunk:
                return ""
            
            output = ""
            self.buffer += new_chunk
            
            processed = 0
            i = 0
            
            while i < len(self.buffer):
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—ç¬¦è¿›è¡Œæ ‡ç­¾åŒ¹é…
                remaining = self.buffer[i:]
                
                if self.state == "normal":
                    if remaining.startswith('<action>'):
                        self.state = "in_action"
                        i += len('<action>')
                        processed = i
                    elif remaining.startswith('<thought>'):
                        self.state = "in_thought"
                        i += len('<thought>')
                        processed = i
                    elif remaining.startswith('<final_answer>'):
                        self.state = "in_final_answer"
                        i += len('<final_answer>')
                        processed = i
                    elif remaining.startswith('<'):
                        # å¯èƒ½æ˜¯ä¸å®Œæ•´çš„æ ‡ç­¾ï¼Œåœæ­¢å¤„ç†
                        potential_tags = ['<action>', '<thought>', '<final_answer>']
                        is_potential = False
                        for tag in potential_tags:
                            if tag.startswith(remaining[:min(len(remaining), len(tag))]):
                                is_potential = True
                                break
                        
                        if is_potential and len(remaining) < 15:  # æ ‡ç­¾é•¿åº¦é™åˆ¶
                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                        else:
                            output += self.buffer[i]
                            i += 1
                            processed = i
                    else:
                        output += self.buffer[i]
                        i += 1
                        processed = i
                        
                elif self.state == "in_action":
                    if remaining.startswith('</action>'):
                        self.state = "normal"
                        i += len('</action>')
                        processed = i
                    elif remaining.startswith('</') and len(remaining) < 10:
                        # å¯èƒ½æ˜¯ä¸å®Œæ•´çš„ç»“æŸæ ‡ç­¾
                        if '</action>'.startswith(remaining):
                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                        else:
                            i += 1
                            processed = i
                    else:
                        # è·³è¿‡actionå†…å®¹
                        i += 1
                        processed = i
                        
                elif self.state == "in_thought":
                    if remaining.startswith('</thought>'):
                        self.state = "normal"
                        i += len('</thought>')
                        processed = i
                    elif remaining.startswith('</') and len(remaining) < 11:
                        # å¯èƒ½æ˜¯ä¸å®Œæ•´çš„ç»“æŸæ ‡ç­¾
                        if '</thought>'.startswith(remaining):
                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                        else:
                            output += self.buffer[i]
                            i += 1
                            processed = i
                    else:
                        output += self.buffer[i]
                        i += 1
                        processed = i
                        
                elif self.state == "in_final_answer":
                    if remaining.startswith('</final_answer>'):
                        self.state = "normal"
                        i += len('</final_answer>')
                        processed = i
                    elif remaining.startswith('</') and len(remaining) < 16:
                        # å¯èƒ½æ˜¯ä¸å®Œæ•´çš„ç»“æŸæ ‡ç­¾
                        if '</final_answer>'.startswith(remaining):
                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                        else:
                            output += self.buffer[i]
                            i += 1
                            processed = i
                    else:
                        output += self.buffer[i]
                        i += 1
                        processed = i
            
            # ä¿ç•™æœªå¤„ç†çš„éƒ¨åˆ†
            self.buffer = self.buffer[processed:]
            return output

    async def _call_openai(self, messages: List[Dict], timeout: int = 270, stream: bool = False, request_type: str = "main") -> str:
        """è°ƒç”¨ OpenAI API - ä½¿ç”¨å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰"""
        return await self.llm_client.call_api(
            messages=messages,
            timeout=timeout,
            stream=stream,
            request_type=request_type,
            llm_logger=self.llm_logger,
            console_logger=self.console_logger
        )
    async def _retry_llm_call_with_correction(self, messages: List[Dict], validation_result: Dict, max_retries: int = 10, timeout: int = 270) -> str:
        """é‡æ–°è°ƒç”¨ LLM å¹¶æä¾›ä¿®æ­£æŒ‡å¯¼"""
        retry_count = 0
        response = ""
        
        while retry_count < max_retries:
            retry_count += 1
            self._log_warning(_("retry.llm_retry", retry=retry_count, max_retries=max_retries))
            self._log_info(_("retry.last_response_issues", issues=', '.join(validation_result['issues'])))
            
            # æ„å»ºä¿®æ­£æç¤º
            correction_prompt = ValidationUtils.build_correction_prompt(validation_result, self.available_tools, self.language)
            
            # æ·»åŠ ä¿®æ­£æç¤ºåˆ°æ¶ˆæ¯å†å²
            correction_messages = messages.copy()
            correction_messages.append({"role": "user", "content": correction_prompt})
            
            # é‡æ–°è°ƒç”¨ LLMï¼Œä¿æŒä¸ä¸»è°ƒç”¨ç›¸åŒçš„æµå¼è¾“å‡ºè®¾ç½®
            response = await self._call_openai(correction_messages, timeout, stream=self.enable_streaming)
            
            # ä¸ºé‡è¯•è°ƒç”¨æ·»åŠ é¢å¤–çš„æ—¥å¿—æ ‡è®°
            if self.llm_logger:
                self._log_info(_("retry.retry_logged", count=retry_count))
            
            # éªŒè¯æ–°å“åº”
            new_validation = ValidationUtils.validate_response_format(response)
            
            if new_validation['valid']:
                self._log_success(_("retry.retry_success"))
                return response
            else:
                self._log_error(_("retry.retry_failed", count=retry_count, issues=', '.join(new_validation['issues'])))
                validation_result = new_validation
        
        self._log_error(_("retry.max_retries_reached"))
        return response
    
    async def _summarize_context(self, messages: List[Dict]) -> List[Dict]:
        """å½“å¯¹è¯å†å²è¿‡é•¿æ—¶ï¼Œæ€»ç»“ä¸Šä¸‹æ–‡ä»¥å‡å°‘tokenæ¶ˆè€—
        
        Args:
            messages: å½“å‰çš„å¯¹è¯å†å²
            
        Returns:
            æ€»ç»“åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        try:
            self._log_warning(_("context.too_long"))
            
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€åˆçš„ç”¨æˆ·æŸ¥è¯¢
            system_message = messages[0] if messages and messages[0]["role"] == "system" else None
            initial_query = messages[1] if len(messages) > 1 and messages[1]["role"] == "user" else None
            
            # æŸ¥æ‰¾ä¹‹å‰çš„æ€»ç»“æ¶ˆæ¯
            existing_summary = None
            summary_index = -1
            for i, msg in enumerate(messages):
                if msg.get("role") == "assistant" and "[ä¸Šä¸‹æ–‡æ€»ç»“]" in msg.get("content", ""):
                    existing_summary = msg
                    summary_index = i
                    break
            
            # ç¡®å®šéœ€è¦æ€»ç»“çš„å¯¹è¯å†å²èŒƒå›´
            if existing_summary and summary_index > 1:
                # å¦‚æœå·²æœ‰æ€»ç»“ï¼Œä»æ€»ç»“åå¼€å§‹åˆ°å€’æ•°å‡ è½®
                conversation_to_summarize = messages[summary_index+1:-4] if len(messages) > summary_index + 5 else messages[summary_index+1:]
                recent_messages = messages[-4:] if len(messages) > summary_index + 5 else []
            else:
                # å¦‚æœæ²¡æœ‰æ€»ç»“ï¼Œä»ç¬¬3æ¡æ¶ˆæ¯å¼€å§‹ï¼ˆè·³è¿‡ç³»ç»Ÿæ¶ˆæ¯å’Œåˆå§‹æŸ¥è¯¢ï¼‰
                conversation_to_summarize = messages[2:-4] if len(messages) > 6 else messages[2:]
                recent_messages = messages[-4:] if len(messages) > 6 else []
            
            if not conversation_to_summarize:
                self._log_info(_("context.no_history"))
                return messages
            
            # æ„å»ºæ€»ç»“prompt
            conversation_text = ""
            for msg in conversation_to_summarize:
                role = msg["role"]
                content = msg["content"]
                conversation_text += f"\n{role.upper()}: {content}\n"
            
            # æ ¹æ®æ˜¯å¦å­˜åœ¨ä¹‹å‰çš„æ€»ç»“æ¥æ„å»ºä¸åŒçš„prompt
            if existing_summary:
                existing_summary_content = existing_summary.get("content", "").replace("[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„å¤„ç†è¿›å±•ï¼š", "").strip()
                summarization_prompt = f"""è¯·åŸºäºä¹‹å‰çš„æ€»ç»“ï¼Œç»§ç»­æ€»ç»“æ–°å¢çš„æ•°æ®å¤„ç†å¯¹è¯å†å²ï¼š

ä¹‹å‰çš„æ€»ç»“ï¼š
{existing_summary_content}

æ–°å¢çš„å¯¹è¯å†å²ï¼š
{conversation_text}

è¯·æ›´æ–°æ€»ç»“ï¼Œæ•´åˆæ–°å‘ç°çš„ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
1. å·²æ‰§è¡Œçš„ä¸»è¦å·¥å…·è°ƒç”¨åŠå…¶ç»“æœ
2. å‘ç°çš„é‡è¦æ•°æ®ä¿¡æ¯æˆ–å¤„ç†çº¿ç´¢  
3. å½“å‰å¤„ç†è¿›å±•å’Œå¾…è§£å†³çš„é—®é¢˜

æ€»ç»“åº”è¯¥ç®€æ˜æ‰¼è¦ï¼Œé‡ç‚¹å…³æ³¨å¯¹åç»­å¤„ç†æœ‰ç”¨çš„ä¿¡æ¯ã€‚"""
            else:
                summarization_prompt = f"""è¯·æ€»ç»“ä»¥ä¸‹æ•°æ®å¤„ç†çš„å¯¹è¯å†å²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œé‡è¦å‘ç°ï¼š

å¯¹è¯å†å²ï¼š
{conversation_text}

è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼ŒåŒ…å«ï¼š
1. å·²æ‰§è¡Œçš„ä¸»è¦å·¥å…·è°ƒç”¨åŠå…¶ç»“æœ
2. å‘ç°çš„é‡è¦æ•°æ®ä¿¡æ¯æˆ–å¤„ç†çº¿ç´¢
3. å½“å‰å¤„ç†è¿›å±•å’Œå¾…è§£å†³çš„é—®é¢˜

æ€»ç»“åº”è¯¥ç®€æ˜æ‰¼è¦ï¼Œé‡ç‚¹å…³æ³¨å¯¹åç»­å¤„ç†æœ‰ç”¨çš„ä¿¡æ¯ã€‚"""

            # è°ƒç”¨OpenAIè¿›è¡Œæ€»ç»“ï¼Œä¿æŒä¸ä¸»è°ƒç”¨ç›¸åŒçš„æµå¼è¾“å‡ºè®¾ç½®
            summary_messages = [{"role": "user", "content": summarization_prompt}]
            summary = await self._call_openai(summary_messages, timeout=self.api_timeout, stream=self.enable_streaming)  # ä¸Šä¸‹æ–‡æ€»ç»“ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
            
            # è®°å½•æ€»ç»“è¯·æ±‚æ—¥å¿—
            if self.llm_logger:
                self._log_info(_("context.summary_logged"))
            
            # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
            new_messages = []
            
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            if system_message:
                new_messages.append(system_message)
            
            # ä¿ç•™åˆå§‹æŸ¥è¯¢  
            if initial_query:
                new_messages.append(initial_query)
            
            # æ·»åŠ æ›´æ–°çš„æ€»ç»“æ¶ˆæ¯ï¼ˆæ›¿æ¢ä¹‹å‰çš„æ€»ç»“ï¼‰
            summary_message = {
                "role": "assistant", 
                "content": f"[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„å¤„ç†è¿›å±•ï¼š{summary}"
            }
            new_messages.append(summary_message)
            
            # æ·»åŠ æœ€è¿‘çš„æ¶ˆæ¯ä»¥ä¿æŒè¿ç»­æ€§
            new_messages.extend(recent_messages)
            
            self._log_success(_("context.summary_completed"))
            self._log_info(_("context.original_messages", count=len(messages)))
            self._log_info(_("context.summarized_messages", count=len(new_messages)))
            self._log_info(_("context.summary_length", length=len(summary)))
            
            return new_messages
            
        except Exception as e:
            error_msg = f"ä¸Šä¸‹æ–‡æ€»ç»“å¤±è´¥: {e}"
            self._log_error(f"âŒ {error_msg}")

            # æ€»ç»“å¤±è´¥æ—¶ï¼Œç®€å•åœ°ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            if len(messages) > 10:
                self._log_info(_("context.simple_strategy"))
                return messages[-10:]
            
            return messages
    
    async def process_data(self, input_data: Union[str, List[str], 'AnnData'], species: Optional[str] = None) -> Dict:
        """å¤„ç†æ•°æ®

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯ï¼š
                - å•ä¸ªæ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
                - æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                - scanpy AnnData å¯¹è±¡
            species: å¯é€‰çš„ç‰©ç§å‚æ•°(ä»MainAgentä¼ é€’)

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        # æ£€æµ‹å¹¶å¤„ç† AnnData å¯¹è±¡
        if ANNDATA_AVAILABLE and AnnData is not None and isinstance(input_data, AnnData):
            self._log_info("ğŸ§¬ æ£€æµ‹åˆ° AnnData å¯¹è±¡ï¼Œæ­£åœ¨ä¿å­˜ä¸º h5ad æ–‡ä»¶...")
            input_data = self._save_anndata_to_h5ad(input_data)
            self._log_success(f"âœ“ AnnData å¯¹è±¡å·²ä¿å­˜ä¸º: {input_data}")
        
        self._log_info(_("data_analysis.starting", files=input_data))
        
        # å‡†å¤‡ç³»ç»Ÿ prompt å’Œç”¨æˆ·æŸ¥è¯¢
        system_prompt = self._get_system_prompt()
        
        # è·å–ç”¨æˆ·æŸ¥è¯¢æ¨¡æ¿
        query_templates = get_user_query_templates(self.language)
        
        # æ ¹æ®è¾“å…¥ç±»å‹ç”Ÿæˆç”¨æˆ·æŸ¥è¯¢
        if isinstance(input_data, str):
            # å•æ–‡ä»¶å¤„ç†
            user_query = query_templates["single_file"].format(file_path=input_data)
        elif isinstance(input_data, (list, tuple)):
            # å¤šæ–‡ä»¶å¤„ç†ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
            sorted_files = self._sort_files_by_priority(input_data)
            main_file = sorted_files[0]
            file_paths_str = ", ".join(sorted_files)
            user_query = query_templates["multiple_files"].format(
                file_paths=file_paths_str, 
                main_file=main_file
            )
        else:
            # å…œåº•å¤„ç†
            user_query = f"è¯·åˆ†æå¹¶å¤„ç†è¿™ä¸ªæ•°æ®æ–‡ä»¶ï¼š{input_data}ã€‚è¯·è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶æ ¼å¼å’Œé€‚åˆçš„å¤„ç†æ–¹å¼ã€‚"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"<question>{user_query}</question>"}
        ]
        
        self._log_info(_("data_analysis.system_prompt_length", length=len(system_prompt)))
        self._log_info(_("data_analysis.user_query", query=user_query) + "\n")
        
        analysis_log = []
        iteration = 0
        final_result = None
        
        while iteration < self.max_iterations:
            iteration += 1
            self._log_info(_("data_analysis.iteration", iteration=iteration))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œä¸Šä¸‹æ–‡æ€»ç»“
            should_summarize = False
            summarize_reason = ""
            
            # æ£€æŸ¥è¿­ä»£æ¬¡æ•°é˜ˆå€¼
            if iteration == self.context_summary_threshold:
                should_summarize = True
                summarize_reason = f"è¾¾åˆ°è¿­ä»£æ¬¡æ•°é˜ˆå€¼ï¼ˆç¬¬{iteration}æ¬¡è¿­ä»£ï¼‰"
                
            # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä¼°ç®—tokenæ•°é‡ï¼‰
            total_content_length = sum(len(msg.get("content", "")) for msg in messages)
            self._log_info(_("data_analysis.current_messages") + f" {len(messages)}")
            self._log_info(_("data_analysis.current_context_length") + f" {total_content_length}")
            if total_content_length > self.max_context_length:
                should_summarize = True
                if summarize_reason:
                    summarize_reason += f" ä¸”ä¸Šä¸‹æ–‡è¿‡é•¿ï¼ˆ{total_content_length}å­—ç¬¦ï¼Œé˜ˆå€¼{self.max_context_length}ï¼‰"
                else:
                    summarize_reason = f"ä¸Šä¸‹æ–‡è¿‡é•¿ï¼ˆ{total_content_length}å­—ç¬¦ï¼Œé˜ˆå€¼{self.max_context_length}ï¼‰"
            
            # æ‰§è¡Œæ€»ç»“
            if should_summarize and len(messages) > 6:
                self._log_info(_("data_analysis.context_compression", reason=summarize_reason))
                messages = await self._summarize_context(messages)
            
            # ä½¿ç”¨å…¨å±€é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            if self.enable_streaming:
                self._log_info(_("data_analysis.streaming_enabled", iteration=iteration))
            
            # è°ƒç”¨ OpenAIï¼ˆä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼‰
            response = await self._call_openai(messages, timeout=self.api_timeout, stream=self.enable_streaming)
            self._log_info(_("data_analysis.ai_response_length", length=len(response)))
            
            # éªŒè¯å“åº”æ ¼å¼
            validation = ValidationUtils.validate_response_format(response)
            
            if not validation['valid']:
                self._log_warning(_("data_analysis.format_validation_failed", issues=', '.join(validation['issues'])))
                response = await self._retry_llm_call_with_correction(messages, validation, 10, self.api_timeout)
            else:
                self._log_success(_("data_analysis.format_validation_passed"))
            
            analysis_log.append({
                "iteration": iteration,
                "response": response,
                "validation": validation,
                "type": "ai_response"
            })
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å« final_answer
            if '</final_answer>' in response:
                final_result = ReactParser.extract_final_answer(response)
                if final_result:
                    self._log_success(_("data_analysis.analysis_completed", result=final_result))
                    break
                else:
                    self._log_warning(_("data_analysis.final_answer_no_result"))
            
            # æå–å¹¶æ‰§è¡Œ action
            action = ReactParser.extract_action(response, self.available_tools)
            if action:
                # æ£€æŸ¥æ˜¯å¦æœ‰è§£æé”™è¯¯
                if 'error' in action:
                    error_msg = action.get('message', action.get('error'))
                    self._log_warning(f"Actionè§£æé”™è¯¯: {error_msg}")
                    # æ„é€ observationè®©LLMçŸ¥é“é—®é¢˜
                    error_observation = f"<observation>{{\"error\": \"{error_msg}\"}}</observation>"
                    messages.append({"role": "user", "content": error_observation})
                    continue

                function_name = action['function']
                parameters_str = action['parameters']
                parameters = ReactParser.parse_parameters(parameters_str)
                
                self._log_info(_("tools.call.calling", name=function_name, params=parameters))
                
                # æ‰§è¡Œ MCP å·¥å…·è°ƒç”¨
                tool_result = await self.mcp_client.call_tool(function_name, parameters)
                
                if tool_result and tool_result.get("success"):
                    raw_content = tool_result.get("content", "")
                    self._log_success(_("tools.call.success", length=len(raw_content)))
                    
                    # å¤„ç†å†…å®¹é•¿åº¦
                    result_content = await self.content_processor.process_tool_result_content(
                        raw_content, self._call_openai, self.language, 
                        tool_name=function_name, tool_params=parameters, mcp_client=self.mcp_client
                    )
                else:
                    result_content = json.dumps({
                        "error": tool_result.get("error", "å·¥å…·è°ƒç”¨å¤±è´¥") if tool_result else "æ— å“åº”",
                        "success": False
                    }, ensure_ascii=False)
                    self._log_error(_("tools.call.failed"))
                
                # è®°å½•åˆ†ææ—¥å¿—
                analysis_log.append({
                    "iteration": iteration,
                    "action": action,
                    "result": result_content,
                    "type": "tool_call"
                })
                
                # æ„é€  observation å¹¶æ·»åŠ åˆ°å¯¹è¯
                observation = f"<observation>{result_content}</observation>"
                messages.append({"role": "assistant", "content": response})
                messages.append({"role": "user", "content": observation})
            else:
                self._log_info(_("tools.call.no_action"))
                break
                
        
        self._log_header(f"\n{_('data_analysis.summary.title')}")
        self._log_info(_("data_analysis.summary.total_iterations", iterations=iteration))
        self._log_info(_("data_analysis.summary.final_result", result=final_result))
        self._log_info(_("data_analysis.summary.tool_calls", count=len([log for log in analysis_log if log['type'] == 'tool_call'])))
        if iteration >= self.context_summary_threshold:
            self._log_info(_("data_analysis.summary.context_summary", threshold=self.context_summary_threshold))
        
        # æ‰“å° LLM æ—¥å¿—ç»Ÿè®¡
        if self.llm_logger:
            try:
                log_summary = self.llm_logger.get_log_summary()
                self._log_info(_("data_analysis.summary.llm_stats", total=log_summary.get('total_requests', 0)))
                self._log_info(_("data_analysis.summary.success_failure", success=log_summary.get('success_count', 0), failure=log_summary.get('error_count', 0)))
                self._log_info(_("data_analysis.summary.log_file", file=log_summary.get('log_file', 'N/A')))
            except Exception as e:
                self._log_error(_("data_analysis.summary.stats_failed", error=e))
        
        # âœ… ä»bundleè¯»å–è·¯å¾„ï¼ˆMCPå·¥å…·å·²è‡ªåŠ¨ä¿å­˜åˆ°bundleï¼‰
        bundle = load_file_paths_bundle()
        output_file_paths = {}

        if bundle.get("success"):
            # æå–æ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶è·¯å¾„å­—æ®µ
            path_keys = ['rds_file', 'h5ad_file', 'h5_file', 'marker_genes_json', 'sce_h5', 'scanpy_h5']
            for key in path_keys:
                value = bundle.get(key)
                if value:  # åªä¿ç•™éç©ºå€¼
                    output_file_paths[key] = value

            self._log_info(f"ğŸ“ ä»bundleè¯»å–åˆ° {len(output_file_paths)} ä¸ªæ–‡ä»¶è·¯å¾„")

            # æ‰“å°è¯»å–çš„è·¯å¾„
            if output_file_paths:
                self._log_header(f"\nğŸ“ ä»bundleè¯»å–çš„æ–‡ä»¶è·¯å¾„:")
                for file_type, path in output_file_paths.items():
                    self._log_info(f"   {file_type}: {path}")
        else:
            self._log_warning("âš ï¸ æœªèƒ½ä»bundleè¯»å–è·¯å¾„")

        # ğŸŒŸ æ–°å¢ï¼šè‡ªåŠ¨ç‰©ç§æ£€æµ‹é€»è¾‘ï¼ˆå¦‚æœæœªæä¾›specieså‚æ•°ï¼‰
        detected_species_info = None
        if not species:
            # æŸ¥æ‰¾JSONæ–‡ä»¶ï¼ˆä»è¾“å‡ºè·¯å¾„æˆ–è¾“å…¥å‚æ•°ï¼‰
            marker_genes_json = output_file_paths.get("marker_genes_json") or output_file_paths.get("marker_json")

            if marker_genes_json:
                self._log_info("ğŸ” æœªæä¾›ç‰©ç§å‚æ•°ï¼Œå°è¯•ä»JSONæ–‡ä»¶è‡ªåŠ¨æ£€æµ‹...")
                detected_species_info = self._auto_detect_species_from_json(marker_genes_json)

                if detected_species_info:
                    self._log_success(f"âœ… è‡ªåŠ¨æ£€æµ‹ç‰©ç§æˆåŠŸ: {detected_species_info.get('detected_species')}")
                else:
                    self._log_warning("âš ï¸ æœªèƒ½ä»JSONæ–‡ä»¶æ£€æµ‹åˆ°ç‰©ç§ä¿¡æ¯")
            else:
                self._log_info("â„¹ï¸  æ— JSONæ–‡ä»¶å¯ç”¨äºç‰©ç§æ£€æµ‹")
        else:
            self._log_info(f"âœ… ä½¿ç”¨ä¼ å…¥çš„ç‰©ç§å‚æ•°: {species}")

        return {
            "input_data": input_data,
            "final_result": final_result,
            "output_file_paths": output_file_paths,  # ä»bundleè¯»å–çš„æ–‡ä»¶è·¯å¾„
            "total_iterations": iteration,
            "context_summarized": iteration >= self.context_summary_threshold,
            "context_summary_threshold": self.context_summary_threshold,
            "analysis_log": analysis_log,
            "llm_log_summary": self.llm_logger.get_log_summary() if self.llm_logger else None,
            "success": final_result is not None,
            # ğŸŒŸ æ–°å¢ï¼šç‰©ç§ä¿¡æ¯
            "species": species,  # ç”¨æˆ·æŒ‡å®šçš„ç‰©ç§(å¦‚æœæœ‰)
            "detected_species": detected_species_info.get("detected_species") if detected_species_info else None,
            "species_detection_info": detected_species_info,  # è¯¦ç»†çš„æ£€æµ‹ä¿¡æ¯
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # åœæ­¢ MCP æœåŠ¡å™¨
            await self.mcp_client.stop_server()
            
            # å…³é—­ LLM æ—¥å¿—è®°å½•å™¨
            if self.llm_logger:
                await self.llm_logger.close()
            
            # æ¸…ç†å¯¹è±¡å¼•ç”¨
            self.available_tools = []
            
            # ç»™å¼‚æ­¥æ¸…ç†è¿‡ç¨‹å……è¶³æ—¶é—´
            await asyncio.sleep(0.3)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œæ¸…ç†æœªå¼•ç”¨çš„å¯¹è±¡
            gc.collect()
            
            # å†æ¬¡å»¶è¿Ÿç¡®ä¿åƒåœ¾å›æ”¶å®Œæˆ
            await asyncio.sleep(0.1)
            
        except Exception as e:
            self._log_warning(f"âš ï¸ æ¸…ç†èµ„æºæ—¶å‡ºç°å¼‚å¸¸: {e}")

        self._log_success("âœ… èµ„æºæ¸…ç†å®Œæˆ")

    def _log_info(self, message: str) -> None:
        """è¾“å‡ºä¿¡æ¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.info(message)
        else:
            print(message)

    def _log_success(self, message: str) -> None:
        """è¾“å‡ºæˆåŠŸæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.success(message)
        else:
            print(message)

    def _log_error(self, message: str) -> None:
        """è¾“å‡ºé”™è¯¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.error(message)
        else:
            print(message)

    def _log_warning(self, message: str) -> None:
        """è¾“å‡ºè­¦å‘Šæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.warning(message)
        else:
            print(message)

    def _log_header(self, message: str) -> None:
        """è¾“å‡ºæ ‡é¢˜æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.header(message)
        else:
            print(message)

    def _log_separator(self, char: str = "=", length: int = 60) -> None:
        """è¾“å‡ºåˆ†éš”çº¿"""
        if self.console_logger:
            self.console_logger.separator(char, length)
        else:
            print(char * length)
