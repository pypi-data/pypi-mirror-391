#!/usr/bin/env python3
"""
agentype - Celltype Annotation Agentæ¨¡å—
Author: cuilei
Version: 1.0
"""

import asyncio
import gc
import json
import os
import requests
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
import sys

# Tokenç»Ÿè®¡æ¨¡å—
from agentype.common.token_statistics import TokenReporter

# Prompts for CellType App Agent
from agentype.appagent.config.prompts import (
    SUPPORTED_LANGUAGES,
    DEFAULT_LANGUAGE,
    get_system_prompt_template,
    get_fallback_prompt_template,
    get_user_query_templates,
    build_unified_user_query,
)
from agentype.common.language_manager import get_current_language

# Core components
from agentype.appagent.config.settings import ConfigManager
from agentype.appagent.clients.mcp_client import MCPClient
from agentype.appagent.utils.content_processor import CelltypeContentProcessor
from agentype.appagent.utils.parser import CelltypeReactParser
from agentype.appagent.utils.validator import CelltypeValidationUtils
from agentype.appagent.utils.i18n import _
from agentype.appagent.utils.output_logger import CelltypeOutputLogger
from agentype.appagent.utils.path_manager import path_manager
# å¯¼å…¥å…±äº«æ¨¡å—
from agentype.common.llm_client import LLMClient
from agentype.common.llm_logger import LLMLogger
from agentype.common.streaming_filter import StreamingFilter
# å¯¼å…¥æ–‡ä»¶è·¯å¾„å·¥å…·
from agentype.mainagent.tools.file_paths_tools import load_file_paths_bundle


class CelltypeAnnotationAgent:
    """ç»†èƒç±»å‹æ³¨é‡Š React Agentï¼ˆApp Agentï¼‰

    å‚è€ƒ DataProcessorReactAgent ä¸ CellTypeReactAgent çš„ç»“æ„ä¸é€»è¾‘ï¼Œ
    ä½¿ç”¨æœ¬æ¨¡å— prompts ä¸­è§„å®šçš„æ³¨é‡Šæµç¨‹ä¸æ ¼å¼è¿›è¡Œè¿­ä»£å¼å·¥å…·è°ƒç”¨ä¸æ¨ç†ã€‚
    """

    def __init__(
        self,
        config: Optional[ConfigManager] = None,
        server_script: str = None,
        max_content_length: int = 10000,
        enable_summarization: bool = True,
        max_iterations: int = 50,
        context_summary_threshold: int = 15,
        max_context_length: int = 150000,
        enable_llm_logging: bool = True,
        log_dir: str = None,
        language: Optional[str] = None,
        enable_streaming: bool = True,
        api_timeout: int = 300,
        enable_console_output: bool = True,
        console_output: bool = None,
        file_output: bool = None,
        session_id: str = None,
    ):
        # ğŸŒŸ è®¾ç½® session_idï¼ˆå¦‚æœæä¾›ï¼‰
        # æ³¨æ„ï¼šåœ¨MCPæ¨¡å¼ä¸‹ï¼Œsession_idå·²åœ¨MCP Serverå¯åŠ¨æ—¶è®¾ç½®ï¼Œæ­¤å¤„ä¸å†é‡å¤æ‰“å°
        if session_id:
            from agentype.mainagent.config.session_config import set_session_id
            set_session_id(session_id)

        # é…ç½®
        self.config = config or ConfigManager()

        # å¦‚æœæ²¡æœ‰æŒ‡å®šserver_scriptï¼Œä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–é»˜è®¤è·¯å¾„
        if server_script is None:
            server_script = str(path_manager.get_mcp_server_path())
        self.mcp_client = MCPClient(server_script, config=self.config)

        # å†…å®¹å¤„ç†ã€è§£æä¸éªŒè¯
        self.content_processor = CelltypeContentProcessor(
            max_content_length=max_content_length,
            enable_summarization=enable_summarization,
        )
        self.parser = CelltypeReactParser()
        self.validator = CelltypeValidationUtils()

        # è¯­è¨€
        resolved_language = language or get_current_language()
        self.language = resolved_language if resolved_language in SUPPORTED_LANGUAGES else DEFAULT_LANGUAGE

        # LLM æ—¥å¿—
        self.enable_llm_logging = enable_llm_logging
        # å¦‚æœæ²¡æœ‰æŒ‡å®šlog_dirï¼Œä½¿ç”¨ ConfigManager çš„æ—¥å¿—ç›®å½•
        if log_dir is None:
            from pathlib import Path
            log_dir = str(Path(self.config.log_dir) / "llm" / "AppAgent")
        self.llm_logger = LLMLogger(log_dir) if enable_llm_logging else None

        # è¿è¡Œæ—¶
        self.available_tools: List[Dict] = []
        self.max_iterations = max_iterations
        self.context_summary_threshold = context_summary_threshold
        self.max_context_length = max_context_length

        # API
        self.enable_streaming = enable_streaming
        self.api_timeout = api_timeout

        # åˆå§‹åŒ–æ§åˆ¶å°è¾“å‡ºæ—¥å¿—å™¨ - æ”¯æŒç²¾ç»†åŒ–æ§åˆ¶
        # å‚æ•°ä¼˜å…ˆçº§ï¼šconsole_output/file_output > enable_console_output
        if console_output is not None or file_output is not None:
            # ä½¿ç”¨ç²¾ç»†åŒ–æ§åˆ¶å‚æ•°
            self.console_output = console_output if console_output is not None else True
            self.file_output = file_output if file_output is not None else True
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨enable_console_output
            self.console_output = enable_console_output
            self.file_output = enable_console_output

        # åªæœ‰åœ¨è‡³å°‘ä¸€ä¸ªè¾“å‡ºæ–¹å¼å¯ç”¨æ—¶æ‰åˆ›å»ºlogger
        if self.console_output or self.file_output:
            self.console_logger = CelltypeOutputLogger(
                log_prefix="celltypeAppAgent",
                console_output=self.console_output,
                file_output=self.file_output,
                log_dir=str(self.config.log_dir)
            )
        else:
            self.console_logger = None

        # åˆå§‹åŒ–tokenæŠ¥å‘Šå™¨
        self.token_reporter = TokenReporter(language=self.language)

        # ğŸŒŸ åˆå§‹åŒ–å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰
        self.llm_client = LLMClient(
            config=config,
            logger_callbacks={
                'info': self._log_info,
                'success': self._log_success,
                'warning': self._log_warning,
                'error': self._log_error
            }
        )

    async def initialize(self) -> bool:
        """å¯åŠ¨ MCP æœåŠ¡å™¨å¹¶è·å–å·¥å…·åˆ—è¡¨"""
        if not await self.mcp_client.start_server():
            return False
        self.available_tools = await self.mcp_client.list_tools()
        return True

    def set_language(self, language: str):
        if language in SUPPORTED_LANGUAGES:
            self.language = language
            self._log_success(_("agent.language_set", language=language))
        else:
            self._log_warning(_("agent.language_not_supported", language=language, supported=SUPPORTED_LANGUAGES))

    def _get_system_prompt(self) -> str:
        """æ ¹æ® AppAgent çš„ç³»ç»Ÿæ¨¡æ¿æ„é€ ç³»ç»Ÿæç¤ºè¯­"""
        try:
            work_dir = os.getenv("CELLTYPE_WORK_DIR", str(Path.cwd()))

            print(f"ğŸ” è°ƒè¯•è¾“å‡º: AppAgenté…ç½®ä¿¡æ¯:")
            print(f"   - å·¥ä½œç›®å½•: {work_dir}")

            template = get_system_prompt_template(self.language)
            tool_list = "\n".join(
                [f"{i+1}. {t.get('name', 'Unknown')}: {t.get('description', '')}" for i, t in enumerate(self.available_tools)]
            )

            cache_status = "âœ“ MCP æœåŠ¡å™¨å·²å¯åŠ¨" if self.language == "zh" else "âœ“ MCP Server Started"

            try:
                files = [f for f in os.listdir(work_dir) if os.path.isfile(os.path.join(work_dir, f))]
                file_list = ", ".join(files)
            except Exception:
                file_list = "celltype_annotation_agent.py, prompts.py, mcp_server.py"

            return template.format(
                tool_list=tool_list,
                operating_system="Linux",
                cache_status=cache_status,
                file_list=file_list,
            )
        except Exception as e:
            self._log_warning(_("agent.template_fallback", error=e))
            fallback = get_fallback_prompt_template(self.language)
            tool_names = ", ".join([t.get("name", "") for t in self.available_tools])
            return fallback.format(tool_names=tool_names)

    async def _call_openai(self, messages: List[Dict], timeout: int = 270, stream: bool = False, request_type: str = "main") -> str:
        """è°ƒç”¨ OpenAI API - ä½¿ç”¨å…±äº« LLM å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰"""
        return await self.llm_client.call_api(
            messages=messages,
            timeout=timeout,
            stream=stream,
            request_type=request_type,
            llm_logger=self.llm_logger,
            console_logger=self.console_logger
        )

    async def _retry_llm_call_with_correction(
        self, messages: List[Dict], validation_result: Dict, max_retries: int = 10, timeout: int = 270
    ) -> str:
        retry_count = 0
        response = ""
        while retry_count < max_retries:
            retry_count += 1
            self._log_warning(_("retry.llm_retry", retry=retry_count, max_retries=max_retries))
            self._log_info(_("retry.last_response_issues", issues=", ".join(validation_result.get("issues", []))))

            correction_prompt = CelltypeValidationUtils.build_annotation_correction_prompt(
                validation_result, self.available_tools, self.language
            )
            correction_messages = messages.copy()
            correction_messages.append({"role": "user", "content": correction_prompt})
            response = await self._call_openai(correction_messages, timeout, stream=self.enable_streaming)

            new_validation = CelltypeValidationUtils.validate_response_format(response)
            if new_validation.get("valid"):
                self._log_success(_("retry.retry_success"))
                return response
            else:
                self._log_error(_("retry.retry_failed", count=retry_count, issues=", ".join(new_validation.get("issues", []))))
                validation_result = new_validation
        self._log_error(_("retry.max_retries_reached"))
        return response

    async def _summarize_context(self, messages: List[Dict]) -> List[Dict]:
        """å½“å¯¹è¯è¿‡é•¿æ—¶ï¼Œå¯¹å†å²è¿›è¡Œæ€»ç»“ï¼Œæ’å…¥ä¸€æ¡ [ä¸Šä¸‹æ–‡æ€»ç»“] æ¶ˆæ¯ä»¥å‹ç¼©ä¸Šä¸‹æ–‡"""
        try:
            self._log_warning(_("context.too_long"))
            system_message = messages[0] if messages and messages[0]["role"] == "system" else None
            initial_query = messages[1] if len(messages) > 1 and messages[1]["role"] == "user" else None

            existing_summary = None
            summary_index = -1
            for i, msg in enumerate(messages):
                if msg.get("role") == "assistant" and "[ä¸Šä¸‹æ–‡æ€»ç»“]" in msg.get("content", ""):
                    existing_summary = msg
                    summary_index = i
                    break

            if existing_summary and summary_index > 1:
                conversation_to_summarize = messages[summary_index + 1 : -4] if len(messages) > summary_index + 5 else messages[summary_index + 1 :]
                recent_messages = messages[-4:] if len(messages) > summary_index + 5 else []
            else:
                conversation_to_summarize = messages[2:-4] if len(messages) > 6 else messages[2:]
                recent_messages = messages[-4:] if len(messages) > 6 else []

            if not conversation_to_summarize:
                self._log_info(_("context.no_history"))
                return messages

            conversation_text = ""
            for msg in conversation_to_summarize:
                conversation_text += f"\n{msg['role'].upper()}: {msg['content']}\n"

            if existing_summary:
                existing_summary_content = existing_summary.get("content", "").replace("[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„æ³¨é‡Šè¿›å±•ï¼š", "").strip()
                summarization_prompt = (
                    "è¯·åŸºäºä¹‹å‰çš„æ€»ç»“ï¼Œç»§ç»­æ€»ç»“æ–°å¢çš„ç»†èƒç±»å‹æ³¨é‡Šå¯¹è¯å†å²ï¼š\n\n"
                    f"ä¹‹å‰çš„æ€»ç»“ï¼š\n{existing_summary_content}\n\næ–°å¢çš„å¯¹è¯å†å²ï¼š\n{conversation_text}\n\n"
                    "è¯·æ›´æ–°æ€»ç»“ï¼Œæ•´åˆæ–°å‘ç°çš„ä¿¡æ¯ï¼ŒåŒ…å«ï¼š\n"
                    "1. å·²æ‰§è¡Œçš„ä¸»è¦å·¥å…·è°ƒç”¨åŠå…¶ç»“æœ\n"
                    "2. å‘ç°çš„é‡è¦ä¿¡æ¯æˆ–çº¿ç´¢\n"
                    "3. å½“å‰è¿›å±•å’Œå¾…è§£å†³çš„é—®é¢˜\n"
                    "æ€»ç»“è¦ç®€æ˜æ‰¼è¦ï¼Œèšç„¦åç»­æ³¨é‡Šæµç¨‹æœ‰ç”¨çš„ä¿¡æ¯ã€‚"
                )
            else:
                summarization_prompt = (
                    "è¯·æ€»ç»“ä»¥ä¸‹ç»†èƒç±»å‹æ³¨é‡Šçš„å¯¹è¯å†å²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œé‡è¦å‘ç°ï¼š\n\n"
                    f"å¯¹è¯å†å²ï¼š\n{conversation_text}\n\n"
                    "è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼ŒåŒ…å«ï¼š\n"
                    "1. å·²æ‰§è¡Œçš„ä¸»è¦å·¥å…·è°ƒç”¨åŠå…¶ç»“æœ\n"
                    "2. å‘ç°çš„é‡è¦ä¿¡æ¯æˆ–çº¿ç´¢\n"
                    "3. å½“å‰è¿›å±•å’Œå¾…è§£å†³çš„é—®é¢˜\n"
                    "æ€»ç»“è¦ç®€æ˜æ‰¼è¦ï¼Œèšç„¦åç»­æ³¨é‡Šæµç¨‹æœ‰ç”¨çš„ä¿¡æ¯ã€‚"
                )

            summary_messages = [{"role": "user", "content": summarization_prompt}]
            summary = await self._call_openai(summary_messages, timeout=self.api_timeout, stream=self.enable_streaming)

            new_messages: List[Dict] = []
            if system_message:
                new_messages.append(system_message)
            if initial_query:
                new_messages.append(initial_query)
            new_messages.append({"role": "assistant", "content": f"[ä¸Šä¸‹æ–‡æ€»ç»“] ä¹‹å‰çš„æ³¨é‡Šè¿›å±•ï¼š{summary}"})
            new_messages.extend(recent_messages)

            self._log_success(_("context.summary_completed"))
            self._log_info(_("context.original_messages", count=len(messages)))
            self._log_info(_("context.summarized_messages", count=len(new_messages)))
            self._log_info(_("context.summary_length", length=len(summary)))
            return new_messages
        except Exception as e:
            self._log_error(f"âŒ ä¸Šä¸‹æ–‡æ€»ç»“å¤±è´¥: {e}")
            return messages[-10:] if len(messages) > 10 else messages

    def _build_user_query(
        self,
        rds_path: Optional[str],
        h5ad_path: Optional[str],
        tissue_description: Optional[str],
        marker_json_path: Optional[str],
        species: Optional[str],
        h5_path: Optional[str] = None,
        cluster_column: Optional[str] = None,
    ) -> str:
        """ä½¿ç”¨ç»Ÿä¸€æ¨¡æ¿æ„å»º <question> å†…å®¹ï¼Œæœªæä¾›å­—æ®µä»¥â€œæ— /Noneâ€å ä½"""
        file_paths = {
            'rds_file': rds_path,
            'h5ad_file': h5ad_path,
            'h5_file': h5_path,
            'marker_genes_json': marker_json_path,
        }
        return build_unified_user_query(
            file_paths=file_paths,
            tissue_description=tissue_description,
            species=species,
            language=self.language,
            cluster_column=cluster_column,
        )

    async def annotate(
        self,
        rds_path: Optional[str],
        h5ad_path: Optional[str],
        tissue_description: Optional[str] = None,
        marker_json_path: Optional[str] = None,
        species: Optional[str] = None,
        h5_path: Optional[str] = None,
        cluster_column: Optional[str] = None,
    ) -> Dict:
        """æ‰§è¡ŒåŸºäº prompts çš„ React æ³¨é‡Šæµç¨‹ï¼ˆè¿­ä»£è°ƒç”¨å·¥å…·ç›´åˆ° <final_answer>ï¼‰"""
        self._log_header(_("annotation.pipeline.starting", default="å¼€å§‹ç»†èƒç±»å‹æ³¨é‡Š React æµç¨‹"))

        # ğŸŒŸ å¢å¼ºç‰©ç§æ£€æµ‹é€»è¾‘
        final_species = species
        species_detection_result = None

        if not final_species:
            self.console_logger.info("ğŸ” æœªæä¾›ç‰©ç§å‚æ•°ï¼ŒAppAgentè‡ªåŠ¨æ£€æµ‹...")

            # ä¼˜å…ˆçº§1: ä» marker JSON æ£€æµ‹
            if marker_json_path:
                from agentype.appagent.tools.species_detection import detect_species_from_marker_json
                try:
                    detected, info = detect_species_from_marker_json(marker_json_path)
                    if info.get("confidence") in ["high", "medium"]:
                        final_species = detected
                        species_detection_result = info
                        self.console_logger.success(f"âœ… ä»JSONæ£€æµ‹åˆ°: {detected} (ç½®ä¿¡åº¦: {info.get('confidence')})")
                except Exception as e:
                    self.console_logger.warning(f"âš ï¸ JSONæ£€æµ‹å¤±è´¥: {e}")

            # ä¼˜å…ˆçº§2: ä» H5AD æ£€æµ‹
            if not final_species and h5ad_path:
                from agentype.appagent.tools.species_detection import detect_species_from_h5ad
                try:
                    detected, info = detect_species_from_h5ad(h5ad_path)
                    if info.get("confidence") in ["high", "medium"]:
                        final_species = detected
                        species_detection_result = info
                        self.console_logger.success(f"âœ… ä»H5ADæ£€æµ‹åˆ°: {detected} (ç½®ä¿¡åº¦: {info.get('confidence')})")
                except Exception as e:
                    self.console_logger.warning(f"âš ï¸ H5ADæ£€æµ‹å¤±è´¥: {e}")

            # ä¼˜å…ˆçº§3: ä» RDS æ£€æµ‹
            if not final_species and rds_path:
                from agentype.appagent.tools.species_detection import detect_species_from_rds
                try:
                    detected, info = detect_species_from_rds(rds_path)
                    if info.get("confidence") in ["high", "medium"]:
                        final_species = detected
                        species_detection_result = info
                        self.console_logger.success(f"âœ… ä»RDSæ£€æµ‹åˆ°: {detected} (ç½®ä¿¡åº¦: {info.get('confidence')})")
                except Exception as e:
                    self.console_logger.warning(f"âš ï¸ RDSæ£€æµ‹å¤±è´¥: {e}")

            # é™çº§åˆ°é»˜è®¤å€¼
            if not final_species:
                final_species = "Human"
                self.console_logger.warning("âš ï¸ æ‰€æœ‰æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç‰©ç§: Human")
        else:
            self.console_logger.info(f"âœ… ä½¿ç”¨ä¼ å…¥çš„ç‰©ç§å‚æ•°: {final_species}")

        system_prompt = self._get_system_prompt()
        user_query = self._build_user_query(
            rds_path,
            h5ad_path,
            tissue_description,
            marker_json_path,
            final_species,  # ğŸŒŸ ä½¿ç”¨æ£€æµ‹æˆ–ä¼ å…¥çš„ç‰©ç§
            h5_path,
            cluster_column,
        )

        messages: List[Dict] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"<question>{user_query}</question>"},
        ]

        self._log_info(_("analysis.system_prompt_length", length=len(system_prompt)))
        self._log_info(_("analysis.user_query", query=user_query) + "\n")

        analysis_log: List[Dict] = []
        iteration = 0
        final_answer = None

        while iteration < self.max_iterations:
            iteration += 1
            self._log_info(_("analysis.iteration", iter=iteration, max=self.max_iterations))

            # é•¿ä¸Šä¸‹æ–‡å¤„ç†
            total_chars = sum(len(m.get("content", "")) for m in messages)
            if total_chars > self.max_context_length:
                messages = await self._summarize_context(messages)

            response = await self._call_openai(messages, timeout=self.api_timeout, stream=self.enable_streaming)
            validation = CelltypeValidationUtils.validate_response_format(response)

            analysis_log.append({"iteration": iteration, "response": response, "validation": validation, "type": "ai_response"})

            if not validation.get("valid"):
                self._log_warning(_("validation.response_invalid"))
                response = await self._retry_llm_call_with_correction(messages, validation, timeout=self.api_timeout)
                analysis_log.append({"iteration": iteration, "response": response, "type": "ai_response_retry"})

            if "</final_answer>" in response:
                final_answer = CelltypeReactParser.extract_final_answer(response)
                if final_answer:
                    self._log_success(_("analysis.analysis_completed", celltype=final_answer[:60] + ("..." if len(final_answer) > 60 else "")))
                    break
                else:
                    self._log_warning(_("analysis.final_answer_no_celltype"))

            action = CelltypeReactParser.extract_action(response, self.available_tools)
            if action:
                # æ£€æŸ¥æ˜¯å¦æœ‰è§£æé”™è¯¯
                if 'error' in action:
                    error_msg = action.get('message', action.get('error'))
                    self._log_warning(f"Actionè§£æé”™è¯¯: {error_msg}")
                    # æ„é€ observationè®©LLMçŸ¥é“é—®é¢˜
                    error_observation = f"<observation>{{\"error\": \"{error_msg}\"}}</observation>"
                    messages.append({"role": "user", "content": error_observation})
                    continue

                function_name = action["function"]
                parameters_str = action["parameters"]
                parameters = CelltypeReactParser.parse_parameters(parameters_str)

                self._log_info(_("tools.call.calling", name=function_name, params=parameters))
                tool_result = await self.mcp_client.call_tool(function_name, parameters)

                if tool_result and tool_result.get("success"):
                    raw_content = tool_result.get("content", "")
                    self._log_success(_("tools.call.success", length=len(raw_content)))

                    # é’ˆå¯¹ä¸‰ç§æ ¸å¿ƒæ–¹æ³•è¿›è¡Œç»“æœå†…å®¹å¤„ç†ï¼Œå…¶ä»–å·¥å…·ç›´æ¥ä¼ é€’åŸå§‹å†…å®¹
                    method_map = {
                        "singleR_annotate_tool": "SingleR",
                        "sctype_annotate_tool": "scType",
                        "celltypist_annotate_tool": "CellTypist",
                    }
                    method = method_map.get(function_name)
                    result_content = (
                        self.content_processor.process_annotation_result(raw_content, method) if method else raw_content
                    )
                else:
                    result_content = json.dumps(
                        {
                            "error": tool_result.get("error", "å·¥å…·è°ƒç”¨å¤±è´¥") if tool_result else "æ— å“åº”",
                            "success": False,
                        },
                        ensure_ascii=False,
                    )
                    self._log_error(_("tools.call.failed"))

                analysis_log.append({"iteration": iteration, "action": action, "result": result_content, "type": "tool_call"})

                observation = f"<observation>{result_content}</observation>"
                messages.append({"role": "assistant", "content": response})
                messages.append({"role": "user", "content": observation})
            else:
                self._log_info(_("tools.call.no_action"))
                break

        # æ€»ç»“
        self._log_header(f"\n{_('analysis.summary.title')}")
        self._log_info(_("analysis.summary.total_iterations", iterations=iteration))
        self._log_info(_("analysis.summary.tool_calls", count=len([log for log in analysis_log if log.get('type') == 'tool_call'])))
        if iteration >= self.context_summary_threshold:
            self._log_info(_("analysis.summary.context_summary", threshold=self.context_summary_threshold))

        # âœ… ä»bundleè¯»å–è·¯å¾„ï¼ˆMCPå·¥å…·å·²è‡ªåŠ¨ä¿å­˜åˆ°bundleï¼‰
        bundle = load_file_paths_bundle()
        output_file_paths = {}

        if bundle.get("success"):
            # æå–AppAgentç›¸å…³çš„7ä¸ªæ–‡ä»¶è·¯å¾„å­—æ®µ
            path_keys = ['rds_file', 'h5ad_file', 'h5_file', 'marker_genes_json',
                         'singler_result', 'sctype_result', 'celltypist_result']
            for key in path_keys:
                value = bundle.get(key)
                if value:  # åªä¿ç•™éç©ºå€¼
                    output_file_paths[key] = value

            self._log_info(f"ğŸ“ ä»bundleè¯»å–åˆ° {len(output_file_paths)} ä¸ªæ–‡ä»¶è·¯å¾„")

            # æ‰“å°è¯»å–çš„è·¯å¾„
            if output_file_paths:
                self._log_header("\nğŸ“ ä»bundleè¯»å–çš„æ–‡ä»¶è·¯å¾„:")
                for file_type, path in output_file_paths.items():
                    self._log_info(f"   {file_type}: {path}")
        else:
            self._log_warning("âš ï¸ æœªèƒ½ä»bundleè¯»å–è·¯å¾„")

        return {
            "inputs": {
                "rds_path": rds_path,
                "h5ad_path": h5ad_path,
                "h5_path": h5_path,
                "tissue_description": tissue_description,
                "marker_json_path": marker_json_path,
                "species": species,
                "cluster_column": cluster_column,
            },
            "final_answer": final_answer,
            "output_file_paths": output_file_paths,
            "total_iterations": iteration,
            "context_summarized": iteration >= self.context_summary_threshold,
            "context_summary_threshold": self.context_summary_threshold,
            "analysis_log": analysis_log,
            "llm_log_summary": (self.llm_logger.get_log_summary() if self.llm_logger else None),
            "success": final_answer is not None,
            # ğŸŒŸ æ–°å¢ï¼šç‰©ç§ä¿¡æ¯
            "species": final_species,  # æœ€ç»ˆä½¿ç”¨çš„ç‰©ç§
            "species_detection_info": species_detection_result,  # æ£€æµ‹è¯¦æƒ…
        }

    # å…¼å®¹æ€§ä¸ä¾¿æ·æ–¹æ³•ï¼ˆéReactï¼‰ï¼š
    def analyze_tissue_type(self, tissue_description: Optional[str]) -> Dict:
        """åŸºäºç®€å•è§„åˆ™çš„ç»„ç»‡ç±»å‹åˆ†æä¸æ¨èï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
        result = CelltypeValidationUtils.validate_tissue_type(tissue_description or "")
        tissue = result.get("tissue_type") or ""
        # ç®€å•çš„æ¨èç­–ç•¥
        recommended = {
            "SingleR_reference_hint": "ImmGenData" if (tissue and any(k in tissue.lower() for k in ["å…ç–«", "immune"])) else "HumanPrimaryCellAtlasData",
            "scType_tissue_hint": "Immune system" if (tissue and any(k in tissue.lower() for k in ["å…ç–«", "immune"])) else "Blood",
            "CellTypist_model_hint": "Immune_All_High.pkl" if (tissue and any(k in tissue.lower() for k in ["å…ç–«", "immune"])) else None,
        }
        return {"input": tissue_description, "normalized_tissue": tissue, "recognized": result.get("has_recognized_keywords", False), "recommendation": recommended}

    def run_full_annotation_pipeline(
        self, rds_path: str, h5ad_path: str, tissue_description: Optional[str] = None
    ) -> Dict:
        """ç›´æ¥è°ƒç”¨ MCP æä¾›çš„ä¸€ä½“åŒ–æµæ°´çº¿å·¥å…·ï¼ˆéReactï¼‰ã€‚åŒæ­¥åŒ…è£…ä»¥ä¾¿ç®€æ˜“ä½¿ç”¨ã€‚"""
        async def _run():
            try:
                if not await self.initialize():
                    return {"success": False, "error": "Failed to start MCP server"}
                result = await self.mcp_client.celltype_annotation_pipeline(rds_path, h5ad_path, tissue_description)
                return result or {"success": False, "error": "No response"}
            finally:
                await self.cleanup()

        return asyncio.get_event_loop().run_until_complete(_run())

    async def cleanup(self):
        """é‡Šæ”¾èµ„æº"""
        try:
            await self.mcp_client.stop_server()

            # å…³é—­ LLM æ—¥å¿—è®°å½•å™¨
            if self.llm_logger:
                await self.llm_logger.close()

            self.available_tools = []
            await asyncio.sleep(0.3)
            gc.collect()
            await asyncio.sleep(0.1)
        except Exception as e:
            self._log_warning(f"âš ï¸ æ¸…ç†èµ„æºæ—¶å‡ºç°å¼‚å¸¸: {e}")
        self._log_success("âœ… èµ„æºæ¸…ç†å®Œæˆ")

    def _log_info(self, message: str) -> None:
        """è¾“å‡ºä¿¡æ¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.info(message)
        else:
            print(message)

    def _log_success(self, message: str) -> None:
        """è¾“å‡ºæˆåŠŸæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.success(message)
        else:
            print(message)

    def _log_error(self, message: str) -> None:
        """è¾“å‡ºé”™è¯¯æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.error(message)
        else:
            print(message)

    def _log_warning(self, message: str) -> None:
        """è¾“å‡ºè­¦å‘Šæ—¥å¿—"""
        if self.console_logger:
            self.console_logger.warning(message)
        else:
            print(message)

    def _log_header(self, message: str) -> None:
        """è¾“å‡ºæ ‡é¢˜æ—¥å¿—"""
        if self.console_logger:
            self.console_logger.header(message)
        else:
            print(message)

    def _log_separator(self, char: str = "=", length: int = 60) -> None:
        """è¾“å‡ºåˆ†éš”çº¿"""
        if self.console_logger:
            self.console_logger.separator(char, length)
        else:
            print(char * length)
