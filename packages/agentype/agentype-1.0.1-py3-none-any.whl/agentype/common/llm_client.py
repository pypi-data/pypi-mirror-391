#!/usr/bin/env python3
"""
agentype - å…±äº« LLM å®¢æˆ·ç«¯
Author: cuilei
Version: 1.0

æ”¯æŒ DeepSeek Reasoner çš„ reasoning_content ç‰¹æ€§
"""

import requests
import time
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime


class ObservationHallucinationError(Exception):
    """LLM åœ¨å“åº”ä¸­ç”Ÿæˆäº† <observation> æ ‡ç­¾ï¼ˆå¹»è§‰é”™è¯¯ï¼‰

    <observation> æ ‡ç­¾åº”è¯¥ç”± Agent åœ¨è°ƒç”¨å·¥å…·åæ·»åŠ ï¼Œ
    LLM ä¸åº”è¯¥è‡ªå·±ç”Ÿæˆæ­¤æ ‡ç­¾ã€‚å¦‚æœæ£€æµ‹åˆ°ï¼Œéœ€è¦ç«‹å³ä¸­æ–­å¹¶é‡è¯•ã€‚
    """
    pass


class LLMClient:
    """ç»Ÿä¸€çš„ LLM API å®¢æˆ·ç«¯

    ç‰¹æ€§ï¼š
    - æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
    - å®Œæ•´æ”¯æŒ DeepSeek Reasoner çš„ reasoning_content
    - ç»Ÿä¸€çš„æ—¥å¿—è®°å½•æ¥å£ï¼ˆé€šè¿‡å›è°ƒå‡½æ•°ï¼‰
    - ç»Ÿä¸€çš„ token ç»Ÿè®¡æ¥å£
    - ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
    """

    def __init__(self, config, logger_callbacks: Optional[Dict[str, Callable]] = None):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å« openai_api_key, openai_api_base, openai_model
            logger_callbacks: æ—¥å¿—å›è°ƒå‡½æ•°å­—å…¸ï¼Œå¯é€‰é”®å€¼ï¼š
                - 'info': ä¿¡æ¯æ—¥å¿—å›è°ƒ
                - 'success': æˆåŠŸæ—¥å¿—å›è°ƒ
                - 'warning': è­¦å‘Šæ—¥å¿—å›è°ƒ
                - 'error': é”™è¯¯æ—¥å¿—å›è°ƒ
        """
        self.config = config
        self.logger_callbacks = logger_callbacks or {}
        self._last_reasoning_length = 0  # è®°å½•æœ€åä¸€æ¬¡è¯·æ±‚çš„ reasoning_content é•¿åº¦

    def _normalize_api_url(self) -> str:
        """æ™ºèƒ½æ ‡å‡†åŒ– API URL

        æ”¯æŒå„ç§è¾“å…¥æ ¼å¼ï¼š
        1. è‡ªåŠ¨æ·»åŠ  https:// å‰ç¼€ï¼ˆå¦‚æœç¼ºå°‘ï¼‰ï¼Œæ”¯æŒ http:// å’Œ https://
        2. è‡ªåŠ¨æ·»åŠ  /v1 è·¯å¾„ï¼ˆå¦‚æœç¼ºå°‘ä¸”éœ€è¦ï¼‰
        3. è‡ªåŠ¨æ·»åŠ  /chat/completions åç¼€ï¼ˆå¦‚æœç¼ºå°‘ï¼‰

        ç¤ºä¾‹ï¼š
            - api.deepseek.com â†’ https://api.deepseek.com/v1/chat/completions
            - https://api.deepseek.com â†’ https://api.deepseek.com/v1/chat/completions
            - http://localhost:8000 â†’ http://localhost:8000/v1/chat/completions
            - api.deepseek.com/v1 â†’ https://api.deepseek.com/v1/chat/completions
            - https://api.openai.com/v1 â†’ https://api.openai.com/v1/chat/completions

        Returns:
            æ ‡å‡†åŒ–çš„å®Œæ•´ API URL
        """
        url = self.config.openai_api_base.strip()

        # 1. å¤„ç†åè®®ï¼ˆhttp/httpsï¼‰
        if not url.startswith(('http://', 'https://')):
            # æ²¡æœ‰åè®®ï¼Œé»˜è®¤æ·»åŠ  https://
            url = f"https://{url}"

        # 2. ç§»é™¤æœ«å°¾çš„æ–œæ 
        url = url.rstrip('/')

        # 3. å¤„ç† /chat/completions åç¼€
        if url.endswith('/chat/completions'):
            # å·²ç»æœ‰å®Œæ•´è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ  /v1
            if '/v1/chat/completions' in url:
                # å·²åŒ…å« /v1ï¼Œç›´æ¥è¿”å›
                return url
            else:
                # ç¼ºå°‘ /v1ï¼Œæ’å…¥åˆ° /chat/completions ä¹‹å‰
                # ä¾‹å¦‚ï¼šhttps://api.deepseek.com/chat/completions
                #   â†’ https://api.deepseek.com/v1/chat/completions
                url = url.replace('/chat/completions', '/v1/chat/completions')
                return url

        # 4. å¤„ç† /v1 è·¯å¾„
        if not url.endswith('/v1'):
            # æ²¡æœ‰ /v1ï¼Œæ·»åŠ å®ƒ
            url = f"{url}/v1"

        # 5. æ·»åŠ  /chat/completions
        url = f"{url}/chat/completions"

        return url

    def has_reasoning(self) -> bool:
        """æ£€æŸ¥æœ€åä¸€æ¬¡è¯·æ±‚æ˜¯å¦åŒ…å« reasoning_content"""
        return self._last_reasoning_length > 0

    def _log(self, level: str, message: str):
        """å†…éƒ¨æ—¥å¿—æ–¹æ³•"""
        callback = self.logger_callbacks.get(level)
        if callback:
            callback(message)

    def _log_info(self, message: str):
        self._log('info', message)

    def _log_success(self, message: str):
        self._log('success', message)

    def _log_warning(self, message: str):
        self._log('warning', message)

    def _log_error(self, message: str):
        self._log('error', message)

    async def call_api(
        self,
        messages: List[Dict],
        timeout: int = 270,
        stream: bool = False,
        enable_thinking: bool = False,
        request_type: str = "main",
        llm_logger = None,
        console_logger = None
    ) -> str:
        """è°ƒç”¨ OpenAI APIï¼ˆæ”¯æŒ DeepSeek Reasonerï¼‰

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒè¾“å‡º
            request_type: è¯·æ±‚ç±»å‹ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            llm_logger: LLMæ—¥å¿—è®°å½•å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            console_logger: æ§åˆ¶å°æ—¥å¿—è®°å½•å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰

        Returns:
            LLM å“åº”å†…å®¹
        """
        start_time = datetime.now()
        data = {}
        payload = {}

        try:
            headers = {
                "Authorization": f"Bearer {self.config.openai_api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": self.config.openai_model,
                "messages": messages,
                "temperature": 0.1,
                "stream": stream,
                "enable_thinking": enable_thinking
            }

            # æ—¥å¿—ï¼šè¯·æ±‚ä¿¡æ¯
            self._log_info("ğŸ“¤ å‘é€ LLM è¯·æ±‚...")
            self._log_info(f"   ğŸŒ URL: {self._normalize_api_url()}")
            self._log_info(f"   ğŸ¤– æ¨¡å‹: {payload['model']}")
            self._log_info(f"   ğŸ’¬ æ¶ˆæ¯æ•°: {len(messages)}")
            self._log_info(f"   â±ï¸  è¶…æ—¶: {timeout}ç§’")
            streaming_text = "âœ… å·²å¯ç”¨" if stream else "âŒ å·²ç¦ç”¨"
            self._log_info(f"   ğŸŒŠ æµå¼è¾“å‡º: {streaming_text}")

            # æ·»åŠ è¶…æ—¶é‡è¯•é€»è¾‘ï¼ˆæœ€å¤š3æ¬¡ï¼‰
            max_retries = 30
            last_timeout_error = None

            for retry_attempt in range(max_retries):
                try:
                    if retry_attempt > 0:
                        self._log_warning(f"âš ï¸ ç¬¬ {retry_attempt + 1}/{max_retries} æ¬¡é‡è¯• LLM è°ƒç”¨...")

                    response = requests.post(
                        self._normalize_api_url(),
                        headers=headers,
                        json=payload,
                        timeout=timeout,
                        stream=stream
                    )
                    response.raise_for_status()
                    break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯

                except (requests.exceptions.Timeout, requests.exceptions.ReadTimeout, requests.exceptions.HTTPError) as error:
                    # å¤„ç† 429 é€Ÿç‡é™åˆ¶é”™è¯¯
                    if isinstance(error, requests.exceptions.HTTPError) and error.response.status_code == 429:
                        if retry_attempt < max_retries - 1:
                            # ä»å“åº”å¤´è¯»å– Retry-Afterï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ 60 ç§’
                            retry_after = error.response.headers.get('Retry-After', '60')
                            try:
                                wait_time = int(retry_after)
                            except ValueError:
                                wait_time = 60

                            self._log_warning(f"â±ï¸ é‡åˆ°é€Ÿç‡é™åˆ¶ (429)ï¼Œæš‚åœ {wait_time} ç§’åé‡è¯•...")
                            time.sleep(wait_time)
                            continue
                        else:
                            self._log_error(f"âŒ é‡åˆ°é€Ÿç‡é™åˆ¶ (429)ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                            raise error

                    # å…¶ä»– HTTP é”™è¯¯ç›´æ¥æŠ›å‡º
                    if isinstance(error, requests.exceptions.HTTPError):
                        raise error

                    # å¤„ç†è¶…æ—¶é”™è¯¯ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    last_timeout_error = error
                    if retry_attempt < max_retries - 1:
                        self._log_warning(f"â±ï¸ LLM è°ƒç”¨è¶…æ—¶ ({timeout}ç§’)ï¼Œå°†è¿›è¡Œé‡è¯•...")
                        continue
                    else:
                        self._log_error(f"âŒ LLM è°ƒç”¨è¶…æ—¶ ({timeout}ç§’)ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                        raise last_timeout_error

            content = ""
            reasoning_content = ""  # ğŸŒŸ DeepSeek Reasoner: ç´¯ç§¯æ¨ç†å†…å®¹

            if stream:
                # æµå¼è¾“å‡ºå¤„ç†
                self._log_info("ğŸ“¥ æ¥æ”¶æµå¼å“åº”...")
                reasoning_char_count = 0  # ğŸŒŸ DeepSeek Reasoner: æ¨ç†å†…å®¹å­—ç¬¦æ•°
                content_char_count = 0
                first_reasoning_chunk = True  # ğŸŒŸ æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªæ¨ç†chunk

                try:
                    for line in response.iter_lines():
                        if line:
                            line_str = line.decode('utf-8')
                            if line_str.startswith('data: '):
                                data_str = line_str[6:]
                                if data_str.strip() == '[DONE]':
                                    break

                                try:
                                    import json
                                    chunk_data = json.loads(data_str)

                                    if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                        delta = chunk_data['choices'][0].get('delta', {})

                                        # ğŸŒŸ DeepSeek Reasoner: å¤„ç† reasoning_content
                                        if 'reasoning_content' in delta:
                                            reasoning_chunk = delta['reasoning_content']
                                            if reasoning_chunk is not None:
                                                reasoning_content += reasoning_chunk
                                                reasoning_char_count += len(reasoning_chunk)
                                                # å®æ—¶æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ï¼ˆç°è‰²æ˜¾ç¤ºï¼ŒåŒºåˆ«äºæœ€ç»ˆç­”æ¡ˆï¼‰
                                                print(f"\033[90m{reasoning_chunk}\033[0m", end='', flush=True)
                                                # å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆä»¿ç…§ content çš„å¤„ç†æ–¹å¼ï¼‰
                                                if console_logger and console_logger.file_output:
                                                    with open(console_logger.log_file, 'a', encoding='utf-8') as f:
                                                        # ç¬¬ä¸€ä¸ªchunkï¼šå†™å…¥å¼€å§‹æ ‡è®°
                                                        if first_reasoning_chunk:
                                                            f.write("[æ¨ç†å¼€å§‹]\n")
                                                            first_reasoning_chunk = False
                                                        # ç›´æ¥å†™å…¥åŸå§‹å†…å®¹ï¼ˆæ— æ ‡ç­¾ï¼‰
                                                        f.write(reasoning_chunk)
                                                        f.flush()

                                        # å¤„ç†æ­£å¸¸çš„ content
                                        if 'content' in delta:
                                            chunk = delta['content']
                                            if chunk is not None:
                                                content += chunk
                                                content_char_count += len(chunk)

                                                # ğŸš¨ æ£€æµ‹ LLM æ˜¯å¦é”™è¯¯ç”Ÿæˆäº† <observation> æ ‡ç­¾
                                                if '<observation>' in content.lower():
                                                    self._log_error("âŒ æ£€æµ‹åˆ° LLM é”™è¯¯ç”Ÿæˆ <observation> æ ‡ç­¾ï¼ˆå¹»è§‰ï¼‰ï¼Œç«‹å³ä¸­æ–­")
                                                    # åœæ­¢æ¥æ”¶æµå¼æ•°æ®
                                                    raise ObservationHallucinationError(
                                                        "LLM åœ¨å“åº”ä¸­ç”Ÿæˆäº† <observation> æ ‡ç­¾ï¼Œè¿™æ˜¯é”™è¯¯çš„è¡Œä¸ºã€‚"
                                                        "<observation> æ ‡ç­¾åº”è¯¥ç”± Agent åœ¨è°ƒç”¨å·¥å…·åæ·»åŠ ã€‚"
                                                    )

                                                print(chunk, end='', flush=True)
                                                if console_logger and console_logger.file_output:
                                                    with open(console_logger.log_file, 'a', encoding='utf-8') as f:
                                                        f.write(chunk)
                                                        f.flush()

                                    # ğŸŒŸ DeepSeek Reasoner: è®°å½• usageï¼ˆåŒ…å« reasoning_tokensï¼‰
                                    if 'usage' in chunk_data:
                                        data['usage'] = chunk_data['usage']
                                        data['model'] = chunk_data.get('model', payload.get('model'))

                                except json.JSONDecodeError as json_err:
                                    self._log_warning(f"âš ï¸ JSON è§£æå¤±è´¥: {json_err}, è¡Œå†…å®¹: {data_str[:100]}")
                                    continue

                    print()  # æ¢è¡Œ

                    # ğŸŒŸ DeepSeek Reasoner: å†™å…¥æ¨ç†ç»“æŸæ ‡è®°
                    if reasoning_char_count > 0 and console_logger and console_logger.file_output:
                        with open(console_logger.log_file, 'a', encoding='utf-8') as f:
                            f.write(f"\n[æ¨ç†ç»“æŸ ({reasoning_char_count}å­—ç¬¦)]\n")
                            f.flush()

                    # ğŸŒŸ DeepSeek Reasoner: æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    if reasoning_content:
                        self._log_success(f"âœ… æ¨ç†å†…å®¹å·²æ¥æ”¶ ({reasoning_char_count} å­—ç¬¦)")
                    self._log_success(f"âœ… æµå¼å“åº”å®Œæˆ ({content_char_count} å­—ç¬¦)")

                except Exception as stream_error:
                    self._log_error(f"\nâŒ æµå¼è¾“å‡ºå¤±è´¥: {stream_error}")
                    self._log_warning("âš ï¸ å°è¯•å›é€€åˆ°éæµå¼æ¨¡å¼...")
                    if not content.strip():
                        self._log_info("ğŸ”„ ä½¿ç”¨éæµå¼æ¨¡å¼é‡è¯•...")
                        payload["stream"] = False
                        response = requests.post(
                            self._normalize_api_url(),
                            headers=headers,
                            json=payload,
                            timeout=timeout,
                        )
                        response.raise_for_status()
                        data = response.json()
                        message = data["choices"][0]["message"]
                        reasoning_content = message.get("reasoning_content", "")  # ğŸŒŸ DeepSeek Reasoner
                        content = message.get("content", "")
                        self._log_success(f"âœ… éæµå¼å“åº”å®Œæˆ ({len(content)} å­—ç¬¦)")
            else:
                # éæµå¼è¾“å‡ºå¤„ç†
                data = response.json()
                message = data["choices"][0]["message"]
                reasoning_content = message.get("reasoning_content", "")  # ğŸŒŸ DeepSeek Reasoner
                content = message.get("content", "")

                # ğŸŒŸ DeepSeek Reasoner: æ˜¾ç¤ºæ¨ç†å†…å®¹
                if reasoning_content:
                    self._log_info(f"ğŸ§  æ¨ç†è¿‡ç¨‹ ({len(reasoning_content)} å­—ç¬¦):")
                    self._log_info(f"   {reasoning_content[:200]}...")

            # æ—¥å¿—ï¼šå“åº”ä¿¡æ¯
            self._log_info("ğŸ“¥ LLM å“åº”")
            self._log_info(f"   ğŸ“ å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")

            # ğŸŒŸ LLM æ—¥å¿—è®°å½•ï¼ˆåŒ…å« reasoning_contentï¼‰
            if llm_logger:
                request_data = {
                    "url": self._normalize_api_url(),
                    "headers": {k: (v if k != "Authorization" else "Bearer [REDACTED]") for k, v in headers.items()},
                    "payload": payload,
                }
                extra_info = {
                    "duration_seconds": (datetime.now() - start_time).total_seconds(),
                    "usage": data.get("usage", {}),
                    "model_used": data.get("model", payload.get("model")),
                    "reasoning_content": reasoning_content,  # ğŸŒŸ è®°å½•æ¨ç†å†…å®¹
                    "reasoning_length": len(reasoning_content)  # ğŸŒŸ æ¨ç†å†…å®¹é•¿åº¦
                }

                # è®°å½•ä¸ºé€šç”¨ chat completion (usage æ•°æ®ä¼šä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶)
                llm_logger.log_request_response(
                    request_type=request_type,
                    request_data=request_data,
                    response_data=content,
                    success=True,
                    extra_info=extra_info,
                )

            # ğŸŒŸ è®°å½• reasoning_content é•¿åº¦ä¾›éªŒè¯ä½¿ç”¨
            self._last_reasoning_length = len(reasoning_content)

            return content

        except Exception as e:
            error_msg = f"OpenAI API è°ƒç”¨å¤±è´¥: {str(e)}"
            self._log_error(f"âŒ {error_msg}")

            # é”™è¯¯æ—¥å¿—è®°å½•
            if llm_logger:
                request_data = {
                    "url": self._normalize_api_url(),
                    "headers": {"Authorization": "Bearer [REDACTED]", "Content-Type": "application/json"},
                    "payload": payload,
                }
                llm_logger.log_request_response(
                    request_type=request_type,
                    request_data=request_data,
                    response_data="",
                    success=False,
                    error_msg=error_msg,
                    extra_info={
                        "duration_seconds": (datetime.now() - start_time).total_seconds()
                    },
                )

            return error_msg
