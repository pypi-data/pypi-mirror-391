"""
FLAMLTrainer - FLAML è®­ç»ƒå™¨

å°è£… FLAML è®­ç»ƒé€»è¾‘ï¼Œæ”¯æŒ Databricks é£æ ¼çš„å‚æ•°
"""
from typing import Any, Dict, List, Optional, Union
import logging
import time
import pandas as pd
import numpy as np
import mlflow
from sklearn.pipeline import Pipeline as SkPipe

# Robust import for FLAML
try:
    from flaml import AutoML
    import flaml as flaml_pkg
except ImportError:
    try:
        from flaml.automl.automl import AutoML
        import flaml as flaml_pkg
    except ImportError as e:
        raise ImportError(
            "Cannot import AutoML from flaml. "
            "Please install flaml with AutoML support: pip install 'flaml[automl]==2.3.6'"
        ) from e

from wedata_automl.summary import AutoMLSummary
from wedata_automl.utils.sk_pipeline import build_numeric_preprocessor
from wedata_automl.utils.spark_utils import compute_split_and_weights
from wedata_automl.utils.print_utils import safe_print, print_separator, print_header

logger = logging.getLogger(__name__)


# ============================================================================
# MLflow Artifact æ—¥å¿—è®°å½•è¾…åŠ©å‡½æ•°
# ============================================================================

def log_feature_list(features: List[str]):
    """è®°å½•ç‰¹å¾åˆ—è¡¨åˆ° MLflow"""
    import json
    mlflow.log_dict({"features": features}, "feature_list.json")


def log_best_config_overall(config: Dict[str, Any]):
    """è®°å½•æœ€ä½³é…ç½®åˆ° MLflow"""
    import json
    mlflow.log_dict(config, "best_config_overall.json")


def log_best_config_per_estimator(config: Dict[str, Any]):
    """è®°å½•æ¯ä¸ªä¼°è®¡å™¨çš„æœ€ä½³é…ç½®åˆ° MLflow"""
    import json
    mlflow.log_dict(config, "best_config_per_estimator.json")


def log_engine_meta(meta: Dict[str, Any]):
    """è®°å½•å¼•æ“å…ƒæ•°æ®åˆ° MLflow"""
    import json
    mlflow.log_dict(meta, "engine_meta.json")


class TrialLogger:
    """
    FLAML Trial æ—¥å¿—è®°å½•å™¨

    ç”¨äºè®°å½•æ¯ä¸ª trial çš„è¯¦ç»†ä¿¡æ¯åˆ° MLflow
    """

    def __init__(self, parent_run_id: str, features: List[str], task: str, metric: str):
        """
        åˆå§‹åŒ– Trial Logger

        Args:
            parent_run_id: çˆ¶ run çš„ ID
            features: ç‰¹å¾åˆ—è¡¨
            task: ä»»åŠ¡ç±»å‹
            metric: è¯„ä¼°æŒ‡æ ‡
        """
        self.parent_run_id = parent_run_id
        self.features = features
        self.task = task
        self.metric = metric
        self.trial_count = 0
        self.trial_runs = []  # å­˜å‚¨æ‰€æœ‰ trial çš„ä¿¡æ¯

    def log_trial(self, config: Dict[str, Any], estimator: str, val_loss: float, train_time: float):
        """
        è®°å½•å•ä¸ª trial åˆ° MLflow

        Args:
            config: è¶…å‚æ•°é…ç½®
            estimator: ä¼°è®¡å™¨åç§°
            val_loss: éªŒè¯é›†æŸå¤±
            train_time: è®­ç»ƒæ—¶é—´
        """
        self.trial_count += 1

        # åˆ›å»ºåµŒå¥— run
        with mlflow.start_run(run_name=f"trial_{self.trial_count}_{estimator}", nested=True) as trial_run:
            trial_run_id = trial_run.info.run_id

            # è®°å½•å‚æ•°
            mlflow.log_param("estimator", estimator)
            mlflow.log_param("trial_number", self.trial_count)
            mlflow.log_param("parent_run_id", self.parent_run_id)

            # è®°å½•è¶…å‚æ•°
            for key, value in config.items():
                try:
                    mlflow.log_param(f"hp_{key}", value)
                except Exception:
                    # æŸäº›å€¼å¯èƒ½æ— æ³•åºåˆ—åŒ–
                    mlflow.log_param(f"hp_{key}", str(value))

            # è®°å½•æŒ‡æ ‡
            mlflow.log_metric("val_loss", val_loss)
            mlflow.log_metric("train_time", train_time)

            # å¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œval_loss æ˜¯è´Ÿçš„å‡†ç¡®ç‡ï¼Œè½¬æ¢å›æ¥
            if self.task == "classification" and self.metric in ["accuracy", "roc_auc", "f1"]:
                val_metric = -val_loss  # FLAML ä½¿ç”¨è´Ÿå€¼è¡¨ç¤ºæŸå¤±
                mlflow.log_metric(f"val_{self.metric}", val_metric)

            # è®°å½•ç‰¹å¾åˆ—è¡¨
            log_feature_list(self.features)

            # å­˜å‚¨ trial ä¿¡æ¯
            trial_info = {
                "run_id": trial_run_id,
                "trial_number": self.trial_count,
                "estimator": estimator,
                "val_loss": val_loss,
                "train_time": train_time,
                "config": config,
            }
            self.trial_runs.append(trial_info)

            safe_print(f"  Trial {self.trial_count:3d} | {estimator:15s} | val_loss={val_loss:.6f} | time={train_time:.2f}s")

    def get_best_trial(self) -> Dict[str, Any]:
        """
        è·å–æœ€ä½³ trial

        Returns:
            æœ€ä½³ trial çš„ä¿¡æ¯å­—å…¸
        """
        if not self.trial_runs:
            return None

        # æŒ‰ val_loss æ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        best_trial = min(self.trial_runs, key=lambda x: x["val_loss"])
        return best_trial


class FLAMLTrainer:
    """
    FLAML è®­ç»ƒå™¨
    
    å°è£… FLAML è®­ç»ƒé€»è¾‘ï¼Œæ”¯æŒ Databricks é£æ ¼çš„å‚æ•°
    """
    
    def __init__(
        self,
        task: str,
        target_col: str,
        timeout_minutes: int = 5,
        max_trials: Optional[int] = None,
        metric: str = "auto",
        exclude_cols: Optional[List[str]] = None,
        exclude_frameworks: Optional[List[str]] = None,
        sample_weight_col: Optional[str] = None,
        pos_label: Optional[Union[str, int]] = None,
        data_split_col: Optional[str] = None,
        experiment_name: Optional[str] = None,
        experiment_id: Optional[str] = None,
        run_name: Optional[str] = None,
        register_model: bool = True,
        model_name: Optional[str] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ– FLAML è®­ç»ƒå™¨
        
        Args:
            task: ä»»åŠ¡ç±»å‹ ("classification" æˆ– "regression")
            target_col: ç›®æ ‡åˆ—å
            timeout_minutes: è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            max_trials: æœ€å¤§è¯•éªŒæ¬¡æ•°
            metric: è¯„ä¼°æŒ‡æ ‡
            exclude_cols: æ’é™¤çš„åˆ—
            exclude_frameworks: æ’é™¤çš„æ¡†æ¶
            sample_weight_col: æ ·æœ¬æƒé‡åˆ—
            pos_label: æ­£ç±»æ ‡ç­¾ï¼ˆäºŒåˆ†ç±»ï¼‰
            data_split_col: æ•°æ®åˆ’åˆ†åˆ—
            experiment_name: MLflow å®éªŒåç§°
            experiment_id: MLflow å®éªŒ ID
            run_name: MLflow run åç§°
            register_model: æ˜¯å¦æ³¨å†Œæ¨¡å‹
            model_name: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°
        """
        self.task = task
        self.target_col = target_col
        self.timeout_minutes = timeout_minutes
        self.max_trials = max_trials
        self.metric = metric if metric != "auto" else self._get_default_metric(task)
        self.exclude_cols = exclude_cols or []
        self.exclude_frameworks = exclude_frameworks or []
        self.sample_weight_col = sample_weight_col
        self.pos_label = pos_label
        self.data_split_col = data_split_col
        self.experiment_name = experiment_name or "wedata_automl"
        self.experiment_id = experiment_id
        self.run_name = run_name or f"flaml_automl_{task}"
        self.register_model = register_model
        self.model_name = model_name
        self.kwargs = kwargs
        
        # å†…éƒ¨çŠ¶æ€
        self.automl = None
        self.pipeline = None
        self.features = None
        self.preprocessor = None
    
    def _get_default_metric(self, task: str) -> str:
        """è·å–é»˜è®¤æŒ‡æ ‡"""
        if task == "classification":
            return "log_loss"
        elif task == "regression":
            return "deviance"
        elif task == "":
            return
        else:
            return "accuracy"
    
    def _get_estimator_list(self) -> List[str]:
        """è·å–ä¼°è®¡å™¨åˆ—è¡¨"""
        all_estimators = ["lgbm", "xgboost", "rf", "extra_tree"]
        if self.task == "classification":
            all_estimators.append("lrl1")

        # æ’é™¤æŒ‡å®šçš„æ¡†æ¶
        estimators = [e for e in all_estimators if e not in self.exclude_frameworks]
        return estimators

    def _evaluate_model(
        self,
        X_train: pd.DataFrame,
        y_train: np.ndarray,
        X_val: pd.DataFrame,
        y_val: np.ndarray,
        X_test: pd.DataFrame,
        y_test: np.ndarray
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        metrics = {}

        if self.task == "classification":
            from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

            for name, X, y_true in [
                ("train", X_train, y_train),
                ("val", X_val, y_val),
                ("test", X_test, y_test),
            ]:
                pred = self.pipeline.predict(X)

                acc = float(accuracy_score(y_true, pred))
                f1 = float(f1_score(y_true, pred, average='weighted', zero_division=0))
                precision = float(precision_score(y_true, pred, average='weighted', zero_division=0))
                recall = float(recall_score(y_true, pred, average='weighted', zero_division=0))

                metrics[f"{name}_accuracy"] = acc
                metrics[f"{name}_f1"] = f1
                metrics[f"{name}_precision"] = precision
                metrics[f"{name}_recall"] = recall

                mlflow.log_metric(f"{name}_accuracy", acc)
                mlflow.log_metric(f"{name}_f1", f1)
                mlflow.log_metric(f"{name}_precision", precision)
                mlflow.log_metric(f"{name}_recall", recall)

                safe_print(f"{name.capitalize():5s} Set - Accuracy: {acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}")

        elif self.task == "regression":
            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

            for name, X, y_true in [
                ("train", X_train, y_train),
                ("val", X_val, y_val),
                ("test", X_test, y_test),
            ]:
                pred = self.pipeline.predict(X)

                r2 = float(r2_score(y_true, pred))
                mse = float(mean_squared_error(y_true, pred))
                mae = float(mean_absolute_error(y_true, pred))
                rmse = float(np.sqrt(mse))

                metrics[f"{name}_r2"] = r2
                metrics[f"{name}_mse"] = mse
                metrics[f"{name}_mae"] = mae
                metrics[f"{name}_rmse"] = rmse

                mlflow.log_metric(f"{name}_r2", r2)
                mlflow.log_metric(f"{name}_mse", mse)
                mlflow.log_metric(f"{name}_mae", mae)
                mlflow.log_metric(f"{name}_rmse", rmse)

                safe_print(f"{name.capitalize():5s} Set - RÂ²: {r2:.4f} | MSE: {mse:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}")

        return metrics
    
    def _prepare_data(
        self,
        pdf: pd.DataFrame
    ) -> tuple:
        """
        å‡†å¤‡æ•°æ®
        
        Returns:
            (X_train, y_train, X_val, y_val, X_test, y_test, features)
        """
        # ç¡®å®šç‰¹å¾åˆ—
        disable_cols = set(self.exclude_cols) | {self.target_col}
        if self.sample_weight_col:
            disable_cols.add(self.sample_weight_col)
        if self.data_split_col:
            disable_cols.add(self.data_split_col)
        
        self.features = [c for c in pdf.columns if c not in disable_cols]

        safe_print(f"Target column: '{self.target_col}'")
        safe_print(f"Feature columns: {len(self.features)} columns")
        if len(self.features) <= 20:
            safe_print(f"  Features: {', '.join(self.features)}")
        else:
            safe_print(f"  First 10 features: {', '.join(self.features[:10])}")
            safe_print(f"  ... and {len(self.features) - 10} more")
        
        # æ•°æ®åˆ’åˆ†
        safe_print("", show_timestamp=False, show_level=False)
        if self.data_split_col and self.data_split_col in pdf.columns:
            # ä½¿ç”¨ç”¨æˆ·æä¾›çš„åˆ’åˆ†åˆ—
            pdf["_automl_split_col"] = pdf[self.data_split_col]
            safe_print(f"âœ… Using user-provided split column: '{self.data_split_col}'")
        else:
            # è‡ªåŠ¨åˆ’åˆ†
            safe_print(f"Auto-generating train/val/test split (60%/20%/20%)")
            if self.task == "classification":
                safe_print(f"  Using stratified split for classification")
            split_col, sample_weights = compute_split_and_weights(
                y=pdf[self.target_col].values,
                task=self.task,
                train_ratio=0.6,
                val_ratio=0.2,
                test_ratio=0.2,
                stratify=True if self.task == "classification" else False,
                random_state=42,
            )
            pdf["_automl_split_col"] = split_col.values
            pdf["_automl_sample_weight"] = sample_weights.values
            safe_print("âœ… Split generated successfully")
        
        # åˆ†å‰²æ•°æ®
        train_df = pdf[pdf["_automl_split_col"] == 0]
        val_df = pdf[pdf["_automl_split_col"] == 1]
        test_df = pdf[pdf["_automl_split_col"] == 2]
        
        X_train = train_df[self.features]
        y_train = train_df[self.target_col].values
        
        X_val = val_df[self.features]
        y_val = val_df[self.target_col].values
        
        X_test = test_df[self.features]
        y_test = test_df[self.target_col].values

        safe_print("", show_timestamp=False, show_level=False)
        safe_print(f"Data split summary:")
        safe_print(f"  Train: {len(train_df):,} samples ({len(train_df)/len(pdf)*100:.1f}%)")
        safe_print(f"  Val:   {len(val_df):,} samples ({len(val_df)/len(pdf)*100:.1f}%)")
        safe_print(f"  Test:  {len(test_df):,} samples ({len(test_df)/len(pdf)*100:.1f}%)")
        safe_print(f"  Total: {len(pdf):,} samples")

        # æ˜¾ç¤ºç›®æ ‡å˜é‡åˆ†å¸ƒï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
        if self.task == "classification":
            safe_print("", show_timestamp=False, show_level=False)
            safe_print(f"Target distribution in training set:")
            train_dist = pd.Series(y_train).value_counts().sort_index()
            for label, count in train_dist.items():
                safe_print(f"  Class {label}: {count:,} samples ({count/len(y_train)*100:.1f}%)")

        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def train(
        self,
        dataset: Union[pd.DataFrame, Any],
        spark=None
    ) -> AutoMLSummary:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            dataset: æ•°æ®é›†ï¼ˆPandas DataFrame æˆ– Spark DataFrameï¼‰
            spark: Spark sessionï¼ˆå¦‚æœ dataset æ˜¯è¡¨åï¼‰
            
        Returns:
            AutoMLSummary å¯¹è±¡
        """
        # è½¬æ¢ä¸º Pandas DataFrame
        if isinstance(dataset, str):
            # è¡¨å
            if spark is None:
                raise ValueError("Spark session is required when dataset is a table name")
            pdf = spark.read.table(dataset).toPandas()
        elif hasattr(dataset, "toPandas"):
            # Spark DataFrame
            pdf = dataset.toPandas()
        else:
            # Pandas DataFrame
            pdf = dataset
        
        print_separator()
        safe_print("ğŸ“Š Data Loading", show_timestamp=False, show_level=False)
        print_separator()
        safe_print(f"Dataset shape: {pdf.shape} (rows Ã— columns)")
        safe_print(f"Memory usage: {pdf.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

        # å‡†å¤‡æ•°æ®
        safe_print("", show_timestamp=False, show_level=False)
        print_separator()
        safe_print("ğŸ”§ Data Preparation", show_timestamp=False, show_level=False)
        print_separator()
        X_train, y_train, X_val, y_val, X_test, y_test = self._prepare_data(pdf)
        
        # æ„å»ºé¢„å¤„ç†å™¨
        safe_print("", show_timestamp=False, show_level=False)
        print_separator()
        safe_print(f"âš™ï¸  Feature Preprocessing")
        print_separator()
        self.preprocessor = build_numeric_preprocessor(self.features)
        X_train_num = self.preprocessor.fit_transform(X_train)
        X_val_num = self.preprocessor.transform(X_val)
        X_test_num = self.preprocessor.transform(X_test)

        safe_print(f"Preprocessor fitted successfully")
        safe_print(f"  - Train set: {X_train_num.shape}")
        safe_print(f"  - Val set:   {X_val_num.shape}")
        safe_print(f"  - Test set:  {X_test_num.shape}")
        
        # è·å–æˆ–åˆ›å»º MLflow å®éªŒ
        safe_print("", show_timestamp=False, show_level=False)
        print_separator()
        safe_print(f"ğŸ“ MLflow Experiment Setup")
        print_separator()
        if self.experiment_id:
            experiment = mlflow.get_experiment(self.experiment_id)
            experiment_name = experiment.name
            safe_print(f"Using experiment by ID: {self.experiment_id}")
        else:
            experiment_name = self.experiment_name
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = mlflow.create_experiment(experiment_name)
                safe_print(f"âœ… Created new experiment: '{experiment_name}' (ID: {experiment_id})")
                # é‡æ–°è·å– experiment å¯¹è±¡
                experiment = mlflow.get_experiment(experiment_id)
            else:
                experiment_id = experiment.experiment_id
                safe_print(f"âœ… Using existing experiment: '{experiment_name}' (ID: {experiment_id})")

        mlflow.set_experiment(experiment_name)

        # å¼€å§‹ MLflow run
        with mlflow.start_run(run_name=self.run_name) as parent_run:
            parent_run_id = parent_run.info.run_id
            safe_print(f"Run name: '{self.run_name}'")
            safe_print(f"Run ID: {parent_run_id}")
            
            # è®°å½•å‚æ•°
            mlflow.log_params({
                "task": self.task,
                "target_col": self.target_col,
                "timeout_minutes": self.timeout_minutes,
                "metric": self.metric,
                "n_rows": len(pdf),
                "n_features": len(self.features),
            })
            
            log_feature_list(self.features)
            log_engine_meta({"engine": "flaml", "version": getattr(flaml_pkg, "__version__", "unknown")})
            
            # FLAML è®¾ç½®
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print(f"ğŸ¤– AutoML Training Configuration")
            print_separator()
            self.automl = AutoML()
            estimator_list = self._get_estimator_list()

            settings = {
                "task": self.task,
                "metric": self.metric,
                "time_budget": int(self.timeout_minutes * 60),  # è½¬æ¢ä¸ºç§’
                "eval_method": "holdout",
                "ensemble": False,
                "verbose": 0,  # æŠ‘åˆ¶æ—¥å¿—
                "estimator_list": estimator_list,
                "seed": 42,
                "log_file_name": None,
                "mlflow_logging": False,  # ç¦ç”¨ FLAML çš„è‡ªåŠ¨ MLflow è®°å½•ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶
            }

            if self.max_trials:
                settings["max_iter"] = self.max_trials

            safe_print(f"Task: {self.task}")
            safe_print(f"Metric: {self.metric}")
            safe_print(f"Time budget: {self.timeout_minutes} minutes ({int(self.timeout_minutes * 60)} seconds)")
            safe_print(f"Max trials: {self.max_trials if self.max_trials else 'unlimited'}")
            safe_print(f"Estimators: {', '.join(estimator_list)}")
            safe_print(f"Evaluation method: holdout")
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print("ğŸš€ Starting AutoML Training...", show_timestamp=False, show_level=False)
            print_separator()
            
            # æŠ‘åˆ¶ FLAML å’Œ MLflow å­ run çš„ debug æ—¥å¿—
            import logging as py_logging
            flaml_logger = py_logging.getLogger("flaml.automl.logger")
            flaml_automl_logger = py_logging.getLogger("flaml.automl")
            mlflow_logger = py_logging.getLogger("mlflow.tracking._tracking_service.client")
            mlflow_utils_logger = py_logging.getLogger("mlflow.utils")

            original_flaml_level = flaml_logger.level
            original_flaml_automl_level = flaml_automl_logger.level
            original_mlflow_level = mlflow_logger.level
            original_mlflow_utils_level = mlflow_utils_logger.level

            # è®¾ç½®ä¸º WARNING çº§åˆ«ï¼Œåªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
            flaml_logger.setLevel(py_logging.WARNING)
            flaml_automl_logger.setLevel(py_logging.WARNING)
            mlflow_logger.setLevel(py_logging.WARNING)
            mlflow_utils_logger.setLevel(py_logging.WARNING)

            safe_print("Training in progress... (FLAML debug logs suppressed)")
            
            start_time = time.time()
            
            try:
                # è®­ç»ƒ
                self.automl.fit(
                    X_train=X_train_num,
                    y_train=y_train,
                    X_val=X_val_num,
                    y_val=y_val,
                    **settings,
                )
            finally:
                # æ¢å¤æ—¥å¿—çº§åˆ«
                flaml_logger.setLevel(original_flaml_level)
                flaml_automl_logger.setLevel(original_flaml_automl_level)
                mlflow_logger.setLevel(original_mlflow_level)
                mlflow_utils_logger.setLevel(original_mlflow_utils_level)

            elapsed_time = time.time() - start_time
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print("âœ… AutoML Training Completed", show_timestamp=False, show_level=False)
            print_separator()
            safe_print(f"Total training time: {elapsed_time:.1f}s ({elapsed_time/60:.2f} minutes)")

            # è®°å½•æ‰€æœ‰ trials åˆ° MLflow
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print("ğŸ“ Logging All Trials to MLflow", show_timestamp=False, show_level=False)
            print_separator()

            trial_logger = TrialLogger(
                parent_run_id=parent_run_id,
                features=self.features,
                task=self.task,
                metric=self.metric
            )

            # è·å–æ‰€æœ‰ trials çš„å†å²è®°å½•
            # FLAML çš„ config_history æ ¼å¼: {trial_id: (estimator_name, config_dict, time)}
            if hasattr(self.automl, "config_history"):
                config_history = self.automl.config_history

                # config_history æ˜¯ä¸€ä¸ªå­—å…¸: {trial_id: (estimator, config, time)}
                for trial_id, trial_data in config_history.items():
                    if isinstance(trial_data, tuple) and len(trial_data) >= 2:
                        estimator = trial_data[0]
                        config_dict = trial_data[1] if len(trial_data) > 1 else {}

                        # ä» AutoML å¯¹è±¡è·å–è¯¥ trial çš„æŸå¤±å€¼
                        # FLAML å†…éƒ¨å­˜å‚¨äº†æ¯ä¸ªé…ç½®çš„éªŒè¯æŸå¤±
                        val_loss = config_dict.get("val_loss", float('inf'))
                        train_time = config_dict.get("time_total_s", 0.0)

                        # è¿‡æ»¤æ‰å†…éƒ¨å­—æ®µ
                        config = {k: v for k, v in config_dict.items()
                                 if k not in ["val_loss", "time_total_s", "trained_estimator", "learner"]}

                        trial_logger.log_trial(
                            config=config,
                            estimator=estimator,
                            val_loss=val_loss,
                            train_time=train_time
                        )

            safe_print(f"Total trials logged: {trial_logger.trial_count}")

            # è·å–æœ€ä½³ trial
            best_trial = trial_logger.get_best_trial()
            if best_trial:
                safe_print(f"Best trial: #{best_trial['trial_number']} ({best_trial['estimator']})")
                safe_print(f"Best val_loss: {best_trial['val_loss']:.6f}")
                best_trial_run_id = best_trial['run_id']
            else:
                best_trial_run_id = parent_run_id

            # è®°å½•æœ€ä½³é…ç½®åˆ°çˆ¶ run
            best_est = self.automl.best_estimator
            best_cfg = self.automl.best_config
            log_best_config_overall(best_cfg)
            if getattr(self.automl, "best_config_per_estimator", None):
                log_best_config_per_estimator(self.automl.best_config_per_estimator)

            mlflow.log_param("best_estimator", best_est)
            mlflow.log_param("best_trial_run_id", best_trial_run_id)
            mlflow.log_param("total_trials", trial_logger.trial_count)

            safe_print("", show_timestamp=False, show_level=False)
            safe_print(f"Best estimator: {best_est}")
            safe_print(f"Best config: {best_cfg}")
            
            # æ„å»ºæœåŠ¡ç®¡é“
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print(f"ğŸ”¨ Building Serving Pipeline")
            print_separator()
            clf = self.automl.model
            self.pipeline = SkPipe([("preprocess", self.preprocessor), ("clf", clf)])
            self.pipeline.fit(X_train, y_train)
            safe_print("Pipeline built: [Preprocessor] -> [Classifier/Regressor]")

            # è¯„ä¼°
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print(f"ğŸ“Š Model Evaluation")
            print_separator()
            metrics = self._evaluate_model(
                X_train, y_train, X_val, y_val, X_test, y_test
            )
            
            # æ³¨å†Œæ¨¡å‹
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print(f"ğŸ’¾ Model Registration")
            print_separator()
            model_uri = f"runs:/{parent_run_id}/model"
            model_version = None

            if self.register_model and self.model_name:
                mlflow.sklearn.log_model(self.pipeline, "model")
                safe_print(f"Model logged to MLflow")
                result = mlflow.register_model(model_uri, self.model_name)
                model_version = result.version
                safe_print(f"âœ… Model registered: '{self.model_name}' version {model_version}")
            else:
                mlflow.sklearn.log_model(self.pipeline, "model")
                safe_print(f"Model logged to MLflow (not registered)")
            
            # åˆ›å»º AutoMLSummary
            summary = AutoMLSummary(
                experiment_id=experiment.experiment_id,
                run_id=parent_run_id,
                best_trial_run_id=best_trial_run_id,  # ä½¿ç”¨æœ€ä½³ trial çš„ run_id
                model_uri=model_uri,
                model_version=model_version,
                metrics=metrics,
                best_estimator=best_est,
                best_params=best_cfg,
            )

            # æœ€ç»ˆæ€»ç»“
            safe_print("", show_timestamp=False, show_level=False)
            print_separator()
            safe_print(f"ğŸ‰ Training Pipeline Completed Successfully!")
            print_separator()
            safe_print(f"Experiment: {experiment_name} (ID: {experiment.experiment_id})")
            safe_print(f"Run ID: {parent_run_id}")
            safe_print(f"Best Model: {best_est}")
            if self.register_model and self.model_name:
                safe_print(f"Registered Model: {self.model_name} v{model_version}")
            safe_print(f"Model URI: {model_uri}")

            # æ˜¾ç¤ºæœ€ä½³æ€§èƒ½æŒ‡æ ‡
            if self.task == "classification":
                test_acc = metrics.get("test_accuracy", 0)
                test_f1 = metrics.get("test_f1", 0)
                safe_print(f"Test Accuracy: {test_acc:.4f}")
                safe_print(f"Test F1 Score: {test_f1:.4f}")
            elif self.task == "regression":
                test_r2 = metrics.get("test_r2", 0)
                test_rmse = metrics.get("test_rmse", 0)
                safe_print(f"Test RÂ²: {test_r2:.4f}")
                safe_print(f"Test RMSE: {test_rmse:.4f}")

            print_separator()

            return summary

