"""
Dango dlt Pipeline Runner

Generic runner for all dlt verified sources + custom CSV/REST API sources.

Key features:
- Dynamic source loading (importlib) - no hardcoded source logic
- Automatic DuckDB configuration
- State management for incremental loading
- Full-refresh support
- Error handling with retry logic
- CSV special handling (uses custom CSV loader)
"""

import importlib
import time
import signal
import platform
import os
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, List
from rich.console import Console
from dotenv import load_dotenv

import dlt
from dlt.common.pipeline import LoadInfo

from dango.config.models import (
    DataSource,
    SourceType,
    CSVSourceConfig,
    RESTAPISourceConfig,
)
from dango.ingestion.csv_loader import CSVLoader
from dango.ingestion.sources.registry import get_source_metadata

console = Console()


class SyncTimeoutError(Exception):
    """Raised when sync exceeds timeout"""
    pass


class DltPipelineRunner:
    """
    Generic pipeline runner for all dlt sources

    Usage:
        runner = DltPipelineRunner(project_root)
        result = runner.run_source(source_config)
    """

    def __init__(self, project_root: Path):
        """
        Initialize pipeline runner

        Args:
            project_root: Path to project root directory
        """
        self.project_root = project_root
        self.duckdb_path = project_root / "data" / "warehouse.duckdb"
        self.duckdb_path.parent.mkdir(parents=True, exist_ok=True)

        # Load .env file for credentials
        env_file = project_root / ".env"
        if env_file.exists():
            load_dotenv(env_file, override=True)

    def _run_with_timeout(self, func, timeout_minutes: int, *args, **kwargs):
        """
        Run a function with timeout (Unix-only using signals)

        Args:
            func: Function to execute
            timeout_minutes: Timeout in minutes
            *args, **kwargs: Arguments to pass to function

        Returns:
            Function result

        Raises:
            SyncTimeoutError: If execution exceeds timeout
        """
        # Check if we're on a Unix-like system (signal.alarm not available on Windows)
        is_unix = platform.system() in ['Linux', 'Darwin']  # Darwin = macOS

        if not is_unix:
            # Windows: No timeout (signal.alarm not available)
            console.print(f"[dim]‚ö†Ô∏è  Timeout not supported on Windows - running without timeout[/dim]")
            return func(*args, **kwargs)

        def timeout_handler(signum, frame):
            raise SyncTimeoutError(f"Sync exceeded {timeout_minutes} minute timeout")

        # Set up signal handler
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_minutes * 60)

        try:
            result = func(*args, **kwargs)
            signal.alarm(0)  # Cancel alarm
            return result
        except SyncTimeoutError:
            console.print(f"[red]‚è±Ô∏è  Sync timeout after {timeout_minutes} minutes[/red]")
            raise
        finally:
            signal.alarm(0)  # Ensure alarm is cancelled
            signal.signal(signal.SIGALRM, old_handler)  # Restore old handler

    def run_source(
        self,
        source_config: DataSource,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        full_refresh: bool = False,
        timeout_minutes: int = 60,
    ) -> Dict[str, Any]:
        """
        Run data pipeline for any source type

        Args:
            source_config: Source configuration (from sources.yml)
            start_date: Override start date for incremental loading
            end_date: Override end date for incremental loading
            full_refresh: Drop existing data and reload from scratch
            timeout_minutes: Timeout in minutes (default: 60)

        Returns:
            Dictionary with load statistics and status
        """
        from dango.utils.activity_log import log_activity
        from dango.utils.sync_history import save_sync_history_entry
        from dango.utils.db_health import check_disk_space, check_duckdb_health, DiskSpaceError

        source_name = source_config.name
        source_type = source_config.type
        start_time = datetime.now()

        console.print(f"\n{'='*60}")
        console.print(f"üç° Syncing: [bold]{source_name}[/bold] ({source_type.value})")
        console.print(f"{'='*60}")

        # Check disk space before starting sync
        try:
            check_disk_space(self.project_root, min_free_gb=5)
        except DiskSpaceError as e:
            error_message = str(e)
            console.print(f"[red]‚ùå {error_message}[/red]")

            # Save failed sync history
            history_entry = {
                "timestamp": start_time.isoformat(),
                "status": "failed",
                "duration_seconds": 0,
                "rows_processed": 0,
                "full_refresh": full_refresh,
                "error_message": error_message
            }
            save_sync_history_entry(self.project_root, source_name, history_entry)

            log_activity(
                project_root=self.project_root,
                level="error",
                source=source_name,
                message=f"Sync blocked: {error_message}"
            )

            return {
                "status": "failed",
                "source": source_name,
                "error": error_message,
                "rows_loaded": 0,
            }

        # Check DuckDB health and log warnings
        try:
            db_health = check_duckdb_health(self.duckdb_path)
            if db_health['status'] == 'large':
                console.print(f"[yellow]‚ö†Ô∏è  Database is large ({db_health['size_gb']}GB) - consider archiving old data[/yellow]")
            elif db_health['status'] == 'critical':
                console.print(f"[yellow]‚ö†Ô∏è  Database is very large ({db_health['size_gb']}GB) - performance may be affected[/yellow]")
        except Exception as e:
            console.print(f"[dim]‚ö†Ô∏è  Could not check database health: {e}[/dim]")

        # Log sync start
        log_activity(
            project_root=self.project_root,
            level="info",
            source=source_name,
            message=f"Starting sync"
        )

        try:
            # CSV: Custom implementation (Phase 1 loader)
            if source_type == SourceType.CSV:
                result = self._run_csv_source(source_config, full_refresh)
            # All other sources: dlt pipelines with timeout
            else:
                try:
                    result = self._run_with_timeout(
                        self._run_dlt_source,
                        timeout_minutes,
                        source_config,
                        start_date,
                        end_date,
                        full_refresh
                    )
                except SyncTimeoutError as e:
                    error_message = str(e)
                    console.print(f"[red]‚ùå {error_message}[/red]")
                    console.print(f"[yellow]‚ÑπÔ∏è  Pipeline state has been restored to prevent corruption[/yellow]")

                    # Return failure result
                    duration = (datetime.now() - start_time).total_seconds()
                    history_entry = {
                        "timestamp": start_time.isoformat(),
                        "status": "failed",
                        "duration_seconds": round(duration, 2),
                        "rows_processed": 0,
                        "full_refresh": full_refresh,
                        "error_message": error_message
                    }
                    save_sync_history_entry(self.project_root, source_name, history_entry)

                    log_activity(
                        project_root=self.project_root,
                        level="error",
                        source=source_name,
                        message=f"Sync timeout: {error_message}"
                    )

                    return {
                        "status": "failed",
                        "source": source_name,
                        "error": error_message,
                        "rows_loaded": 0,
                    }

            # Calculate duration
            duration = (datetime.now() - start_time).total_seconds()

            # Save sync history and log result
            success = result.get("status") == "success"
            rows_loaded = result.get("rows_loaded", 0)
            error_message = result.get("error")
            uses_replace_mode = result.get("uses_replace_mode", False)

            # Determine if this is a full refresh:
            # Either user explicitly requested it OR source uses replace write_disposition
            is_full_refresh = full_refresh or uses_replace_mode

            history_entry = {
                "timestamp": start_time.isoformat(),
                "status": "success" if success else "failed",
                "duration_seconds": round(duration, 2),
                "rows_processed": rows_loaded,
                "full_refresh": is_full_refresh,
                "error_message": error_message
            }
            save_sync_history_entry(self.project_root, source_name, history_entry)

            if success:
                log_activity(
                    project_root=self.project_root,
                    level="success",
                    source=source_name,
                    message=f"Sync completed in {round(duration, 1)}s - {rows_loaded:,} rows"
                )
            else:
                log_activity(
                    project_root=self.project_root,
                    level="error",
                    source=source_name,
                    message=f"Sync failed: {error_message}"
                )

            return result

        except Exception as e:
            # Get user-friendly error message
            friendly_error = self._analyze_error(e, source_name)
            console.print(f"[red]{friendly_error}[/red]")

            # Log error
            duration = (datetime.now() - start_time).total_seconds()
            error_message = str(e)

            history_entry = {
                "timestamp": start_time.isoformat(),
                "status": "failed",
                "duration_seconds": round(duration, 2),
                "rows_processed": 0,
                "full_refresh": full_refresh,
                "error_message": error_message
            }
            save_sync_history_entry(self.project_root, source_name, history_entry)

            log_activity(
                project_root=self.project_root,
                level="error",
                source=source_name,
                message=f"Sync failed: {error_message}"
            )

            return {
                "status": "failed",
                "source": source_name,
                "error": error_message,
                "rows_loaded": 0,
            }

    def _run_csv_source(
        self, source_config: DataSource, full_refresh: bool = False
    ) -> Dict[str, Any]:
        """
        Run CSV source using custom CSV loader

        Args:
            source_config: Source configuration
            full_refresh: If True, drop existing table and reload

        Returns:
            Load statistics
        """
        if not source_config.csv:
            raise ValueError(f"CSV config missing for source: {source_config.name}")

        # Full refresh: drop existing table and clear metadata
        if full_refresh:
            import duckdb

            conn = duckdb.connect(str(self.duckdb_path))
            try:
                conn.execute(f"DROP TABLE IF EXISTS raw.{source_config.name}")
                console.print("  üîÑ Full refresh: dropped existing table")

                # Also clear metadata for this source so files are treated as new
                conn.execute("""
                    DELETE FROM _dango_file_metadata
                    WHERE source_name = ?
                """, [source_config.name])
                console.print("  üîÑ Full refresh: cleared file metadata")
            except Exception as e:
                console.print(f"  ‚ö†Ô∏è  Could not drop table/metadata: {e}")
            finally:
                conn.close()

        # Run CSV loader
        loader = CSVLoader(self.project_root, self.duckdb_path)
        result = loader.load(
            source_name=source_config.name,
            config=source_config.csv,
            target_schema="raw",
        )

        return {
            "status": result.get("status", "success"),
            "source": source_config.name,
            "rows_loaded": result.get("total_rows", 0),
            "files_processed": result.get("new", 0) + result.get("updated", 0),
            **result,
        }

    def _detect_write_disposition(self, source: Any) -> bool:
        """
        Detect if dlt source uses 'replace' write_disposition.

        This determines if the source performs full refreshes (replace mode)
        or true incremental loading (append/merge mode).

        Args:
            source: dlt source object

        Returns:
            True if source uses replace mode, False otherwise
        """
        try:
            # dlt sources can be callables or DltResource objects
            # Try to extract resources and check their write_disposition
            if hasattr(source, 'resources'):
                # Source has resources attribute (DltSource object)
                resources = source.resources
                if hasattr(resources, 'values'):
                    # resources is a dict-like object
                    for resource in resources.values():
                        if hasattr(resource, 'write_disposition'):
                            if resource.write_disposition == "replace":
                                return True
            elif hasattr(source, '__iter__'):
                # Source is iterable (list of resources)
                for resource in source:
                    if hasattr(resource, 'write_disposition'):
                        if resource.write_disposition == "replace":
                            return True
            elif hasattr(source, 'write_disposition'):
                # Source itself has write_disposition
                if source.write_disposition == "replace":
                    return True

            return False

        except Exception:
            # If we can't detect, assume it's not replace mode (safer default)
            return False

    def _get_dataset_name(
        self,
        source_config: DataSource,
        source_type: Any,
        metadata: Dict[str, Any]
    ) -> str:
        """
        Determine dataset name based on source characteristics.

        Logic:
        - Multi-resource API sources (Stripe, Shopify, etc.) ‚Üí raw_{source_name}
        - Single-resource sources (CSV, single table) ‚Üí raw

        This prevents table name collisions while keeping simple governance rules.
        """
        source_name = source_config.name

        # Check if source is marked as multi-resource in registry
        if metadata.get("multi_resource", False):
            return f"raw_{source_name}"

        # Additional check: inspect source config for multiple resources
        # This handles database sources that can sync multiple tables
        source_config_obj = getattr(source_config, source_type.value, None)
        if source_config_obj:
            try:
                source_dict = source_config_obj.model_dump() if hasattr(source_config_obj, 'model_dump') else {}

                # Check for endpoints/resources/tables parameter
                resources = (
                    source_dict.get('endpoints') or
                    source_dict.get('resources') or
                    source_dict.get('tables') or
                    source_dict.get('objects')
                )

                # If multiple resources configured, use prefixed schema
                if resources and isinstance(resources, list) and len(resources) > 1:
                    return f"raw_{source_name}"
            except Exception:
                # If we can't inspect config, default to single-resource
                pass

        # Default: single-resource source uses shared "raw" schema
        return "raw"

    def _run_dlt_source(
        self,
        source_config: DataSource,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        full_refresh: bool = False,
    ) -> Dict[str, Any]:
        """
        Run dlt pipeline for any verified source (generic implementation)

        This method dynamically imports and executes the correct dlt source
        without hardcoding any source-specific logic.

        Implements state backup/restore to prevent partial failure state corruption.

        Args:
            source_config: Source configuration
            start_date: Override start date
            end_date: Override end date
            full_refresh: Drop pipeline state and reload

        Returns:
            Load statistics
        """
        source_name = source_config.name
        source_type = source_config.type

        # Get source metadata from registry
        metadata = get_source_metadata(source_type.value)
        if not metadata:
            raise ValueError(
                f"Source type '{source_type.value}' not found in registry. "
                f"Available sources: {list(get_source_metadata.keys())}"
            )

        dlt_package = metadata.get("dlt_package")
        dlt_function = metadata.get("dlt_function")
        if not dlt_package or not dlt_function:
            raise ValueError(f"No dlt package/function defined for source type: {source_type.value}")

        console.print(f"  üì¶ Loading dlt source: {dlt_package}.{dlt_function}")

        # Build source configuration
        source_kwargs = self._build_source_config(
            source_config, source_type, start_date, end_date
        )

        # Dynamic import of dlt source
        source = self._load_dlt_source(dlt_package, dlt_function, source_kwargs)

        # Detect actual load type from dlt source configuration
        # Check if source uses replace write_disposition (full refresh by design)
        uses_replace_mode = self._detect_write_disposition(source)

        # Determine dataset name based on source characteristics
        # Multi-resource sources ‚Üí raw_{source_name} (prevents table collisions)
        # Single-resource sources ‚Üí raw (simple governance)
        dataset_name = self._get_dataset_name(source_config, source_type, metadata)

        # Create pipeline with DuckDB destination
        pipeline = dlt.pipeline(
            pipeline_name=source_name,
            destination=dlt.destinations.duckdb(credentials=str(self.duckdb_path)),
            dataset_name=dataset_name,
        )

        # Full refresh: drop pipeline state
        if full_refresh:
            console.print("  üîÑ Full refresh: dropping pipeline state")
            try:
                pipeline.drop()
            except Exception as e:
                console.print(f"  ‚ö†Ô∏è  Could not drop pipeline: {e}")

        # Backup dlt state before running (protects against partial failures)
        state_backup = self._backup_dlt_state(source_name)

        try:
            # Run pipeline with retry logic
            load_info = self._run_with_retry(pipeline, source, max_retries=3)

            # Extract load statistics
            stats = self._extract_load_stats(load_info)

            # Success criteria: rows_loaded >= 0 means we got valid data (even if 0 rows)
            # rows_loaded == -1 means we couldn't extract stats but load succeeded
            rows_loaded = stats.get("rows_loaded", 0)

            if rows_loaded >= 0:
                # Success - we got a valid row count (including 0)
                self._cleanup_state_backup(state_backup)
                console.print(f"  ‚úì Loaded {rows_loaded:,} rows")

                return {
                    "status": "success",
                    "source": source_name,
                    "uses_replace_mode": uses_replace_mode,
                    **stats,
                }
            else:
                # rows_loaded is -1: unknown row count but load succeeded
                # This should also be treated as success
                self._cleanup_state_backup(state_backup)
                console.print(f"  ‚úì Load completed (row count unavailable)")

                return {
                    "status": "success",
                    "source": source_name,
                    "uses_replace_mode": uses_replace_mode,
                    **stats,
                }

        except Exception as e:
            # Pipeline failed - restore previous state
            console.print(f"  ‚ùå Pipeline failed: {e}")
            console.print(f"  üîÑ Restoring previous state...")
            self._restore_dlt_state(state_backup)
            raise

    def _build_source_config(
        self,
        source_config: DataSource,
        source_type: SourceType,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """
        Build source-specific configuration dictionary

        Extracts config from the appropriate field in DataSource based on source type
        and merges with override parameters.

        Args:
            source_config: Source configuration
            source_type: Source type
            start_date: Override start date
            end_date: Override end date

        Returns:
            Dictionary of source-specific parameters
        """
        # Map source type to config field name
        config_field_map = {
            SourceType.REST_API: "rest_api",
            SourceType.FACEBOOK_ADS: "facebook_ads",
            SourceType.GOOGLE_ANALYTICS: "google_analytics",
            SourceType.GOOGLE_SHEETS: "google_sheets",
            SourceType.HUBSPOT: "hubspot",
            SourceType.SALESFORCE: "salesforce",
            SourceType.STRIPE: "stripe",
            SourceType.SHOPIFY: "shopify",
            SourceType.GITHUB: "github",
            SourceType.SLACK: "slack",
            # Add more as we implement them
        }

        # Get the config object
        config_field = config_field_map.get(source_type)
        if not config_field:
            # Use generic_config for sources without dedicated models
            config_obj = source_config.generic_config or {}
        else:
            config_obj = getattr(source_config, config_field, None)
            if config_obj is None:
                raise ValueError(
                    f"Missing {config_field} configuration for source: {source_config.name}"
                )

        # Convert Pydantic model to dict if needed
        if hasattr(config_obj, "dict"):
            config_dict = config_obj.dict(exclude_none=True)
        else:
            config_dict = dict(config_obj) if isinstance(config_obj, dict) else {}

        # Resolve environment variables (fields ending in _env)
        import os

        resolved_config = {}
        for key, value in config_dict.items():
            if key.endswith("_env") and isinstance(value, str):
                # Get actual value from environment
                env_value = os.getenv(value)
                if env_value is None:
                    console.print(f"  ‚ö†Ô∏è  Warning: Environment variable '{value}' not set")
                # Remove _env suffix for actual parameter name
                param_name = key[:-4]  # Remove '_env'
                resolved_config[param_name] = env_value
            else:
                resolved_config[key] = value

        # Override dates if provided
        if start_date:
            resolved_config["start_date"] = start_date
        if end_date:
            resolved_config["end_date"] = end_date

        return resolved_config

    def _load_dlt_source(self, dlt_package: str, dlt_function: str, source_kwargs: Dict[str, Any]) -> Any:
        """
        Dynamically import and instantiate a dlt source from bundled or built-in sources

        Args:
            dlt_package: Name of the dlt source package (e.g., 'stripe_analytics', 'rest_api')
            dlt_function: Name of the source function to call (e.g., 'stripe_source', 'rest_api_source')
            source_kwargs: Keyword arguments to pass to the source function

        Returns:
            dlt source object
        """
        # Try bundled sources first (verified sources we've included in dango)
        module_path = f"dango.ingestion.dlt_sources.{dlt_package}"

        try:
            console.print(f"    Importing: {module_path}")
            source_module = importlib.import_module(module_path)
        except ImportError:
            # Fall back to built-in dlt sources (e.g., rest_api, filesystem, sql_database)
            module_path = f"dlt.sources.{dlt_package}"
            try:
                console.print(f"    Importing built-in: {module_path}")
                source_module = importlib.import_module(module_path)
            except ImportError as e:
                raise ImportError(
                    f"Could not import dlt source package '{dlt_package}' from bundled sources or built-in dlt sources. "
                    f"Error: {e}"
                )

        try:
            # Get the source function from the module
            console.print(f"    Loading function: {dlt_function}")
            source_function = getattr(source_module, dlt_function)

            # Call source function with config
            console.print(f"    Calling: {dlt_function}(**config)")
            source = source_function(**source_kwargs)

            return source

        except AttributeError as e:
            raise AttributeError(
                f"dlt source package '{dlt_package}' does not have function '{dlt_function}'. "
                f"Available functions: {[name for name in dir(source_module) if not name.startswith('_')]}"
                f"\nError: {e}"
            )
        except Exception as e:
            raise Exception(f"Error loading dlt source '{dlt_package}.{dlt_function}': {e}")

    def _analyze_error(self, error: Exception, source_name: str) -> str:
        """
        Analyze exception and provide user-friendly error message

        Args:
            error: The exception that occurred
            source_name: Name of the data source

        Returns:
            User-friendly error message with guidance
        """
        error_str = str(error).lower()
        error_type = type(error).__name__

        # Authentication errors
        if any(keyword in error_str for keyword in ['unauthorized', '401', 'invalid api key', 'invalid token', 'authentication failed', 'invalid credentials', 'forbidden', '403']):
            return f"""
Authentication Error: Invalid credentials for '{source_name}'

Possible causes:
  ‚Ä¢ API key/token expired or invalid
  ‚Ä¢ Insufficient permissions/scopes
  ‚Ä¢ Wrong environment (test vs live mode)

How to fix:
  1. Check .env file for correct credentials
  2. Verify token hasn't expired
  3. Confirm required permissions are granted
  4. Test credentials manually (curl/API docs)

Example fix:
  # Update .env with correct credentials
  {source_name.upper()}_API_KEY=sk_live_xxx...

Error details: {str(error)}
"""

        # Rate limit errors
        if any(keyword in error_str for keyword in ['rate limit', '429', 'too many requests', 'quota exceeded', 'throttled']):
            return f"""
Rate Limit Error: API rate limit exceeded for '{source_name}'

Possible causes:
  ‚Ä¢ Syncing too frequently
  ‚Ä¢ Large dataset hitting API limits
  ‚Ä¢ Shared API key with other services

How to fix:
  1. Wait a few minutes and try again
  2. Reduce sync frequency
  3. Use smaller date ranges for initial sync
  4. Contact API provider to increase limits

Most sources have these limits:
  ‚Ä¢ Stripe: 100 req/sec
  ‚Ä¢ GitHub: 5,000 req/hour
  ‚Ä¢ Airtable: 5 req/sec

Next steps:
  # Wait and retry
  dango sync --source {source_name}

  # Or use smaller date range
  dango sync --source {source_name} --start-date 2024-12-01 --end-date 2024-12-31

Error details: {str(error)}
"""

        # Schema/data validation errors
        if any(keyword in error_str for keyword in ['schema', 'validation', 'column', 'field', 'type error', 'data type']):
            return f"""
Data Validation Error: Schema mismatch or invalid data for '{source_name}'

Possible causes:
  ‚Ä¢ API schema changed (new/removed fields)
  ‚Ä¢ Data type mismatch
  ‚Ä¢ Staging model out of sync

How to fix:
  1. Run full refresh to reload schema
  2. Check if staging model needs update
  3. Review API documentation for changes

Fix commands:
  # Reload all data with latest schema
  dango sync --source {source_name} --full-refresh

  # Re-run dbt to update models
  dango run

Error details: {str(error)}
"""

        # Connection/network errors
        if any(keyword in error_str for keyword in ['connection', 'timeout', 'network', 'dns', 'unreachable', 'refused']):
            return f"""
Connection Error: Cannot reach API for '{source_name}'

Possible causes:
  ‚Ä¢ Network connectivity issues
  ‚Ä¢ API service down
  ‚Ä¢ Firewall/proxy blocking requests
  ‚Ä¢ Invalid API endpoint URL

How to fix:
  1. Check internet connection
  2. Verify API status page
  3. Test connectivity: ping/curl
  4. Check firewall/proxy settings

Next steps:
  # Wait and retry
  dango sync --source {source_name}

  # Check API status
  Visit source's status page (docs.api-provider.com/status)

Error details: {str(error)}
"""

        # Generic error with helpful context
        return f"""
Sync Error for '{source_name}':

{str(error)}

Troubleshooting steps:
  1. Check logs: dango start ‚Üí Activity Logs
  2. Verify configuration: .dango/sources.yml
  3. Test with smaller date range
  4. Try full refresh: dango sync --source {source_name} --full-refresh
  5. Check API documentation for breaking changes

Need help? Visit: https://github.com/anthropics/dango/issues
"""

    def _run_with_retry(
        self, pipeline: dlt.Pipeline, source: Any, max_retries: int = 3
    ) -> LoadInfo:
        """
        Run pipeline with exponential backoff retry logic

        Args:
            pipeline: dlt pipeline object
            source: dlt source object
            max_retries: Maximum number of retry attempts

        Returns:
            LoadInfo from successful run

        Raises:
            Exception: If all retries fail
        """
        last_exception = None

        for attempt in range(1, max_retries + 1):
            try:
                console.print(f"  ‚è≥ Running pipeline... (attempt {attempt}/{max_retries})")
                load_info = pipeline.run(source)
                return load_info

            except Exception as e:
                last_exception = e
                console.print(f"  ‚ö†Ô∏è  Attempt {attempt} failed: {e}")

                if attempt < max_retries:
                    # Exponential backoff: 2^attempt seconds
                    wait_time = 2**attempt
                    console.print(f"  ‚è∏Ô∏è  Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    console.print(f"  ‚ùå All {max_retries} attempts failed")

        raise last_exception

    def _extract_load_stats(self, load_info: LoadInfo) -> Dict[str, Any]:
        """
        Extract statistics from dlt LoadInfo and query database for row counts.

        dlt's LoadInfo doesn't provide row counts directly, so we extract the list
        of loaded tables from metrics and query the database for actual row counts.

        Args:
            load_info: LoadInfo object from pipeline.run()

        Returns:
            Dictionary with load statistics
        """
        stats = {
            "rows_loaded": 0,
            "tables_loaded": [],
            "load_id": load_info.load_id if hasattr(load_info, "load_id") else None,
        }

        # Extract list of tables that were loaded from metrics
        loaded_tables = set()
        try:
            if hasattr(load_info, "metrics") and isinstance(load_info.metrics, dict):
                for load_id, metric_list in load_info.metrics.items():
                    if isinstance(metric_list, list):
                        for metric_entry in metric_list:
                            if isinstance(metric_entry, dict) and 'job_metrics' in metric_entry:
                                for job_metrics in metric_entry['job_metrics'].values():
                                    if hasattr(job_metrics, 'table_name'):
                                        table_name = job_metrics.table_name
                                        # Skip dlt internal tables
                                        if not table_name.startswith('_dlt_'):
                                            loaded_tables.add(table_name)
                                            stats["tables_loaded"].append(table_name)
        except Exception as e:
            console.print(f"[dim]Warning: Could not extract table list from metrics: {e}[/dim]")

        # Query database for row counts
        # Get dataset name (schema) from load_info
        dataset_name = load_info.dataset_name if hasattr(load_info, "dataset_name") else None

        if loaded_tables and dataset_name:
            try:
                import duckdb
                conn = duckdb.connect(str(self.duckdb_path))

                total_rows = 0
                for table_name in loaded_tables:
                    try:
                        result = conn.execute(
                            f'SELECT COUNT(*) FROM "{dataset_name}"."{table_name}"'
                        ).fetchone()
                        if result:
                            table_rows = result[0]
                            total_rows += table_rows
                    except Exception as table_err:
                        console.print(f"[dim]Warning: Could not count rows for {table_name}: {table_err}[/dim]")
                        continue

                conn.close()
                stats["rows_loaded"] = total_rows

            except Exception as e:
                console.print(f"[dim]Warning: Could not query database for row counts: {e}[/dim]")
                # If we can't get row counts, mark as unknown
                stats["rows_loaded"] = -1
        elif not loaded_tables:
            # No tables loaded (or couldn't extract list)
            stats["rows_loaded"] = -1  # -1 means "unknown but successful"

        return stats

    def _backup_dlt_state(self, pipeline_name: str) -> Optional[Path]:
        """
        Backup dlt pipeline state before running.

        dlt stores state in ~/.dlt/pipelines/{pipeline_name}/

        Args:
            pipeline_name: Name of the dlt pipeline

        Returns:
            Path to backup directory if backup was created, None otherwise
        """
        import shutil
        import os

        # Determine dlt state location
        # dlt uses ~/.dlt by default
        dlt_home = Path(os.path.expanduser("~/.dlt"))
        pipeline_state_dir = dlt_home / "pipelines" / pipeline_name

        if not pipeline_state_dir.exists():
            # No state to backup (first run)
            return None

        # Create backup with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = pipeline_state_dir.parent / f"{pipeline_name}_backup_{timestamp}"

        try:
            shutil.copytree(pipeline_state_dir, backup_dir)
            console.print(f"  [dim]üíæ State backed up to {backup_dir.name}[/dim]")
            return backup_dir
        except Exception as e:
            console.print(f"  [dim]‚ö†Ô∏è  Could not backup state: {e}[/dim]")
            return None

    def _restore_dlt_state(self, backup_dir: Optional[Path]):
        """
        Restore dlt pipeline state from backup.

        Args:
            backup_dir: Path to backup directory (from _backup_dlt_state)
        """
        import shutil

        if not backup_dir or not backup_dir.exists():
            console.print(f"  [dim]‚ÑπÔ∏è  No state backup to restore[/dim]")
            return

        # Extract pipeline name from backup directory name
        # Format: {pipeline_name}_backup_{timestamp}
        backup_name = backup_dir.name
        pipeline_name = "_".join(backup_name.split("_backup_")[0].split("_"))

        # Determine original state location
        pipeline_state_dir = backup_dir.parent / pipeline_name

        try:
            # Remove current (corrupted) state
            if pipeline_state_dir.exists():
                shutil.rmtree(pipeline_state_dir)

            # Restore from backup
            shutil.copytree(backup_dir, pipeline_state_dir)
            console.print(f"  [green]‚úì State restored from backup[/green]")

            # Clean up backup
            shutil.rmtree(backup_dir)
            console.print(f"  [dim]‚úì Backup cleaned up[/dim]")

        except Exception as e:
            console.print(f"  [yellow]‚ö†Ô∏è  Could not restore state: {e}[/yellow]")
            console.print(f"  [dim]Backup preserved at: {backup_dir}[/dim]")

    def _cleanup_state_backup(self, backup_dir: Optional[Path]):
        """
        Clean up state backup after successful pipeline run.

        Args:
            backup_dir: Path to backup directory (from _backup_dlt_state)
        """
        import shutil

        if not backup_dir or not backup_dir.exists():
            return

        try:
            shutil.rmtree(backup_dir)
            console.print(f"  [dim]‚úì State backup cleaned up[/dim]")
        except Exception as e:
            console.print(f"  [dim]‚ö†Ô∏è  Could not clean up backup: {e}[/dim]")


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


def _display_dbt_output(dbt_output: str) -> None:
    """
    Parse and display dbt model execution details.

    Extracts per-model status and timing from dbt output to show consistent
    logging across all data source types.

    Args:
        dbt_output: Raw dbt stdout+stderr output
    """
    import re

    # Strip ANSI color codes from dbt output
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    clean_output = ansi_escape.sub('', dbt_output)

    # Parse dbt output for model execution lines
    # Format: "1 of 3 OK created sql table model staging.stg_name ... [OK in 0.10s]"
    # or: "1 of 3 ERROR creating sql table model ... [ERROR in 0.10s]"

    # Match lines like: "1 of 3 OK created sql table model staging.stg_stripe_test_1__charge ............ [OK in 0.10s]"
    success_pattern = r'(\d+) of (\d+) (OK|ERROR|SKIP) .*? model (\w+)\.(\S+).*?\[(OK|ERROR|SKIP)(?: in ([\d.]+)s)?\]'

    found_any = False
    for line in clean_output.split('\n'):
        match = re.search(success_pattern, line)
        if match:
            found_any = True
            seq_num, total, status_word, schema, model_name, result, timing = match.groups()

            # Format output consistently
            if result == "OK":
                status_icon = "‚úì"
                status_color = "green"
            elif result == "ERROR":
                status_icon = "‚úó"
                status_color = "red"
            elif result == "SKIP":
                status_icon = "‚äò"
                status_color = "yellow"
            else:
                continue

            timing_str = f" in {timing}s" if timing else ""
            console.print(f"  [{status_color}]{status_icon}[/{status_color}] {schema}.{model_name}{timing_str}")


def run_sync(
    project_root: Path,
    sources: List[DataSource],
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    full_refresh: bool = False,
) -> Dict[str, Any]:
    """
    Sync multiple sources and return summary

    Args:
        project_root: Path to project root
        sources: List of source configurations
        start_date: Override start date
        end_date: Override end date
        full_refresh: Full refresh mode

    Returns:
        Summary dictionary with success/failed counts
    """
    runner = DltPipelineRunner(project_root)

    results = []
    success_sources = []
    failed_sources = []
    skipped_sources = []

    for source_config in sources:
        if not source_config.enabled:
            console.print(f"\n‚è≠Ô∏è  Skipping disabled source: {source_config.name}")
            skipped_sources.append(source_config.name)
            continue

        result = runner.run_source(source_config, start_date, end_date, full_refresh)
        results.append(result)

        if result.get("status") == "success":
            success_sources.append(source_config.name)
        else:
            failed_sources.append({
                "name": source_config.name,
                "error": result.get("error", "Unknown error")
            })

    # Print detailed summary
    console.print(f"\n{'='*60}")
    console.print(f"üìä Sync Summary:")
    console.print(f"{'='*60}\n")

    if success_sources:
        console.print(f"[green]‚úì Succeeded ({len(success_sources)}):[/green]")
        for name in success_sources:
            console.print(f"  ‚Ä¢ {name}")
        console.print()

    if failed_sources:
        console.print(f"[red]‚úó Failed ({len(failed_sources)}):[/red]")
        for item in failed_sources:
            console.print(f"  ‚Ä¢ {item['name']}")
            console.print(f"    [dim]{item['error']}[/dim]")
        console.print()

    if skipped_sources:
        console.print(f"[dim]‚è≠  Skipped ({len(skipped_sources)}):[/dim]")
        for name in skipped_sources:
            console.print(f"  ‚Ä¢ {name} [dim](disabled)[/dim]")
        console.print()

    # Overall stats
    total = len(success_sources) + len(failed_sources)
    if total > 0:
        success_rate = (len(success_sources) / total) * 100
        console.print(f"Overall: {len(success_sources)}/{total} sources succeeded ({success_rate:.0f}%)")

    console.print(f"{'='*60}\n")

    # Auto-generate staging models for successful sources
    if success_sources:
        console.print("üîÑ [bold]Generating staging models...[/bold]\n")

        from dango.transformation.generator import DbtModelGenerator

        generator = DbtModelGenerator(project_root)

        # Get successful source configs
        successful_configs = [src for src in sources if src.name in success_sources]

        gen_summary = generator.generate_all_models(
            sources=successful_configs,
            generate_schema_yml=True,  # Required for dbt source() references
            skip_customized=True,  # Don't overwrite user customizations
        )

        if gen_summary.get("generated"):
            console.print(f"[green]‚úì Generated/updated {len(gen_summary['generated'])} staging model(s)[/green]")
            for item in gen_summary['generated']:
                console.print(f"  ‚Ä¢ {item['source']}")

        if gen_summary.get("skipped"):
            console.print(f"[yellow]‚ö†Ô∏è  Skipped {len(gen_summary['skipped'])} model(s) (user-customized)[/yellow]")
            for item in gen_summary['skipped']:
                console.print(f"  ‚Ä¢ {item['source']}: {item['reason']}")

        console.print()

        # Run dbt to create staging/marts tables
        # Use selective runs to only process models dependent on synced sources
        console.print("üîÑ [bold]Running dbt models...[/bold]\n")
        from dango.transformation import run_dbt_models

        # Build source-based selection criteria
        # Format: "source:source1+ source:source2+" (runs models dependent on these sources)
        if success_sources:
            select_criteria = " ".join([f"source:{source}+" for source in success_sources])
            console.print(f"[dim]Targeting models for sources: {', '.join(success_sources)}[/dim]")
            dbt_success, dbt_output = run_dbt_models(project_root, select=select_criteria)
        else:
            # No sources synced, run all models (backward compatibility)
            dbt_success, dbt_output = run_dbt_models(project_root)

        if dbt_success:
            # Parse and display dbt model execution details
            _display_dbt_output(dbt_output)
            console.print("[green]‚úì dbt models executed successfully[/green]")

            # Generate dbt docs
            console.print("[dim]Generating dbt documentation...[/dim]")
            from dango.transformation import generate_dbt_docs

            docs_success, docs_output = generate_dbt_docs(project_root)
            if docs_success:
                console.print("[green]‚úì dbt docs generated[/green]")
            else:
                console.print("[yellow]‚ö†Ô∏è  dbt docs generation failed (non-critical)[/yellow]")

            # Refresh Metabase connection to see new data
            console.print("[dim]Refreshing Metabase connection...[/dim]")
            from dango.visualization.metabase import refresh_metabase_connection, sync_metabase_schema

            if refresh_metabase_connection(project_root):
                console.print("[green]‚úì Metabase connection refreshed[/green]")

                # Sync schema metadata to ensure all tables are discovered and descriptions updated
                console.print("[dim]Syncing Metabase schema metadata...[/dim]")
                if sync_metabase_schema(project_root):
                    console.print("[green]‚úì Metabase schema synced[/green]")
            else:
                console.print("[dim]‚Ñπ Metabase not running (will sync when started)[/dim]")
        else:
            console.print("[red]‚úó dbt run failed[/red]")
            console.print(f"[dim]{dbt_output}[/dim]")
            console.print("[yellow]‚ö†Ô∏è  Staging/marts tables were not created[/yellow]")

        console.print()

    return {
        "success_count": len(success_sources),
        "failed_count": len(failed_sources),
        "skipped_count": len(skipped_sources),
        "success_sources": success_sources,
        "failed_sources": failed_sources,
        "skipped_sources": skipped_sources,
        "results": results,
    }
