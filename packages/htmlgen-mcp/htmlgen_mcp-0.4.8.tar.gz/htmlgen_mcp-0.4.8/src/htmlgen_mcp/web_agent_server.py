#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Smart Web Agent MCP æœåŠ¡

åŸºäº SmartWebAgent æä¾›ç½‘é¡µç”Ÿæˆçš„è§„åˆ’ä¸æ‰§è¡Œæ¥å£ï¼Œå…¼å®¹ Model Context Protocolã€‚
"""

from __future__ import annotations

import asyncio
import json
import os
import sys
import time
import traceback
import zipfile
import tempfile
import aiohttp
from typing import Any, Dict, Optional

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨æ¨¡å—æœç´¢è·¯å¾„ä¸­
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# å½“å‰æ–‡ä»¶åœ¨ src/htmlgen_mcp/ ä¸‹ï¼Œæ‰€ä»¥éœ€è¦å‘ä¸Šä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(CURRENT_DIR)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from fastmcp import FastMCP  # type: ignore

import uuid
from pathlib import Path

from htmlgen_mcp.agents.smart_web_agent import SmartWebAgent
from htmlgen_mcp.nas_storage import get_nas_storage
from htmlgen_mcp.nas_log_manager import get_nas_log_manager, ensure_job_log, log_progress, query_progress
from htmlgen_mcp.prompt_enhancer import enhance_prompt_for_real_data
from datetime import datetime

# ä½¿ç”¨ NAS ä½œä¸ºé»˜è®¤å­˜å‚¨è·¯å¾„
NAS_PATH = os.environ.get("NAS_STORAGE_PATH", "/app/mcp-servers/mcp-servers/html_agent")
# é¡¹ç›®æ ¹ç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ WEB_AGENT_PROJECT_ROOT ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨ NAS_PATH/projects
DEFAULT_PROJECT_ROOT = os.path.abspath(
    os.environ.get("WEB_AGENT_PROJECT_ROOT", f"{NAS_PATH}/projects")
)
# æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆé¡¹ç›®å­ç›®å½•ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
AUTO_CREATE_PROJECT_DIR = os.environ.get("AUTO_CREATE_PROJECT_DIR", "true").lower() == "true"
DEFAULT_UPLOAD_URL = os.environ.get(
    "UPLOAD_URL", "https://www.mcpcn.cc/api/fileUploadAndDownload/uploadMcpFile"
)
DEFAULT_MODEL = os.environ.get("WEB_AGENT_MODEL", "qwen3-coder-plus-2025-09-23")
DEFAULT_BASE_URL = os.environ.get(
    "OPENAI_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"
)

mcp = FastMCP("smart-web-agent")

# MCP æœåŠ¡æŒä¹…åŒ–ç›®å½•ï¼šä½¿ç”¨ NAS è·¯å¾„ä»¥ä¾¿é›†ç¾¤å…±äº«
MCP_SERVICE_NAME = os.environ.get("MCP_SERVICE_NAME", "make_web")
MCP_DATA_ROOT = Path(
    os.environ.get("MCP_DATA_DIR", f"{NAS_PATH}/mcp_data/{MCP_SERVICE_NAME}")
)
MCP_DATA_ROOT.mkdir(parents=True, exist_ok=True)

# ç®€å•çš„ç¼“å­˜ï¼šè®°å½•æœ€è¿‘ä¸€æ¬¡ç”Ÿæˆçš„è®¡åˆ’ï¼Œé¿å…â€œcreate_simple_site â†’ execute_planâ€æ—¶éœ€æ‰‹åŠ¨ä¼ é€’
PLAN_CACHE_DIR = MCP_DATA_ROOT / "plan_cache"
PLAN_CACHE_DIR.mkdir(exist_ok=True)

# è¿›åº¦æ—¥å¿—ç›®å½•ï¼Œå­˜å‚¨æ¯ä¸ªä»»åŠ¡çš„å®æ—¶è¿›åº¦
PROGRESS_LOG_DIR = MCP_DATA_ROOT / "progress_logs"
PROGRESS_LOG_DIR.mkdir(exist_ok=True)

# ä»»åŠ¡çŠ¶æ€ç›®å½•ï¼Œæ¯ä¸ªä»»åŠ¡ä¸€ä¸ª JSON æ–‡ä»¶
JOB_STATE_DIR = MCP_DATA_ROOT / "jobs" / "state"
JOB_STATE_DIR.mkdir(parents=True, exist_ok=True)

# ä¸Šä¸‹æ–‡ç¼“å­˜ç›®å½•
CONTEXT_CACHE_DIR = MCP_DATA_ROOT / "context_cache"
CONTEXT_CACHE_DIR.mkdir(exist_ok=True)

_PLAN_CACHE: dict[tuple[str, str], Dict[str, Any]] = {}
_PLAN_CACHE_BY_ID: dict[str, Dict[str, Any]] = {}
_PROGRESS_LOG_BY_ID: dict[str, str] = {}
_PROGRESS_LOG_BY_JOB: dict[str, str] = {}
_JOB_REGISTRY: dict[str, Dict[str, Any]] = {}
_CONTEXT_CACHE_BY_ID: dict[str, Dict[str, Any]] = {}
_CONTEXT_ID_BY_PLAN: dict[str, str] = {}


def _resolve_edgeone_deploy_env() -> str:
    """è§£æ EdgeOne è‡ªåŠ¨éƒ¨ç½²ç¯å¢ƒï¼Œé»˜è®¤ Productionã€‚"""
    env_value = (
        os.environ.get("EDGEONE_AUTO_DEPLOY_ENV")
        or os.environ.get("EDGEONE_PAGES_DEPLOY_ENV")
        or "Production"
    )
    return env_value if env_value in {"Production", "Preview"} else "Production"


def _should_upload_zip_to_oss() -> bool:
    """æ˜¯å¦åœ¨ EdgeOne éƒ¨ç½²å‰ä¸Šä¼  ZIP åˆ° OSSã€‚"""
    flag = os.environ.get("KEEP_OSS_UPLOAD", "true").strip().lower()
    return flag not in {"0", "false", "no", "off"}


def _extract_zip_url(upload_result: Dict[str, Any]) -> Optional[str]:
    """ä»ä¸Šä¼ ç»“æœä¸­æå–åŒ…å« .zip çš„ä¸‹è½½åœ°å€ã€‚"""
    try:
        candidates = [
            upload_result.get("oss_url"),
            upload_result.get("upload_url"),
            (upload_result.get("oss_response") or {}).get("url"),
            upload_result.get("url"),
        ]
        for candidate in candidates:
            if candidate and ".zip" in str(candidate).lower():
                return candidate
    except Exception:
        pass
    return None

def _job_state_path(job_id: str) -> Path:
    return JOB_STATE_DIR / f"{job_id}.json"


def _persist_job_state(job_id: str) -> None:
    job = _JOB_REGISTRY.get(job_id)
    if not job:
        return
    job_copy = {k: v for k, v in job.items() if k not in {"agent"}}
    job_copy["updated_at"] = time.time()
    
    # åŒæ—¶ä¿å­˜åˆ°æœ¬åœ°å’Œ NAS
    path = _job_state_path(job_id)
    try:
        path.write_text(
            json.dumps(job_copy, ensure_ascii=False, indent=2), encoding="utf-8"
        )
    except Exception:
        pass
    
    # ä¿å­˜åˆ° NAS æ—¥å¿—
    try:
        log_manager = get_nas_log_manager()
        plan_id = job.get("plan_id")
        log_manager.create_job_log(job_id, plan_id)
        log_progress(job_id, status="registered", job_info=job_copy)
    except Exception:
        pass


def _load_job_states() -> None:
    for path in JOB_STATE_DIR.glob("*.json"):
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            job_id = data.get("job_id") or path.stem
            if not job_id:
                continue
            if data.get("status") == "running":
                data["status"] = "stopped"
                data["message"] = "ä»»åŠ¡åœ¨æœåŠ¡å™¨é‡å¯æ—¶ä¸­æ–­ï¼Œè¯·é‡æ–°æ‰§è¡Œ"
            _JOB_REGISTRY[job_id] = data
            progress_log = data.get("progress_log")
            if progress_log:
                if not os.path.isabs(progress_log):
                    progress_log = os.path.join(PROJECT_ROOT, progress_log)
                _PROGRESS_LOG_BY_JOB[job_id] = progress_log
                plan_id = data.get("plan_id")
                if plan_id and plan_id not in _PROGRESS_LOG_BY_ID:
                    _PROGRESS_LOG_BY_ID[plan_id] = progress_log
        except Exception:
            continue


_load_job_states()


def _load_job_state_from_disk(job_id: str) -> Optional[Dict[str, Any]]:
    path = _job_state_path(job_id)
    if not path.exists():
        return None
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        data.setdefault("job_id", job_id)
        return data
    except Exception:
        return None


def _context_cache_path(context_id: str) -> Path:
    return CONTEXT_CACHE_DIR / f"{context_id}.json"


def _load_context_cache() -> None:
    for path in CONTEXT_CACHE_DIR.glob("*.json"):
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            context_id = data.get("context_id") or path.stem
            if not context_id:
                continue
            data.setdefault("path", str(path))
            _CONTEXT_CACHE_BY_ID[context_id] = data
        except Exception:
            continue


_load_context_cache()


def _resolve_cached_context(context_id: Optional[str]) -> Optional[Dict[str, Any]]:
    if not context_id:
        return None
    cached = _CONTEXT_CACHE_BY_ID.get(context_id)
    if cached:
        return cached
    path = _context_cache_path(context_id)
    if path.exists():
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            data.setdefault("path", str(path))
            _CONTEXT_CACHE_BY_ID[context_id] = data
            return data
        except Exception:
            return None
    return None


def _resolve_project_directory(project_root: Optional[str], project_name: Optional[str] = None) -> str:
    """
    è§£æé¡¹ç›®ç›®å½•è·¯å¾„
    
    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•æˆ–å®Œæ•´è·¯å¾„
        project_name: é¡¹ç›®åç§°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        å®Œæ•´çš„é¡¹ç›®ç›®å½•è·¯å¾„
    """
    if project_root:
        # å¦‚æœæä¾›äº† project_root
        if os.path.isabs(project_root):
            # ç»å¯¹è·¯å¾„ï¼šç›´æ¥ä½¿ç”¨
            abs_path = project_root
        else:
            # ç›¸å¯¹è·¯å¾„ï¼šç›¸å¯¹äºé»˜è®¤æ ¹ç›®å½•
            # å¦‚æœ project_root çœ‹èµ·æ¥åƒé¡¹ç›®åï¼ˆä¸å«/ï¼‰ï¼Œåˆ™ä½œä¸ºå­ç›®å½•
            if '/' not in project_root and '\\' not in project_root:
                abs_path = os.path.join(DEFAULT_PROJECT_ROOT, project_root)
            else:
                # åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼Œä½œä¸ºç›¸å¯¹è·¯å¾„å¤„ç†
                abs_path = os.path.abspath(os.path.join(DEFAULT_PROJECT_ROOT, project_root))
    else:
        # æ²¡æœ‰æä¾› project_rootï¼Œä½¿ç”¨é»˜è®¤æ ¹ç›®å½•
        base = DEFAULT_PROJECT_ROOT
        
        # å¦‚æœæä¾›äº† project_name ä¸”å¯ç”¨äº†è‡ªåŠ¨åˆ›å»ºå­ç›®å½•
        if project_name and AUTO_CREATE_PROJECT_DIR:
            # æ¸…ç†é¡¹ç›®åç§°ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
            safe_name = "".join(c for c in project_name if c.isalnum() or c in (' ', '-', '_', '.'))
            safe_name = safe_name.strip().replace(' ', '_')
            if safe_name:
                abs_path = os.path.join(base, safe_name)
            else:
                abs_path = base
        else:
            abs_path = base
    
    # åˆ›å»ºç›®å½•
    os.makedirs(abs_path, exist_ok=True)
    return abs_path


def _build_agent(
    project_directory: str,
    model: Optional[str] = None,
    *,
    show_code: bool = False,
    verbose: bool = False,
    save_output: bool = False,
    force_single_page: bool = True,
) -> SmartWebAgent:
    return SmartWebAgent(
        project_directory=project_directory,
        model=model or DEFAULT_MODEL,
        show_code=show_code,
        verbose=verbose,
        save_output=save_output,
        force_single_page=force_single_page,
    )


def _prepare_agent_run(agent: SmartWebAgent, description: str) -> None:
    agent.execution_start_time = time.time()
    agent.execution_history = []
    agent.created_files = []
    agent.latest_user_request = description
    agent.current_plan = None


def _decode_plan(agent: SmartWebAgent, plan: Any) -> Dict[str, Any]:
    if isinstance(plan, str):
        plan = json.loads(plan)
    if not isinstance(plan, dict):
        raise ValueError("plan åº”è¯¥æ˜¯ JSON å¯¹è±¡")
    source_description = plan.pop("__source_description", None)
    plan.pop("__plan_id", None)
    plan.pop("__plan_path", None)
    if source_description:
        agent.latest_user_request = source_description
    repaired = agent._repair_plan_tools_sequence(plan)
    if not agent._validate_plan(repaired):
        raise ValueError("æ‰§è¡Œè®¡åˆ’ä¸å®Œæ•´æˆ–æ ¼å¼é”™è¯¯")
    agent.current_plan = repaired
    return repaired


def _create_plan(agent: SmartWebAgent, description: str) -> Dict[str, Any]:
    _prepare_agent_run(agent, description)
    enhanced = agent._enhance_user_input(description)
    plan = agent._get_execution_plan_with_retry(enhanced)
    if not plan:
        raise RuntimeError("æœªèƒ½ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œè¯·æ£€æŸ¥æè¿°æˆ–æ¨¡å‹é…ç½®")
    if not isinstance(plan, dict):
        raise ValueError("æ¨¡å‹è¿”å›çš„è®¡åˆ’æ ¼å¼å¼‚å¸¸ï¼Œåº”ä¸º JSON å¯¹è±¡")
    plan_id = uuid.uuid4().hex
    plan_path = PLAN_CACHE_DIR / f"{plan_id}.json"

    plan_for_storage = plan.copy()
    plan_for_storage["__source_description"] = description
    plan_path.write_text(
        json.dumps(plan_for_storage, ensure_ascii=False, indent=2), encoding="utf-8"
    )

    plan["__source_description"] = description

    _PLAN_CACHE_BY_ID[plan_id] = {
        "plan": plan,
        "project_directory": agent.project_directory,
        "description": description,
        "source_description": description,
        "path": str(plan_path),
        "plan_id": plan_id,
    }

    cache_key = (agent.project_directory, description)
    _PLAN_CACHE[cache_key] = {
        "plan": plan,
        "plan_id": plan_id,
        "description": description,
        "source_description": description,
    }

    plan["__plan_id"] = plan_id
    plan["__plan_path"] = str(plan_path)
    return plan


def _execute_plan(
    agent: SmartWebAgent,
    plan: Dict[str, Any],
    *,
    progress_log_path: Optional[str] = None,
    job_id: Optional[str] = None,
) -> Dict[str, Any]:
    progress_events: list[Dict[str, Any]] = []

    def _collect(event: Dict[str, Any]) -> None:
        if isinstance(event, dict):
            progress_events.append(event)
            
            # å†™å…¥æœ¬åœ°æ—¥å¿—
            if progress_log_path:
                try:
                    log_record = dict(event)
                    log_record.setdefault("timestamp", time.time())
                    with open(progress_log_path, "a", encoding="utf-8") as log_file:
                        log_file.write(json.dumps(log_record, ensure_ascii=False))
                        log_file.write("\n")
                except Exception:
                    pass
            
            # åŒæ—¶å†™å…¥ NAS æ—¥å¿—
            if job_id:
                try:
                    log_progress(job_id, **event)
                except Exception:
                    pass

    results = agent._execute_plan_with_recovery(
        plan,
        confirm_each_step=False,  # åå°æ‰§è¡Œæ¨¡å¼ï¼Œä¸éœ€è¦ç¡®è®¤
        progress_callback=_collect,
    )

    if any(
        r.get("status") == "success"
        and r.get("tool") in {"create_html_file", "create_css_file"}
        for r in results
    ):
        agent._run_consistency_review(plan)

    report = agent._generate_execution_report(plan, results)

    return {
        "report": report,
        "progress": progress_events,
        "results": results,
        "created_files": list(agent.created_files),
    }


# @mcp.tool()
# async def plan_site(
#     description: str,
#     project_root: Optional[str] = None,
#     context_id: Optional[str] = None,
#     context_content: Optional[str] = None,
# ) -> Dict[str, Any]:
#     """æ ¹æ®éœ€æ±‚ä¸ä¸Šä¸‹æ–‡ç”Ÿæˆç½‘é¡µæ„å»ºè®¡åˆ’ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½ä¼šäº¤ç”± AI æ¨¡å‹ç»Ÿä¸€åˆ†æã€‚
#
#     âš ï¸ é‡è¦å‚æ•°è¯´æ˜ï¼š
#     - description: ç½‘ç«™å»ºè®¾éœ€æ±‚æˆ–ç›®æ ‡ï¼Œä¾§é‡æè¿°è¦å®ç°çš„ç»“æ„ã€åŠŸèƒ½ã€é£æ ¼
#     - context_content: ğŸ”¥ æ ¸å¿ƒå‚æ•°ï¼ç½‘é¡µåˆ¶ä½œæ‰€éœ€çš„å…¨éƒ¨åŸå§‹æ–‡æœ¬æˆ–æ•°æ®
#       * ä¾‹å¦‚ï¼šåœ°å›¾æŸ¥è¯¢ç»“æœã€å’–å•¡é¦†ä¿¡æ¯ã€äº§å“æ•°æ®ã€è¥ä¸šæ—¶é—´ã€åœ°å€ç­‰
#       * è¿™æ˜¯æ¨¡å‹è·å–ä¸Šä¸‹æ–‡å†…å®¹çš„å”¯ä¸€å…¥å£ï¼Œä¸ä¼šè‡ªåŠ¨ä» description æ¨æ–­
#       * è¯·æŠŠéœ€è¦å¼•ç”¨çš„å®Œæ•´ä¿¡æ¯ç›´æ¥æ”¾å…¥è¯¥å‚æ•°
#       * æ”¯æŒå„ç§æ ¼å¼ï¼šæ–‡æœ¬ã€JSONå­—ç¬¦ä¸²ã€ç»“æ„åŒ–æ•°æ®ç­‰
#
#     å…¶ä»–å‚æ•°ï¼š
#     - project_root: å¯é€‰ï¼Œè‡ªå®šä¹‰é¡¹ç›®æ ¹ç›®å½•ï¼›ç¼ºçœæ—¶ä½¿ç”¨é»˜è®¤ç›®å½•
#     - context_id: å¯é€‰ï¼Œå¼•ç”¨å·²ç¼“å­˜çš„ä¸Šä¸‹æ–‡å¿«ç…§ä»¥å¤ç”¨å†å²èµ„æ–™
#
#     è¿”å›å€¼è¯´æ˜ï¼š
#     - status: æ“ä½œçŠ¶æ€ ("success" æˆ– "error")
#     - plan_id: ç”Ÿæˆçš„è®¡åˆ’å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåç»­æ‰§è¡Œ
#     - plan_path: è®¡åˆ’ JSON æ–‡ä»¶çš„ä¿å­˜è·¯å¾„
#     - project_directory: è§£æåçš„é¡¹ç›®ç›®å½•è·¯å¾„
#     - model: ä½¿ç”¨çš„ AI æ¨¡å‹åç§°
#
#     ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
#     å¦‚æœä½ å…ˆç”¨å…¶ä»–å·¥å…·ï¼ˆå¦‚åœ°å›¾æŸ¥è¯¢ï¼‰è·å–äº†æ•°æ®ï¼Œè¯·å°†ç»“æœå®Œæ•´ä¼ é€’ç»™ context_contentï¼Œ
#     è¿™æ · AI å°±èƒ½åŸºäºçœŸå®æ•°æ®ç”Ÿæˆä¸ªæ€§åŒ–ç½‘ç«™ã€‚
#     """
#     try:
#         project_dir = _resolve_project_directory(project_root)
#         agent = _build_agent(project_dir)
#
#         # ç›´æ¥ä½¿ç”¨ descriptionï¼Œå¦‚æœæœ‰é¢å¤–çš„ä¸Šä¸‹æ–‡åˆ™é™„åŠ 
#         final_description = description
#
#         # å¦‚æœæä¾›äº† context_contentï¼Œé™„åŠ åˆ°æè¿°ä¸­
#         if context_content:
#             final_description = f"{description}\n\nã€é™„åŠ å†…å®¹ã€‘\n{context_content}"
#
#         # å¦‚æœæä¾›äº† context_idï¼Œå°è¯•è·å–ç¼“å­˜çš„å†…å®¹
#         elif context_id:
#             cached = _resolve_cached_context(context_id)
#             if cached:
#                 cached_content = cached.get("context")
#                 if cached_content:
#                     # å°è¯•è§£æJSONæ ¼å¼çš„å¢å¼ºæ•°æ®
#                     try:
#                         enhanced_data = json.loads(cached_content)
#                         if "original_content" in enhanced_data:
#                             cached_content = enhanced_data["original_content"]
#                     except (json.JSONDecodeError, TypeError):
#                         pass  # ä½¿ç”¨åŸå§‹å†…å®¹
#
#                     if cached_content:
#                         final_description = f"{description}\n\nã€ç¼“å­˜å†…å®¹ã€‘\n{cached_content}"
#
#         # è®© AI æ¨¡å‹ç›´æ¥å¤„ç†æ‰€æœ‰å†…å®¹
#         plan = await asyncio.to_thread(_create_plan, agent, final_description)
#         plan_id = plan.pop("__plan_id", None)
#         plan_path = plan.pop("__plan_path", None)
#
#         return {
#             "status": "success",
#             "plan_id": plan_id,
#             "plan_path": plan_path,
#             "project_directory": project_dir,
#             "model": agent.model,
#             "message": "è®¡åˆ’å·²ç”Ÿæˆï¼ŒAIæ¨¡å‹å·²åˆ†ææ‰€æä¾›çš„å…¨éƒ¨å†…å®¹"
#         }
#     except Exception as exc:
#         return {
#             "status": "error",
#             "message": str(exc),
#             "traceback": traceback.format_exc(),
#         }


@mcp.tool()
async def execute_plan(
    plan_id: str,
    project_root: Optional[str] = None,
    # auto_plan: bool = False,  # å·²ç¦ç”¨ï¼Œæ²¡æœ‰å®é™…ä½œç”¨
    # confirm_each_step: bool = False,  # åå°æ‰§è¡Œæ¨¡å¼ä¸‹ç”¨æˆ·æ— æ³•äº¤äº’ç¡®è®¤
    # show_code: bool = False,  # åå°æ‰§è¡Œæ—¶ç”¨æˆ·çœ‹ä¸åˆ°è¾“å‡º
    # verbose: bool = False,  # åå°æ‰§è¡Œæ—¶è¯¦ç»†æ—¥å¿—æ„ä¹‰ä¸å¤§
    # save_output: bool = True,  # å·²å›ºå®šä¸º Trueï¼Œå§‹ç»ˆåˆ›å»ºè¿›åº¦æ—¥å¿—
    progress_log: Optional[str] = None,
) -> Dict[str, Any]:
    """æ‰§è¡Œç½‘é¡µæ„å»ºè®¡åˆ’ï¼Œå§‹ç»ˆä»¥åå°æ¨¡å¼è¿è¡Œã€‚

    å‚æ•°è¯¦ç»†è¯´æ˜ï¼š
    - plan_id: è®¡åˆ’çš„å”¯ä¸€æ ‡è¯†ç¬¦
        ç”± create_simple_site å·¥å…·è¿”å›çš„è®¡åˆ’IDï¼Œç”¨äºä»ç¼“å­˜æˆ–æ–‡ä»¶ç³»ç»Ÿä¸­æŸ¥æ‰¾å¯¹åº”çš„æ‰§è¡Œè®¡åˆ’ã€‚
        ä¾‹å¦‚ï¼š"a1b2c3d4e5f6..." è¿™æ ·çš„32ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ã€‚

    - project_root: ç½‘ç«™æ–‡ä»¶ç”Ÿæˆçš„ç›®æ ‡ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        æŒ‡å®šé¡¹ç›®æ–‡ä»¶çš„è¾“å‡ºä½ç½®ï¼Œå¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ã€‚
        ä¾‹å¦‚ï¼š"/path/to/my/website" æˆ– "./my-project"
        å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ›å»ºã€‚
        å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è®¡åˆ’ä¸­ä¿å­˜çš„é¡¹ç›®ç›®å½•ã€‚

    - progress_log: è‡ªå®šä¹‰è¿›åº¦æ—¥å¿—æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        å¦‚æœæŒ‡å®šï¼šä½¿ç”¨è¯¥è·¯å¾„ä¿å­˜è¿›åº¦æ—¥å¿—ï¼ˆJSONLæ ¼å¼ï¼‰
        å¦‚æœæœªæŒ‡å®šï¼šè‡ªåŠ¨åœ¨ ~/.mcp/make_web/progress_logs ç›®å½•åˆ›å»ºæ—¶é—´æˆ³å‘½åçš„æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        è·¯å¾„å¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äº project_root çš„ç›¸å¯¹è·¯å¾„

    æ‰§è¡Œæµç¨‹ï¼š
    - ä»»åŠ¡åœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å› job_id å’Œ progress_log è·¯å¾„
    - ä»»åŠ¡å®Œæˆåï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ¨é€é€šçŸ¥ç»™ç”¨æˆ·ï¼ŒåŒ…å«æ‰§è¡ŒæŠ¥å‘Šã€ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨å’Œä¸Šä¼ ç»“æœ
    - ç³»ç»Ÿä¼šè‡ªåŠ¨è®°å½•è¯¦ç»†çš„æ‰§è¡Œæ­¥éª¤å’Œç»“æœåˆ°è¿›åº¦æ—¥å¿—æ–‡ä»¶
    - å¦‚éœ€å®æ—¶äº†è§£ä»»åŠ¡è¿›å±•æˆ–è°ƒè¯•ï¼Œå¯ä½¿ç”¨ get_progress(job_id=...) å·¥å…·æ‰‹åŠ¨æŸ¥è¯¢çŠ¶æ€
    """
    try:
        # å¦‚æœæ²¡æœ‰æä¾›project_rootï¼Œå°è¯•ä»ç¼“å­˜ä¸­è·å–
        if not project_root:
            cached_by_id = _PLAN_CACHE_BY_ID.get(plan_id)
            if cached_by_id and cached_by_id.get("project_directory"):
                project_root = cached_by_id["project_directory"]
            else:
                # å°è¯•ä»æ–‡ä»¶ä¸­è¯»å–
                possible_paths = [
                    PLAN_CACHE_DIR / f"{plan_id}.json",
                    PLAN_CACHE_DIR / f"simple_site_plan_{plan_id}.json",
                ]
                for path in possible_paths:
                    if path.exists():
                        try:
                            plan_data = json.loads(path.read_text(encoding="utf-8"))
                            project_root = plan_data.get("project_directory")
                            if project_root:
                                break
                        except Exception:
                            pass

                if not project_root:
                    # ä½¿ç”¨é»˜è®¤æ ¹ç›®å½•ï¼Œä½†ä¸æ·»åŠ å­ç›®å½•
                    project_root = None

        # ä»è®¡åˆ’ä¸­è·å–é¡¹ç›®åç§°ï¼ˆå¦‚æœæœ‰ï¼‰
        project_name = None
        if plan_id:
            cached_plan = _PLAN_CACHE_BY_ID.get(plan_id)
            if cached_plan:
                # å°è¯•ä»ç¼“å­˜ä¸­è·å–é¡¹ç›®åç§°
                project_name = cached_plan.get("site_title") or cached_plan.get(
                    "project_name"
                )

        project_dir = _resolve_project_directory(project_root, project_name)

        # è¿›åº¦æ—¥å¿—å§‹ç»ˆå¯ç”¨
        if progress_log:
            # ç”¨æˆ·æŒ‡å®šäº†è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„
            progress_log_path = (
                progress_log
                if os.path.isabs(progress_log)
                else os.path.join(project_dir, progress_log)
            )
        else:
            # è‡ªåŠ¨ç”Ÿæˆæ—¥å¿—æ–‡ä»¶
            progress_log_path = os.path.join(
                PROGRESS_LOG_DIR, f"agent_progress_{int(time.time())}.jsonl"
            )

        # å°è¯•åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        if progress_log_path:
            try:
                Path(progress_log_path).parent.mkdir(parents=True, exist_ok=True)
                Path(progress_log_path).write_text("", encoding="utf-8")
            except Exception:
                progress_log_path = None

        # save_output å›ºå®šä¸º True
        agent = _build_agent(
            project_dir,
            save_output=True,
        )

        # é€šè¿‡ plan_id æŸ¥è¯¢è®¡åˆ’
        cached_by_id = _PLAN_CACHE_BY_ID.get(plan_id)

        # å°è¯•å¤šç§æ–‡ä»¶å‘½åæ ¼å¼
        possible_paths = [
            PLAN_CACHE_DIR / f"{plan_id}.json",  # æ ‡å‡†æ ¼å¼
            PLAN_CACHE_DIR
            / f"simple_site_plan_{plan_id}.json",  # create_simple_siteæ ¼å¼
        ]

        plan_path = None
        for path in possible_paths:
            if path.exists():
                plan_path = path
                break

        if not cached_by_id and plan_path:
            try:
                cached_plan_file = json.loads(plan_path.read_text(encoding="utf-8"))
                source_description = None
                if isinstance(cached_plan_file, dict):
                    source_description = cached_plan_file.get("__source_description")
                    # æå–å®é™…çš„planéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ–‡ä»¶å†…å®¹
                    actual_plan = cached_plan_file.get("plan")
                    if not actual_plan:
                        # å¦‚æœæ²¡æœ‰planå­—æ®µï¼Œå¯èƒ½æ˜¯æ—§æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶å†…å®¹
                        actual_plan = cached_plan_file

                cached_by_id = {
                    "plan": actual_plan,  # ä¼ é€’å®é™…çš„planå†…å®¹
                    "project_directory": project_dir,
                    "plan_id": plan_id,
                    "description": source_description
                    or cached_plan_file.get("description"),
                    "source_description": source_description,
                }
                _PLAN_CACHE_BY_ID[plan_id] = cached_by_id
            except Exception:
                cached_by_id = None

        if not cached_by_id:
            raise ValueError(
                f"æœªæ‰¾åˆ° plan_id '{plan_id}' å¯¹åº”çš„è®¡åˆ’ï¼Œè¯·å…ˆè°ƒç”¨ create_simple_site ç”Ÿæˆè®¡åˆ’"
            )

        plan_dict = _decode_plan(agent, cached_by_id.get("plan"))
        effective_description = (
            cached_by_id.get("source_description")
            or cached_by_id.get("description")
            or plan_dict.get("task_analysis")
            or plan_dict.get("project_name")
            or plan_dict.get("site_type")
            or "Web Project Execution"
        )

        _prepare_agent_run(agent, effective_description)
        agent.current_plan = plan_dict

        # å§‹ç»ˆä»¥åå°æ¨¡å¼æ‰§è¡Œ
        job_id = uuid.uuid4().hex
        job_info = {
            "job_id": job_id,
            "status": "running",
            "plan_id": plan_id,
            "description": effective_description,
            "project_directory": project_dir,
            "model": agent.model,
            "progress_log": progress_log_path,
            "deployment_env": _resolve_edgeone_deploy_env(),
            "started_at": time.time(),
            "updated_at": time.time(),
        }
        _JOB_REGISTRY[job_id] = job_info

        if plan_id and progress_log_path:
            _PROGRESS_LOG_BY_ID[plan_id] = progress_log_path
        if progress_log_path:
            _PROGRESS_LOG_BY_JOB[job_id] = progress_log_path

        _persist_job_state(job_id)

        asyncio.create_task(
            _run_execution_job(
                job_id,
                agent,
                plan_dict,
                progress_log_path=progress_log_path,
            )
        )

        message = (
            "æ‰§è¡Œå·²åœ¨åå°å¯åŠ¨ï¼ˆå«è‡ªåŠ¨ä¸Šä¼ ï¼‰ï¼šè°ƒç”¨ get_progress(job_id='{}', limit=20) "
            "æˆ–ä¼ å…¥ progress_log='{}' å¯è·å–å®æ—¶è¿›åº¦ä¸ä¸Šä¼ ç»“æœ"
        ).format(job_id, progress_log_path or "<æœªå¯ç”¨è¿›åº¦æ—¥å¿—>")

        return {
            "status": "started",
            "job_id": job_id,
            "plan_id": plan_id,
            "progress_log": progress_log_path,
            "upload_url": None,
            "web_url": None,
            "deployment_env": job_info["deployment_env"],
            "message": message,
        }
    except Exception as exc:
        return {
            "status": "error",
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }


async def _run_execution_job(
    job_id: str,
    agent: SmartWebAgent,
    plan_dict: Dict[str, Any],
    *,
    progress_log_path: Optional[str],
) -> None:
    job_info = _JOB_REGISTRY.get(job_id)
    if not job_info:
        return

    try:
        result = await asyncio.to_thread(
            _execute_plan,
            agent,
            plan_dict,
            progress_log_path=progress_log_path,
        )
        job_info["status"] = "completed"
        job_info["result"] = result
        job_info["completed_at"] = time.time()
        _persist_job_state(job_id)

        await _handle_auto_upload(job_id, job_info, progress_log_path)

    except Exception as exc:
        job_info["status"] = "failed"
        job_info["error"] = {
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }
        _persist_job_state(job_id)
    finally:
        job_info["updated_at"] = time.time()
        _persist_job_state(job_id)


async def _handle_auto_upload(
    job_id: str,
    job_info: Dict[str, Any],
    progress_log_path: Optional[str],
) -> None:
    project_dir = job_info.get("project_directory")
    if not project_dir:
        return

    upload_target = project_dir

    created_files = (job_info.get("result") or {}).get("created_files") or []
    for file_path in created_files:
        if not file_path:
            continue
        try:
            candidate_path = file_path
            if not os.path.isabs(candidate_path):
                candidate_path = os.path.join(project_dir, candidate_path)
            if not os.path.exists(candidate_path):
                continue
            candidate_dir = (
                candidate_path if os.path.isdir(candidate_path) else os.path.dirname(candidate_path)
            )
            rel_path = os.path.relpath(candidate_dir, project_dir)
            if rel_path == "." or rel_path.startswith(".."):
                continue
            top_level = rel_path.split(os.sep, 1)[0]
            top_level_dir = os.path.join(project_dir, top_level)
            if os.path.isdir(top_level_dir):
                upload_target = top_level_dir
                break
        except Exception:
            continue

    job_info["upload_status"] = "uploading"
    _persist_job_state(job_id)

    try:
        upload_func = getattr(upload_project_to_mcp_server, "fn", None)
        if not callable(upload_func):
            raise RuntimeError("upload_project_to_mcp_server ç¼ºå°‘å¯è°ƒç”¨å®ç°")

        upload_result = await upload_func(folder_path=upload_target)

        job_info["upload_result"] = upload_result
        job_info["upload_status"] = upload_result.get("status")
        if upload_result.get("deployment_env"):
            job_info["deployment_env"] = upload_result["deployment_env"]

        if upload_result.get("status") == "success":
            zip_url = _extract_zip_url(upload_result)
            web_url = (
                upload_result.get("web_url")
                or (upload_result.get("result") or {}).get("url")
            )
            if zip_url:
                job_info["upload_url"] = zip_url
            if web_url:
                job_info["web_url"] = web_url
            job_info["upload_completed_at"] = time.time()
            job_info["uploaded_directory"] = upload_target

            if progress_log_path:
                upload_event = {
                    "timestamp": time.time(),
                    "type": "upload_completed",
                    "status": "success",
                    "web_url": web_url,
                    "upload_url": zip_url,
                    "oss_url": zip_url,
                    "deployment_env": job_info.get("deployment_env"),
                    "message": upload_result.get("message"),
                    "uploaded_directory": upload_target,
                }
                try:
                    with open(progress_log_path, "a", encoding="utf-8") as log_file:
                        log_file.write(json.dumps(upload_event, ensure_ascii=False))
                        log_file.write("\n")
                except Exception:
                    pass

    except Exception as exc:
        job_info["upload_status"] = "failed"
        job_info["upload_error"] = str(exc)
    finally:
        _persist_job_state(job_id)


@mcp.tool()
async def create_simple_site(
    description: str,
    site_title: str = "æˆ‘çš„ç½‘ç«™",
    context_content: Optional[str] = None,
) -> Dict[str, Any]:
    """ä½¿ç”¨AIåˆ†æéœ€æ±‚ï¼Œç”Ÿæˆç®€å•ä½†ç¾è§‚çš„ç½‘ç«™è®¡åˆ’ã€‚

    å‚æ•°è¯´æ˜ï¼š
    - description: ç½‘ç«™éœ€æ±‚æè¿°ï¼Œä¾‹å¦‚"ä¸ªäººä½œå“å±•ç¤ºç½‘ç«™"ã€"å°é¤å…å®˜ç½‘"ã€"åšå®¢ç½‘ç«™"ç­‰
    - site_title: ç½‘ç«™æ ‡é¢˜ï¼Œé»˜è®¤ä¸º"æˆ‘çš„ç½‘ç«™"
    - context_content: å¯é€‰ï¼Œç”¨äºä¼ é€’ç½‘é¡µåˆ¶ä½œæ‰€éœ€çš„æ‰€æœ‰åŸå§‹æ•°æ®å†…å®¹
      * ä¾‹å¦‚ï¼šå’–å•¡é¦†åˆ—è¡¨ã€äº§å“ä»‹ç»ã€èœå•å†…å®¹ã€åœ°å€ä¿¡æ¯ã€è¥ä¸šæ—¶é—´ç­‰
      * è¿™æ˜¯AIè·å–å…·ä½“ä¸šåŠ¡ä¿¡æ¯çš„å”¯ä¸€æ¸ é“ï¼Œè¯·åŠ¡å¿…å°†æŸ¥è¯¢åˆ°çš„è¯¦ç»†ä¿¡æ¯å®Œæ•´ä¼ å…¥
      * å¦‚æœæœ‰åœ°å›¾æŸ¥è¯¢ç»“æœã€APIè¿”å›æ•°æ®ç­‰ï¼Œéƒ½åº”è¯¥æ”¾åœ¨è¿™ä¸ªå‚æ•°ä¸­
      * æ ¼å¼å¯ä»¥æ˜¯æ–‡æœ¬ã€JSONå­—ç¬¦ä¸²æˆ–ç»“æ„åŒ–æ•°æ®çš„å­—ç¬¦ä¸²è¡¨ç¤º

    è¿”å›å€¼è¯´æ˜ï¼š
    - status: æ“ä½œçŠ¶æ€ ("success" æˆ– "error")
    - plan_id: ç”Ÿæˆçš„è®¡åˆ’å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåç»­æ‰§è¡Œ
    - plan_path: è®¡åˆ’ JSON æ–‡ä»¶çš„ä¿å­˜è·¯å¾„
    - project_directory: è§£æåçš„é¡¹ç›®ç›®å½•è·¯å¾„
    - plan: ç”Ÿæˆçš„ç®€åŒ–æ‰§è¡Œè®¡åˆ’æ¦‚è§ˆ
    - context_id: ä¸Šä¸‹æ–‡ç¼“å­˜IDï¼ˆå¦‚æœä½¿ç”¨äº†ä¸Šä¸‹æ–‡ï¼‰

    ä½¿ç”¨æµç¨‹ï¼š
    1. è°ƒç”¨æ­¤å·¥å…·ç”Ÿæˆè®¡åˆ’ï¼Œè·å¾— plan_id
    2. ä½¿ç”¨ plan_id è°ƒç”¨ execute_plan æ‰§è¡Œæ„å»º
    3. æ„å»ºå®Œæˆåä¼šè‡ªåŠ¨æ¨é€é€šçŸ¥ç»™ç”¨æˆ·ï¼ŒåŒ…å«é¡¹ç›®æ–‡ä»¶å’Œè®¿é—®é“¾æ¥

    ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
    å¦‚æœä½ æœ‰åœ°å›¾æŸ¥è¯¢ç»“æœã€APIæ•°æ®ç­‰ï¼Œè¯·å°†å®Œæ•´ä¿¡æ¯ä¼ é€’ç»™ context_content å‚æ•°ï¼Œ
    è¿™æ ·AIå°±èƒ½åŸºäºçœŸå®æ•°æ®æ¥ç”Ÿæˆä¸ªæ€§åŒ–çš„ç½‘ç«™å†…å®¹ã€‚
    """
    try:
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        used_model = DEFAULT_MODEL

        # ä½¿ç”¨æ–°çš„è·¯å¾„è§£æé€»è¾‘ï¼Œé»˜è®¤åœ¨å…±äº«ç›®å½•ä¸‹æŒ‰æ ‡é¢˜åˆ›å»ºé¡¹ç›®
        project_directory = _resolve_project_directory(None, site_title)

        # å¤„ç†ä¸Šä¸‹æ–‡
        context_data = ""
        actual_context_id: Optional[str] = None

        if context_content:
            # ä½¿ç”¨æ–°æä¾›çš„ä¸Šä¸‹æ–‡å†…å®¹
            context_data = context_content
            # ç”Ÿæˆæ–°çš„ä¸Šä¸‹æ–‡IDå¹¶ç¼“å­˜
            actual_context_id = str(uuid.uuid4())
            _CONTEXT_CACHE_BY_ID[actual_context_id] = {
                "content": context_content,
                "created_at": time.time(),
                "site_title": site_title,
                "description": description,
            }

        # åˆ›å»ºAIä»£ç†è¿›è¡Œåˆ†æ
        agent = SmartWebAgent(
            project_directory=project_directory,
            model=used_model,
            show_code=False,
            verbose=False,
            force_single_page=True,
        )

        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡å†…å®¹ï¼Œå°†å…¶æ•´åˆåˆ°æè¿°ä¸­
        enhanced_description = description
        if context_data:
            enhanced_description = f"""{description}

ã€å¿…é¡»ä½¿ç”¨çš„å…·ä½“æ•°æ®å†…å®¹ã€‘ï¼š
{context_data}

ã€é‡è¦æç¤ºã€‘ï¼šä¸Šè¿°æ•°æ®æ˜¯çœŸå®çš„ä¸šåŠ¡æ•°æ®ï¼Œå¿…é¡»å®Œæ•´å‡†ç¡®åœ°å±•ç¤ºåœ¨ç½‘é¡µä¸­ï¼Œä¸è¦ç”Ÿæˆè™šæ„çš„ç¤ºä¾‹å†…å®¹ã€‚"""

        # æ„å»ºæ”¹è¿›çš„æç¤ºè¯ï¼Œå¼ºè°ƒä½¿ç”¨çœŸå®æ•°æ®
        simple_prompt = f"""è¯·ä¸ºä»¥ä¸‹éœ€æ±‚åˆ›å»ºä¸€ä¸ªç½‘ç«™ï¼Œå¹¶ä¸¥æ ¼ä½¿ç”¨æä¾›çš„çœŸå®æ•°æ®ï¼š

**ç½‘ç«™æ ‡é¢˜**: {site_title}
**å…·ä½“éœ€æ±‚å’Œæ•°æ®**: 
{enhanced_description}

**æ‰§è¡Œè¦æ±‚**ï¼š
1. ã€æ•°æ®è¦æ±‚ã€‘å¦‚æœæä¾›äº†å…·ä½“æ•°æ®ï¼ˆå¦‚åº—é“ºåˆ—è¡¨ã€äº§å“ä¿¡æ¯ç­‰ï¼‰ï¼Œå¿…é¡»100%ä½¿ç”¨è¿™äº›çœŸå®æ•°æ®ï¼Œä¸è¦åˆ›å»ºè™šæ„å†…å®¹
2. ã€å†…å®¹å±•ç¤ºã€‘å°†æ‰€æœ‰æä¾›çš„æ•°æ®é¡¹å®Œæ•´å±•ç¤ºï¼Œä½¿ç”¨åˆé€‚çš„å¸ƒå±€ï¼ˆå¦‚å¡ç‰‡ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ï¼‰
3. ã€æ ·å¼è®¾è®¡ã€‘ä¿æŒç®€æ´ç¾è§‚ï¼Œä½¿ç”¨å“åº”å¼è®¾è®¡
4. ã€ä»£ç é™åˆ¶ã€‘CSSä¸è¶…è¿‡300è¡Œï¼Œé¿å…å¤æ‚ç‰¹æ•ˆ
5. ã€åŠŸèƒ½å®ç°ã€‘åŒ…å«åŸºç¡€äº¤äº’åŠŸèƒ½ï¼ˆå¯¼èˆªã€æ»šåŠ¨ç­‰ï¼‰

**ç‰¹åˆ«å¼ºè°ƒ**ï¼š
- å½“åˆ›å»ºHTMLå†…å®¹æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šé¢æä¾›çš„çœŸå®æ•°æ®
- ä¸è¦ç”Ÿæˆ"ç¤ºä¾‹å®¢æˆ·è¯„ä»·"ã€"è™šæ‹Ÿå®šä»·æ–¹æ¡ˆ"ç­‰å ä½å†…å®¹
- å¦‚æœæ˜¯å’–å•¡é¦†åˆ—è¡¨ï¼Œå°±å±•ç¤ºçœŸå®çš„å’–å•¡é¦†åç§°å’Œåœ°å€
- å¦‚æœæ˜¯äº§å“ä¿¡æ¯ï¼Œå°±å±•ç¤ºçœŸå®çš„äº§å“æ•°æ®
- æ¯ä¸ªcreate_html_fileæˆ–add_content_sectionå·¥å…·è°ƒç”¨æ—¶ï¼Œéƒ½è¦åŒ…å«çœŸå®æ•°æ®

è¯·ç”Ÿæˆ3-6ä¸ªæ­¥éª¤çš„æ‰§è¡Œè®¡åˆ’ï¼Œç¡®ä¿æ¯ä¸ªæ­¥éª¤éƒ½èƒ½æ­£ç¡®ä½¿ç”¨æä¾›çš„æ•°æ®ã€‚
"""

        # ç”Ÿæˆç®€åŒ–è®¡åˆ’ï¼ˆä»…è§„åˆ’ï¼Œä¸æ‰§è¡Œï¼‰
        # ä¼ é€’å¼ºåŒ–åçš„æç¤ºè¯ï¼Œç¡®ä¿AIä½¿ç”¨çœŸå®æ•°æ®
        # ä½¿ç”¨æç¤ºè¯å¢å¼ºå™¨è¿›ä¸€æ­¥å¼ºåŒ–
        final_prompt = enhance_prompt_for_real_data(simple_prompt, context_data)
        plan = agent._get_execution_plan(final_prompt)

        # åœ¨è®¡åˆ’ä¸­æ ‡è®°ä¸ºç®€å•ç½‘ç«™ç±»å‹å’Œç›¸å…³ä¿¡æ¯
        plan["site_type"] = "simple"
        plan["complexity"] = "ç®€å•ä½†ç¾è§‚"
        plan["css_limit"] = "ä¸è¶…è¿‡300è¡Œ"
        plan["model_used"] = used_model
        plan["has_context"] = bool(context_data)
        if actual_context_id:
            plan["context_id"] = actual_context_id

        # ç”Ÿæˆå”¯ä¸€çš„è®¡åˆ’ID
        plan_id = str(uuid.uuid4())

        # æ„å»ºå®Œæ•´çš„æºæè¿°ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        # ä½¿ç”¨enhanced_descriptionä»¥ç¡®ä¿æ•°æ®è¢«ä¼ é€’
        source_description = enhanced_description

        # åœ¨è®¡åˆ’ä¸­æ·»åŠ æºæè¿°å­—æ®µ
        plan["__source_description"] = source_description
        plan["__plan_id"] = plan_id

        # ä¿å­˜è®¡åˆ’åˆ°ç¼“å­˜ï¼ˆç»“æ„ä¸ create_simple_site ä¿æŒä¸€è‡´ï¼Œä¾¿äº execute_plan å¤ç”¨é€»è¾‘ï¼‰
        cached_entry = {
            "plan": plan,
            "project_directory": project_directory,
            "description": description,
            "source_description": source_description,
            "site_title": site_title,
            "plan_id": plan_id,
        }

        if actual_context_id:
            cached_entry["context_id"] = actual_context_id

        _PLAN_CACHE_BY_ID[plan_id] = cached_entry
        cache_key = (project_directory, description)
        _PLAN_CACHE[cache_key] = cached_entry

        # å°†ä¸Šä¸‹æ–‡ä¿¡æ¯å…³è”åˆ°è®¡åˆ’
        if actual_context_id:
            _CONTEXT_ID_BY_PLAN[plan_id] = actual_context_id

        # ä¿å­˜è®¡åˆ’åˆ°æ–‡ä»¶
        plan_filename = f"simple_site_plan_{plan_id}.json"
        plan_path = PLAN_CACHE_DIR / plan_filename

        try:
            # æ„å»ºå®Œæ•´çš„æºæè¿°ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
            source_description = description
            if context_data:
                source_description = f"{description}\n\nã€é™„åŠ å†…å®¹ã€‘\n{context_data}"

            plan_data = {
                "plan_id": plan_id,
                "site_title": site_title,
                "description": description,
                "project_directory": project_directory,
                "model": used_model,
                "plan_type": "simple_site",
                "created_at": time.time(),
                "plan": plan,
                "__source_description": source_description,  # æ·»åŠ å®Œæ•´çš„æºæè¿°å­—æ®µ
            }
            if actual_context_id:
                plan_data["context_id"] = actual_context_id
                plan_data["has_context"] = True

            with open(plan_path, "w", encoding="utf-8") as f:
                json.dump(plan_data, f, ensure_ascii=False, indent=2)
        except Exception:
            # æ–‡ä»¶ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass

        # ç”Ÿæˆè®¡åˆ’æ¦‚è§ˆ
        tools_sequence = plan.get("tools_sequence", [])
        plan_overview = {
            "description": plan.get("description", "ç®€å•ç½‘ç«™æ„å»ºè®¡åˆ’"),
            "steps": len(tools_sequence),
            "step_list": [
                step.get("description", step.get("tool", "")) for step in tools_sequence
            ],
            "estimated_files": plan.get("estimated_files", "3-5ä¸ªæ–‡ä»¶"),
            "features": plan.get("features", ["å“åº”å¼è®¾è®¡", "è½»é‡çº§æ ·å¼", "åŸºç¡€äº¤äº’"]),
            "has_context": bool(context_data),
        }

        result = {
            "status": "success",
            "message": f"ç®€å•ç½‘ç«™è®¡åˆ’ç”ŸæˆæˆåŠŸï¼ŒåŒ…å«{len(tools_sequence)}ä¸ªæ‰§è¡Œæ­¥éª¤",
            "plan_id": plan_id,
            "plan_path": str(plan_path),
            "project_directory": project_directory,
            "plan": plan_overview,
            "model_used": used_model,
            "next_step": f"ä½¿ç”¨ execute_plan(plan_id='{plan_id}') å¼€å§‹æ„å»ºç½‘ç«™",
        }

        if actual_context_id:
            result["context_id"] = actual_context_id
            result["context_used"] = True

        return result

    except Exception as exc:
        return {
            "status": "error",
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }


@mcp.tool()
async def get_progress(
    plan_id: Optional[str] = None,
    job_id: Optional[str] = None,
    log_path: Optional[str] = None,
    limit: int = 20,
) -> Dict[str, Any]:
    """æŸ¥è¯¢ç½‘é¡µæ„å»ºä»»åŠ¡çš„æ‰§è¡Œè¿›åº¦å’ŒçŠ¶æ€ã€‚

    å‚æ•°è¯´æ˜ï¼š
    - plan_id: create_simple_site è¿”å›çš„ IDï¼Œå¯å®šä½é»˜è®¤çš„è¿›åº¦æ—¥å¿—ã€‚
    - job_id: execute_plan(background=true) è¿”å›çš„ä»»åŠ¡ IDï¼Œå¯ç›´æ¥æŸ¥è¯¢åå°ä»»åŠ¡çŠ¶æ€ã€‚
    - log_path: è¿›åº¦æ—¥å¿— JSONL æ–‡ä»¶çš„è·¯å¾„ï¼ˆç»å¯¹æˆ–ç›¸å¯¹ï¼‰ï¼Œä¼˜å…ˆçº§æœ€é«˜ã€‚
    - limit: è¿”å›çš„æœ€æ–°äº‹ä»¶æ•°é‡ï¼ˆé»˜è®¤ 20 æ¡ï¼‰ã€‚

    ä½¿ç”¨æç¤ºï¼š
    1. æ¨èç›´æ¥ä¼ å…¥ execute_plan è¿”å›çš„ job_id å’Œ progress_logã€‚
    2. è‹¥æœªæä¾› log_pathï¼Œæœ¬å·¥å…·ä¼šæŒ‰ job_id -> plan_id çš„é¡ºåºå°è¯•æŸ¥æ‰¾å·²ç¼“å­˜çš„æ—¥å¿—ã€‚
    3. è¿”å›å†…å®¹åŒ…æ‹¬æœ€æ–°äº‹ä»¶åˆ—è¡¨ã€æ—¥å¿—è·¯å¾„ä»¥åŠï¼ˆè‹¥æœ‰ï¼‰ä»»åŠ¡å¿«ç…§æˆ–ç»“æœæ‘˜è¦ï¼Œå¯ç”¨äºæŒç»­è¿½è¸ªæ„å»ºè¿›åº¦ã€‚
    """

    try:
        # SSE æ¨¡å¼ä¼˜åŒ–ï¼šä½¿ç”¨å¼‚æ­¥ I/O
        loop = asyncio.get_event_loop()
        
        if limit <= 0:
            limit = 20

        job_info = None
        resolved_path = None

        if job_id:
            job_info = _JOB_REGISTRY.get(job_id)
            if not job_info:
                disk_state = _load_job_state_from_disk(job_id)
                if disk_state:
                    _JOB_REGISTRY[job_id] = disk_state
                    job_info = disk_state
                    progress_log_from_state = job_info.get("progress_log")
                    if progress_log_from_state:
                        progress_log_str = str(progress_log_from_state)
                        _PROGRESS_LOG_BY_JOB[job_id] = progress_log_str
                        plan_in_state = job_info.get("plan_id")
                        if plan_in_state:
                            _PROGRESS_LOG_BY_ID.setdefault(
                                plan_in_state, progress_log_str
                            )
            if job_info and not plan_id:
                plan_id = job_info.get("plan_id")
            if job_id in _PROGRESS_LOG_BY_JOB:
                resolved_path = _PROGRESS_LOG_BY_JOB[job_id]
            elif job_info and job_info.get("progress_log"):
                resolved_path = job_info.get("progress_log")

        if not resolved_path and plan_id and plan_id in _PROGRESS_LOG_BY_ID:
            resolved_path = _PROGRESS_LOG_BY_ID[plan_id]

        if log_path:
            resolved_path = log_path

        if resolved_path:
            if not os.path.isabs(resolved_path):
                candidate = os.path.join(PROJECT_ROOT, resolved_path)
                # å¼‚æ­¥æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
                exists = await loop.run_in_executor(None, os.path.exists, candidate)
                if exists:
                    resolved_path = candidate
                else:
                    alt = os.path.sep + resolved_path.lstrip(os.path.sep)
                    exists_alt = await loop.run_in_executor(None, os.path.exists, alt)
                    if exists_alt:
                        resolved_path = alt

        # å¼‚æ­¥æ£€æŸ¥æœ€ç»ˆè·¯å¾„
        path_exists = await loop.run_in_executor(
            None, lambda: resolved_path and os.path.exists(resolved_path)
        )
        
        if not path_exists:
            return {
                "status": "error",
                "message": "æœªæ‰¾åˆ°è¿›åº¦æ—¥å¿—ï¼Œè¯·ç¡®è®¤ job_id/plan_id æˆ–æä¾› log_pathï¼ˆæ³¨æ„ç»å¯¹è·¯å¾„éœ€ä»¥/å¼€å¤´ï¼Œæ‰©å±•åä¸º .jsonlï¼‰",
            }

        # å¼‚æ­¥è¯»å–æ–‡ä»¶
        def read_file():
            events = []
            total = 0
            try:
                with open(resolved_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()
                    total = len(lines)
                    for line in lines[-limit:]:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            events.append(json.loads(line))
                        except Exception:
                            continue
            except Exception:
                pass
            return events, total
        
        events, total_lines = await loop.run_in_executor(None, read_file)

        response: Dict[str, Any] = {
            "status": "success",
            "plan_id": plan_id,
            "job_id": job_id,
            "log_path": resolved_path,
            "events": events,
            "total_records": total_lines,
            "returned": len(events),
        }

        if job_info:
            snapshot_keys = [
                "job_id",
                "status",
                "plan_id",
                "progress_log",
                "started_at",
                "updated_at",
                "completed_at",
                "project_directory",
                "model",
                "upload_status",
                "upload_url",
                "web_url",
                "deployment_env",
                "upload_completed_at",
                "uploaded_directory",
            ]
            job_snapshot = {
                k: job_info.get(k) for k in snapshot_keys if job_info.get(k) is not None
            }

            if job_info.get("status") == "completed":
                job_snapshot["result_summary"] = {
                    "report": job_info.get("result", {}).get("report"),
                    "created_files": job_info.get("result", {}).get("created_files"),
                }
                if job_info.get("upload_result"):
                    job_snapshot["upload_result"] = job_info.get("upload_result")

            if job_info.get("status") == "failed":
                job_snapshot["error"] = job_info.get("error")

            if job_info.get("upload_error"):
                job_snapshot["upload_error"] = job_info.get("upload_error")

            response["job"] = job_snapshot

        return response
    except Exception as exc:
        return {
            "status": "error",
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }


@mcp.tool()
async def upload_project_to_mcp_server(
    folder_path: str,
) -> Dict[str, Any]:
    """å°†é¡¹ç›®æ–‡ä»¶å¤¹æ‰“åŒ…æˆZIPå¹¶ä¸Šä¼ åˆ° EdgeOne Pagesï¼ˆè‡ªåŠ¨éƒ¨ç½²æµç¨‹ï¼‰ã€‚

    å‚æ•°è¯´æ˜ï¼š
    - folder_path: é¡¹ç›®æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„

    è¿”å›å€¼ï¼š
    - status: ä¸Šä¼ çŠ¶æ€ ("success" æˆ– "error")
    - web_url: éƒ¨ç½²æˆåŠŸåè¿”å›çš„ EdgeOne è®¿é—®åœ°å€
    - deployment_env: éƒ¨ç½²ç¯å¢ƒï¼ˆProduction/Previewï¼‰
    - deployment_result: EdgeOne åŸå§‹éƒ¨ç½²ç»“æœ
    - deployment_logs: éƒ¨ç½²æ—¥å¿—
    - message: çŠ¶æ€ä¿¡æ¯
    """
    try:
        # éªŒè¯æ–‡ä»¶å¤¹è·¯å¾„
        if not os.path.exists(folder_path):
            return {"status": "error", "message": f"é¡¹ç›®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}"}

        if not os.path.isdir(folder_path):
            return {"status": "error", "message": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}"}

        if not os.getenv("EDGEONE_PAGES_API_TOKEN"):
            return {
                "status": "error",
                "message": "ç¼ºå°‘ EDGEONE_PAGES_API_TOKEN ç¯å¢ƒå˜é‡ï¼Œæ— æ³•æ‰§è¡Œ EdgeOne éƒ¨ç½²",
            }

        # åˆ›å»ºä¸´æ—¶ZIPæ–‡ä»¶
        project_name = os.path.basename(folder_path.rstrip("/"))
        temp_dir = tempfile.gettempdir()
        zip_filename = f"{project_name}_{int(time.time())}.zip"
        zip_path = os.path.join(temp_dir, zip_filename)

        # æ‰“åŒ…é¡¹ç›®æ–‡ä»¶
        with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
                    arcname = os.path.relpath(file_path, folder_path)
                    zipf.write(file_path, arcname)

        # æ£€æŸ¥ZIPæ–‡ä»¶å¤§å°
        zip_size = os.path.getsize(zip_path)
        if zip_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
            os.remove(zip_path)
            return {
                "status": "error",
                "message": f"ZIPæ–‡ä»¶è¿‡å¤§: {zip_size / 1024 / 1024:.1f}MBï¼Œè¶…è¿‡50MBé™åˆ¶",
            }

        oss_url: Optional[str] = None
        oss_response: Optional[Dict[str, Any]] = None
        if _should_upload_zip_to_oss():
            async with aiohttp.ClientSession() as session:
                with open(zip_path, "rb") as f:
                    data = aiohttp.FormData()
                    data.add_field(
                        "file", f, filename=zip_filename, content_type="application/zip"
                    )

                    async with session.post(DEFAULT_UPLOAD_URL, data=data) as response:
                        response_text = await response.text()

                        if response.status != 200:
                            return {
                                "status": "error",
                                "message": f"OSS ä¸Šä¼ å¤±è´¥ï¼ŒHTTP {response.status}: {response_text}",
                            }

                        try:
                            result = json.loads(response_text)
                        except json.JSONDecodeError:
                            return {
                                "status": "error",
                                "message": f"OSS ä¸Šä¼ å“åº”è§£æå¤±è´¥: {response_text}",
                            }

                        upload_data = result.get("data") or {}
                        oss_response = upload_data
                        if result.get("code") == 0 and upload_data.get("url"):
                            oss_url = upload_data["url"]
                        else:
                            return {
                                "status": "error",
                                "message": f"OSS ä¸Šä¼ å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}",
                                "response": response_text,
                            }

        # å¯¼å…¥ EdgeOne éƒ¨ç½²å·¥å…·
        from htmlgen_mcp.agents.web_tools.edgeone_deploy import (
            deploy_folder_or_zip_to_edgeone,
        )

        deployment_env = _resolve_edgeone_deploy_env()

        # å°† ZIP åŒ…éƒ¨ç½²åˆ° EdgeOneï¼ˆä½¿ç”¨çº¿ç¨‹é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        result_json = await asyncio.to_thread(
            deploy_folder_or_zip_to_edgeone, zip_path, deployment_env
        )

        try:
            deploy_result = json.loads(result_json)
        except json.JSONDecodeError:
            return {
                "status": "error",
                "message": f"EdgeOne è¿”å›æ•°æ®è§£æå¤±è´¥: {result_json}",
            }

        edgeone_result = deploy_result.get("result") or {}
        web_url = edgeone_result.get("url")

        response: Dict[str, Any] = {
            "status": "success",
            "web_url": web_url,
            "oss_url": oss_url,
            "oss_response": oss_response,
            "deployment_env": deployment_env,
            "deployment_result": edgeone_result,
            "deployment_logs": deploy_result.get("deployment_logs"),
            "zip_size": f"{zip_size / 1024:.1f}KB",
            "message": f"é¡¹ç›® '{project_name}' å·²éƒ¨ç½²åˆ° EdgeOne ({deployment_env})",
        }
        if oss_url:
            response["upload_url"] = oss_url

        if not web_url:
            response["message"] += "ï¼Œä½†æœªè·å–åˆ°è®¿é—®é“¾æ¥"

        return response

    except Exception as exc:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if "zip_path" in locals() and os.path.exists(zip_path):
            try:
                os.remove(zip_path)
            except:
                pass

        error_message = str(exc)
        try:
            # EdgeOne éƒ¨ç½²é”™è¯¯é€šå¸¸æ˜¯ JSON å­—ç¬¦ä¸²
            if error_message.startswith("{"):
                error_data = json.loads(error_message)
                error_response = {
                    "status": "error",
                    "message": error_data.get("error", error_message),
                    "deployment_logs": error_data.get("deployment_logs", ""),
                    "traceback": traceback.format_exc(),
                }
                if "oss_url" in locals() and oss_url:
                    error_response["upload_url"] = oss_url
                if "oss_response" in locals() and oss_response:
                    error_response["oss_response"] = oss_response
                return error_response
        except Exception:
            pass

        error_response = {
            "status": "error",
            "message": error_message,
            "traceback": traceback.format_exc(),
        }
        if "oss_url" in locals() and oss_url:
            error_response["upload_url"] = oss_url
        if "oss_response" in locals() and oss_response:
            error_response["oss_response"] = oss_response
        return error_response
    finally:
        # ç¡®ä¿æ¸…ç†ä¸´æ—¶ZIPæ–‡ä»¶
        if "zip_path" in locals() and os.path.exists(zip_path):
            try:
                os.remove(zip_path)
            except:
                pass


@mcp.tool()
async def deploy_folder_or_zip(
    folder_path: str, env: str = "Production"
) -> Dict[str, Any]:
    """å°†æ„å»ºå¥½çš„ç½‘ç«™æ–‡ä»¶å¤¹æˆ–ZIPæ–‡ä»¶éƒ¨ç½²åˆ°EdgeOne Pagesã€‚

    å‚æ•°è¯´æ˜ï¼š
    - folder_path: æœ¬åœ°æ–‡ä»¶å¤¹æˆ–ZIPæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        æŒ‡å®šè¦éƒ¨ç½²çš„å‰ç«¯æ„å»ºäº§ç‰©ä½ç½®ï¼Œå¯ä»¥æ˜¯ï¼š
        * æ„å»ºå¥½çš„é™æ€ç½‘ç«™æ–‡ä»¶å¤¹ï¼ˆå¦‚ ./dist, ./build ç­‰ï¼‰
        * åŒ…å«ç½‘ç«™æ–‡ä»¶çš„ZIPå‹ç¼©åŒ…
        ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹è·¯å¾„ç±»å‹å¹¶é‡‡ç”¨ç›¸åº”çš„ä¸Šä¼ ç­–ç•¥

    - env: éƒ¨ç½²ç¯å¢ƒï¼ˆå¯é€‰ï¼‰
        * "Production": ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œä½¿ç”¨è‡ªå®šä¹‰åŸŸåï¼ˆå¦‚å·²é…ç½®ï¼‰
        * "Preview": é¢„è§ˆç¯å¢ƒéƒ¨ç½²ï¼Œç”Ÿæˆä¸´æ—¶é¢„è§ˆé“¾æ¥
        é»˜è®¤ä¸º "Production"

    ç¯å¢ƒå˜é‡è¦æ±‚ï¼š
    - EDGEONE_PAGES_API_TOKEN: EdgeOne Pages APIè®¿é—®ä»¤ç‰Œï¼ˆå¿…éœ€ï¼‰
    - EDGEONE_PAGES_PROJECT_NAME: é¡¹ç›®åç§°ï¼ˆå¯é€‰ï¼ŒæœªæŒ‡å®šæ—¶è‡ªåŠ¨åˆ›å»ºä¸´æ—¶é¡¹ç›®ï¼‰

    è¿”å›å€¼è¯´æ˜ï¼š
    - status: éƒ¨ç½²çŠ¶æ€ ("success" æˆ– "error")
    - deployment_logs: è¯¦ç»†çš„éƒ¨ç½²è¿‡ç¨‹æ—¥å¿—
    - result: éƒ¨ç½²ç»“æœä¿¡æ¯
        * type: åŸŸåç±»å‹ ("custom" è‡ªå®šä¹‰åŸŸå æˆ– "temporary" ä¸´æ—¶åŸŸå)
        * url: ç½‘ç«™è®¿é—®URL
        * project_id: EdgeOneé¡¹ç›®ID
        * project_name: é¡¹ç›®åç§°
        * console_url: EdgeOneæ§åˆ¶å°ç®¡ç†é“¾æ¥

    ä½¿ç”¨åœºæ™¯ï¼š
    1. å°†æœ¬åœ°å¼€å‘çš„é™æ€ç½‘ç«™éƒ¨ç½²ä¸Šçº¿
    2. å°†æ„å»ºå·¥å…·ï¼ˆå¦‚Webpackã€Viteç­‰ï¼‰ç”Ÿæˆçš„distç›®å½•éƒ¨ç½²
    3. å°†æ‰“åŒ…å¥½çš„ç½‘ç«™ZIPæ–‡ä»¶å¿«é€Ÿéƒ¨ç½²
    4. åˆ›å»ºç½‘ç«™çš„é¢„è§ˆç‰ˆæœ¬è¿›è¡Œæµ‹è¯•

    éƒ¨ç½²æµç¨‹ï¼š
    1. éªŒè¯æœ¬åœ°è·¯å¾„å’Œæ–‡ä»¶
    2. æ£€æµ‹å¯ç”¨çš„APIç«¯ç‚¹
    3. è·å–æˆ–åˆ›å»ºEdgeOneé¡¹ç›®
    4. ä¸Šä¼ æ–‡ä»¶åˆ°è…¾è®¯äº‘COS
    5. åˆ›å»ºéƒ¨ç½²ä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆ
    6. ç”Ÿæˆè®¿é—®é“¾æ¥å’Œç®¡ç†ä¿¡æ¯

    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - éœ€è¦æœ‰æ•ˆçš„EdgeOne Pages APIä»¤ç‰Œ
    - ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œä¸Šä¼ å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
    - å¤§æ–‡ä»¶æˆ–å¤§é‡æ–‡ä»¶çš„ä¸Šä¼ ä¼šç›¸åº”å¢åŠ éƒ¨ç½²æ—¶é—´
    - ä¸´æ—¶åŸŸåé“¾æ¥åŒ…å«æ—¶æ•ˆæ€§è®¿é—®ä»¤ç‰Œ
    """
    try:
        # å¯¼å…¥EdgeOneéƒ¨ç½²å·¥å…·
        from htmlgen_mcp.agents.web_tools.edgeone_deploy import deploy_folder_or_zip_to_edgeone

        # éªŒè¯ç¯å¢ƒå˜é‡
        api_token = os.getenv("EDGEONE_PAGES_API_TOKEN")
        if not api_token:
            return {
                "status": "error",
                "message": "Missing EDGEONE_PAGES_API_TOKEN environment variable. Please set your EdgeOne Pages API token.",
            }

        # éªŒè¯è·¯å¾„æ ¼å¼
        if not os.path.isabs(folder_path):
            return {
                "status": "error",
                "message": f"Path must be absolute: {folder_path}",
            }

        # éªŒè¯ç¯å¢ƒå‚æ•°
        if env not in ["Production", "Preview"]:
            return {
                "status": "error",
                "message": "env must be 'Production' or 'Preview'",
            }

        # æ‰§è¡Œéƒ¨ç½²
        result_json = await asyncio.to_thread(
            deploy_folder_or_zip_to_edgeone, folder_path, env
        )
        result = json.loads(result_json)

        return {
            "status": "success",
            "message": f"Deployment to {env} environment completed successfully",
            "deployment_logs": result.get("deployment_logs", ""),
            "result": result.get("result", {}),
        }

    except Exception as exc:
        error_message = str(exc)

        # å¦‚æœæ˜¯EdgeOneéƒ¨ç½²é”™è¯¯ï¼Œå°è¯•è§£æJSONæ ¼å¼çš„é”™è¯¯ä¿¡æ¯
        try:
            if error_message.startswith("{"):
                error_data = json.loads(error_message)
                return {
                    "status": "error",
                    "message": error_data.get("error", error_message),
                    "deployment_logs": error_data.get("deployment_logs", ""),
                    "traceback": traceback.format_exc(),
                }
        except:
            pass

        return {
            "status": "error",
            "message": error_message,
            "traceback": traceback.format_exc(),
        }


def main() -> None:
    transport = os.environ.get("MCP_TRANSPORT", "stdio")
    print("ğŸš€ Smart Web Agent MCP æœåŠ¡å™¨å·²å¯åŠ¨")
    print(f"ğŸ“ é»˜è®¤é¡¹ç›®æ ¹ç›®å½•: {DEFAULT_PROJECT_ROOT}")
    print(f"ğŸ¤– é»˜è®¤æ¨¡å‹: {DEFAULT_MODEL}")
    print(f"ğŸŒ é»˜è®¤APIåœ°å€: {DEFAULT_BASE_URL}")
    print("ğŸŒ EdgeOne Pages éƒ¨ç½²å·¥å…·å·²åŠ è½½")
    mcp.run(transport=transport)


if __name__ == "__main__":
    main()
