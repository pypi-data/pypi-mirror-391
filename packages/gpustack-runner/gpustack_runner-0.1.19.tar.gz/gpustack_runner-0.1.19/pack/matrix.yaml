# Runner Packing Rules
#
# - backend:   (Required) The accelerated backend to pack for,
#              used to select the packing rules and Dockerfile below `pack` directory.
# - services:  (Required) The inference service to pack for,
#              used to select the Docker build phase described in `pack/${backend}/Dockerfile`.
# - platforms: (Optional) The platforms to build for,
#              used to select the Docker Linux build platforms.
#              Default to `linux/amd64` and `linux/arm64`.
# - args:      (Optional) The build arguments to pass to the Docker build,
#              used to override the default build arguments in `pack/${backend}/Dockerfile`.

rules:

  #
  # Ascend CANN
  #

  ## Ascend CANN 8.2.rc2, using CANN Kernel for A3.
  ##
  - backend: "cann"
    services:
      - "mindie"
      - "vllm"
      - "sglang"
    args:
      - "CANN_VERSION=8.2.rc2"
      - "CANN_ARCHS=a3"
  ## Ascend CANN 8.2.rc2, using CANN Kernel for 910B.
  ##
  - backend: "cann"
    services:
      - "mindie"
      - "vllm"
      - "sglang"
    args:
      - "CANN_VERSION=8.2.rc2"
      - "CANN_ARCHS=910b"
  ## Ascend CANN 8.2.rc2, using CANN Kernel for 310P.
  ##
  - backend: "cann"
    services:
      - "mindie"
      - "vllm"
    args:
      - "CANN_VERSION=8.2.rc2"
      - "CANN_ARCHS=310p"

  #
  # Iluvatar CoreX
  #

  ## Iluvatar CoreX 4.2.0
  ##
  - backend: "corex"
    services:
      - "vllm"
    platforms:
      - "linux/amd64"
    args:
      - "COREX_VERSION=4.2.0"

  #
  # NVIDIA CUDA
  #

  ## NVIDIA CUDA 12.4.1, using PyTorch +cu126 in linux/amd64.
  ##
  - backend: "cuda"
    services:
      - "voxbox"
      - "vllm"
      - "sglang"
    args:
      - "CUDA_VERSION=12.4.1"
      - "VOXBOX_TORCH_CUDA_VERSION=12.6.3"
      - "VLLM_TORCH_CUDA_VERSION=12.6.3"
      - "VLLM_NVIDIA_GDRCOPY_VERSION=2.4.1"
      - "VLLM_NVIDIA_HPCX_VERSION=2.21.3"
      - "VLLM_AWS_EFA_VERSION=1.43.3"
      - "SGLANG_BASE_IMAGE=gpustack/runner:cuda12.4-vllm0.11.0"
      - "SGLANG_KERNEL_VERSION=0.3.12"
  ## NVIDIA CUDA 12.6.3, using PyTorch +cu126 in linux/amd64.
  ##
  - backend: "cuda"
    services:
      - "voxbox"
      - "vllm"
      - "sglang"
    args:
      - "CUDA_VERSION=12.6.3"
      - "VLLM_NVIDIA_GDRCOPY_VERSION=2.4.1"
      - "VLLM_NVIDIA_HPCX_VERSION=2.21.3"
      - "VLLM_AWS_EFA_VERSION=1.43.3"
      - "SGLANG_BASE_IMAGE=gpustack/runner:cuda12.6-vllm0.11.0"
      - "SGLANG_KERNEL_VERSION=0.3.12"
  ## NVIDIA CUDA 12.8.1, using PyTorch +cu128 in both linux/amd64 and linux/arm64.
  ##
  - backend: "cuda"
    services:
      - "voxbox"
      - "vllm"
      - "sglang"
    args:
      - "CUDA_VERSION=12.8.1"
      - "VLLM_NVIDIA_GDRCOPY_VERSION=2.4.1"
      - "VLLM_NVIDIA_HPCX_VERSION=2.22.1rc4"
      - "VLLM_AWS_EFA_VERSION=1.43.3"
      - "SGLANG_BASE_IMAGE=gpustack/runner:cuda12.8-vllm0.11.0"

  #
  # Hygon DTK
  #

  ## Hygon DTK 25.04.2
  ##
  - backend: "dtk"
    services:
      - "vllm"
    platforms:
      - "linux/amd64"
    args:
      - "DTK_VERSION=25.04.2"

  #
  # MateX MACA
  #

  ## MateX MACA 3.2.1
  ##
  - backend: "maca"
    services:
      - "vllm"
    platforms:
      - "linux/amd64"
    args:
      - "MACA_VERSION=3.2.1"

  #
  # AMD ROCm
  #

  ## AMD ROCm 7.0.2, using PyTorch +rocm7.0 in linux/amd64.
  ##
  - backend: "rocm"
    services:
      - "vllm"
      - "sglang"
    platforms:
      - "linux/amd64"
    args:
      - "ROCM_VERSION=7.0.2"
      - "SGLANG_BASE_IMAGE=gpustack/runner:rocm7.0-vllm0.11.0"
