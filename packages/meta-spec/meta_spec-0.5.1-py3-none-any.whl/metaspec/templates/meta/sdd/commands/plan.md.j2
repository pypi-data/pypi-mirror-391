---
description: Execute the implementation planning workflow to design toolkit architecture, including parser, validator, and CLI components
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

**Goal**: Transform the specification into a concrete implementation plan with architecture design, component interfaces, and file structure.

**Important**: This command runs AFTER `/metaspec.specify` and `/metaspec.clarify`. It focuses on HOW to implement, not WHAT to build.

**ðŸ—ï¸ Two-Feature Architecture Awareness**

This command must handle two types of specifications:

**Feature 1 (Specification Spec)**: Usually doesn't need a plan.md
- Domain specifications are primarily **documentation**
- No code implementation needed for the specification itself
- **Skip plan generation** unless explicitly requested

**Feature 2 (Toolkit Spec)**: Requires a plan.md
- Defines how to implement Parser, Validator, CLI
- Needs detailed architecture and technical design
- **This is the primary use case for `/metaspec.plan`**

### Execution Flow

#### 1. Identify Feature Type and Load Context

**Step 1a: Determine which Feature** you're planning:
- Check current directory: `specs/domain/001-*` or `specs/toolkit/001-*`?
- Read spec.md to understand if it's specification or toolkit

**Step 1b: For Feature 2 (Toolkit Spec)**:
- âœ… Read `specs/toolkit/001-toolkit/spec.md` (the WHAT)
- âœ… **Verify Feature 1 exists**: Check `specs/domain/001-{domain}-spec/spec.md`
- âœ… Read `/memory/constitution.md` (the principles)
- âœ… Create `specs/toolkit/001-toolkit/plan.md` (the HOW)

**Step 1c: For Feature 1 (Specification Spec)** :
- âš ï¸ **Ask user**: "Feature 1 is a domain specification and typically doesn't need a plan.md. Are you sure you want to create one?"
- If yes, proceed with simplified planning
- If no, suggest: "Run `/metaspec.plan` in Feature 2 (toolkit spec) instead"

#### 2. Validate Dependencies (Feature 2 Only)

**For Feature 2 (Toolkit Spec)**:

**Critical Validation**:
1. âœ… Feature 1 spec exists at `specs/domain/001-{domain}-spec/spec.md`
2. âœ… Feature 2 spec declares dependency in a **Dependencies** section:
   ```markdown
   ## Dependencies
   - **Depends on**: 001-{domain}-spec
   ```
3. âœ… Components reference Feature 1 validation rules

**If dependencies missing**:
- âš ï¸ **Stop and alert**: "Feature 2 must depend on Feature 1. Please update spec.md to add Dependencies section."
- Provide template for adding dependencies

#### 3. Extract Implementation Details from spec.md (NEW ðŸŽ¯)

**CRITICAL**: Read the "## Implementation" section from `specs/toolkit/001-*/spec.md`:

```bash
# Extract implementation details
grep -A 50 "^## Implementation" specs/toolkit/001-*/spec.md
```

**Extract**:
1. **Primary Language**: Python / TypeScript / Go / Rust / Other
2. **Rationale**: Why this language was chosen
3. **Key Dependencies**: What frameworks/libraries
4. **Structure**: Monolithic / Modular / Plugin-based
5. **Core Components**: Which are included (Parser, Validator, CLI, Generator, SDK)
6. **Extensibility**: How users can extend the toolkit

**Example extracted data**:
```markdown
Primary Language: Python 3.10+
Rationale: Target Python developers, rich ecosystem (Pydantic, Typer)
Key Dependencies:
  - Pydantic 2.0+ (data validation)
  - Typer 0.9+ (CLI)
  - PyYAML (YAML parsing)
Structure: Modular
Core Components: Parser âœ“, Validator âœ“, CLI âœ“, Generator âœ—, SDK âœ—
Extensibility: Custom validator plugins via entry points
```

#### 4. Fill Technical Context Based on Language

**IMPORTANT**: Design architecture based on the **chosen language** from spec.md, not hardcoded Python.

**For Python Stack**:
```markdown
**Language**: Python {version from spec.md}
**Parser**: PyYAML / ruamel.yaml / pydantic-yaml
**Validator**: Pydantic 2.0+ / marshmallow / cerberus
**CLI Framework**: Typer / Click / argparse
**Testing**: pytest / unittest
**Type Checking**: mypy --strict / pyright
**Package Manager**: pip / poetry / uv
**Documentation**: Markdown + docstrings + Sphinx
```

**For TypeScript Stack**:
```markdown
**Language**: TypeScript {version from spec.md}
**Parser**: js-yaml / yaml / json5
**Validator**: Zod / Yup / io-ts / ajv
**CLI Framework**: Commander.js / yargs / oclif
**Testing**: Vitest / Jest / Mocha
**Type Checking**: tsc --strict
**Package Manager**: npm / pnpm / yarn
**Documentation**: Markdown + TSDoc + TypeDoc
```

**For Go Stack**:
```markdown
**Language**: Go {version from spec.md}
**Parser**: gopkg.in/yaml.v3 / encoding/json
**Validator**: go-playground/validator / go-ozzo/ozzo-validation
**CLI Framework**: cobra / urfave/cli / flag
**Testing**: testing package / testify
**Type Checking**: Built-in
**Package Manager**: go modules
**Documentation**: Markdown + godoc
```

**For Rust Stack**:
```markdown
**Language**: Rust {version from spec.md}
**Parser**: serde_yaml / serde_json
**Validator**: validator / garde
**CLI Framework**: clap / structopt
**Testing**: Built-in test framework
**Type Checking**: Built-in
**Package Manager**: cargo
**Documentation**: Markdown + rustdoc
```

**Custom Context** (from spec.md):
```markdown
**Domain**: [from spec.md]
**Primary Entity**: [from specification spec]
**Validation Strategy**: [from spec.md - based on specification rules]
**Performance Targets**: [from spec.md quality criteria]
**Extensibility Requirements**: [from spec.md - plugin system/hooks]
```

**Mark unknowns as**:
```
[NEEDS CLARIFICATION: specific question]
```

#### 4. Constitution Check

**Extract from constitution.md**:
- Entity-First Design requirements
- Validator Extensibility requirements
- AI-Agent Friendly requirements
- Progressive Enhancement plan
- Domain Specificity constraints

**Evaluate gates**:
- Does planned architecture follow constitution?
- Are there justified exceptions?
- Document any violations with rationale

**GATE**: Must pass before Phase 0 research.

#### 5. Project Structure

**Define toolkit file structure**:

```
[toolkit-name]/
â”œâ”€â”€ README.md
â”œâ”€â”€ AGENTS.md
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ constitution.md
â”œâ”€â”€ specs/
â”‚   â”œâ”€â”€ spec.md
â”‚   â”œâ”€â”€ plan.md                    # This file
â”‚   â”œâ”€â”€ research.md               # Phase 0 output
â”‚   â”œâ”€â”€ architecture.md           # Phase 1 output
â”‚   â””â”€â”€ parser-design.md          # Phase 1 output
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ spec-template.yaml        # User spec template
â”œâ”€â”€ src/
â”‚   â””â”€â”€ [toolkit_name]/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ models.py             # Pydantic models
â”‚       â”œâ”€â”€ parser.py             # YAML/JSON parser
â”‚       â”œâ”€â”€ validator.py          # Validation logic
â”‚       â”œâ”€â”€ cli.py                # Typer CLI
â”‚       â””â”€â”€ errors.py             # Error types
â””â”€â”€ tests/
    â”œâ”€â”€ unit/
    â”‚   â”œâ”€â”€ test_parser.py
    â”‚   â”œâ”€â”€ test_validator.py
    â”‚   â””â”€â”€ test_models.py
    â”œâ”€â”€ integration/
    â”‚   â””â”€â”€ test_cli.py
    â””â”€â”€ fixtures/
        â”œâ”€â”€ valid_specs/
        â””â”€â”€ invalid_specs/
```

#### 6. Phase 0: Domain Research

**Goal**: Research domain standards and conventions before designing architecture.

**Research Tasks**:

1. **Domain Standards**:
   - Task: "Research [domain] industry standards and specifications"
   - Example: For API Testing â†’ REST/GraphQL standards, HTTP RFCs
   - Example: For Design Systems â†’ W3C design tokens, CSS specs

2. **Reference Implementations**:
   - Task: "Find existing [domain] tools and analyze their approaches"
   - Example: For API Testing â†’ analyze Postman, REST Client, etc.
   - Example: For Design Systems â†’ analyze Figma Tokens, Style Dictionary

3. **Validation Patterns**:
   - Task: "Research common validation patterns for [domain]"
   - Example: For API Testing â†’ assertion types, matcher patterns
   - Example: For Design Systems â†’ token naming conventions

4. **Error Message Best Practices**:
   - Task: "Research effective error messages for [domain] tooling"
   - Focus on AI-friendliness and actionable feedback

**Research agents** (run parallel web searches):
```
For each research task:
  - Search for relevant standards, RFCs, or specs
  - Identify common patterns in existing tools
  - Document findings in research.md
```

**Output**: `/specs/research.md`

**Format**:
```markdown
# Domain Research: [Toolkit Name]

## Domain Standards
- Standard 1: [name] - [URL] - [relevance]
- Standard 2: ...

## Reference Implementations
- Tool 1: [name] - [approach] - [lessons learned]
- Tool 2: ...

## Validation Patterns
- Pattern 1: [description] - [example]
- Pattern 2: ...

## Error Message Insights
- Insight 1: [description]
- Insight 2: ...

## Recommendations
- Recommendation 1: [what to adopt]
- Recommendation 2: [what to avoid]
```

#### 7. Phase 1: Architecture Design

**Prerequisites**: research.md complete, no NEEDS CLARIFICATION remaining

**Generate 3 documents**:

##### 6.1 Architecture Overview (`/specs/architecture.md`)

**Content**:
```markdown
# Architecture: [Toolkit Name]

## Component Diagram
[Describe how components interact]

User writes spec.yaml
  â†“
CLI (cli.py) reads file
  â†“
Parser (parser.py) loads YAML
  â†“
Models (models.py) validate structure
  â†“
Validator (validator.py) checks rules
  â†“
CLI outputs results

## Component Responsibilities

### Parser (parser.py)
- Load YAML/JSON files
- Handle file I/O errors
- Preserve line numbers for error reporting
- Return raw dict/list

### Models (models.py)
- Define Pydantic models for entities
- Handle type coercion
- Provide field descriptions
- Export JSON Schema

### Validator (validator.py)
- Structural validation (Pydantic)
- Semantic validation (custom rules)
- Domain-specific validation
- Generate actionable error messages
- Support custom validator plugins

### CLI (cli.py)
- Define commands (init, validate, etc.)
- Handle arguments and options
- Format output (text, JSON)
- Manage exit codes

### Errors (errors.py)
- Custom exception types
- Error formatting utilities
- Fix suggestion generation

## Data Flow

1. User Input â†’ File Path
2. Parser â†’ Raw Data (dict/list)
3. Models â†’ Validated Entity
4. Validator â†’ Validation Result
5. CLI â†’ Formatted Output

## Extension Points

1. Custom Validators:
   - register_validator(name, func)
   - Validator plugins via entry points

2. Custom Commands:
   - Typer command registration
   - Plugin CLI commands

3. Custom Output Formatters:
   - register_formatter(name, func)
```

##### 6.2 Parser Design (`/specs/parser-design.md`)

**Content**:
```markdown
# Parser Design: [Toolkit Name]

## Parser Interface

```python
def parse_spec(file_path: Path) -> Dict[str, Any]:
    """Parse specification file.
    
    Args:
        file_path: Path to YAML/JSON file
        
    Returns:
        Parsed specification as dict
        
    Raises:
        ParseError: If file invalid or syntax error
    """
```

## Error Handling

### File Not Found
```
âŒ Error: Specification file not found
  File: spec.yaml
  Current directory: /Users/dev/project
  Fix: Check file path or create file with 'toolkit init spec.yaml'
```

### Invalid YAML
```
âŒ Error: Invalid YAML syntax
  File: spec.yaml
  Line: 5
  Issue: Unexpected indentation
  Fix: Ensure consistent indentation (2 or 4 spaces)
```

### Empty File
```
âŒ Error: Specification file is empty
  File: spec.yaml
  Fix: Initialize with 'toolkit init spec.yaml' or copy example
```

## Implementation Notes
- Use ruamel.yaml for line number preservation
- Cache parsed specs for performance
- Support both YAML and JSON formats
- Validate UTF-8 encoding
```

##### 6.3 Validator Design (`/specs/validator-design.md`)

**Content**:
```markdown
# Validator Design: [Toolkit Name]

## Validation Layers

### Layer 1: Structural Validation (Pydantic)
- Field types match
- Required fields present
- Enum values valid
- Array/object structures correct

### Layer 2: Semantic Validation (Custom)
- Cross-field dependencies
- Unique constraints
- Reference integrity
- Logic consistency

### Layer 3: Domain Validation (Domain-Specific)
- [Domain rule 1 from research]
- [Domain rule 2 from research]
- [Domain rule 3 from research]

## Validator Interface

```python
class ValidationResult:
    is_valid: bool
    errors: List[ValidationError]
    warnings: List[ValidationWarning]

def validate_spec(entity: EntityModel) -> ValidationResult:
    """Validate entity against all rules.
    
    Args:
        entity: Parsed and type-checked entity
        
    Returns:
        Validation result with errors/warnings
    """
```

## Error Message Format

```
âŒ Error: [Error Type]
  Entity: [entity_name]
  Field: [field_path]
  Value: [actual_value]
  Issue: [what's wrong]
  Expected: [what should be]
  Fix: [how to fix it]
  Example: [correct example]
```

## Custom Validator Registration

```python
# In validator.py
_custom_validators: Dict[str, Callable] = {}

def register_validator(name: str, validator: Callable) -> None:
    """Register custom validator."""
    _custom_validators[name] = validator

# User code
from my_toolkit.validator import register_validator

def validate_custom_rule(value, context):
    # Custom validation logic
    return True, None  # is_valid, error_message

register_validator("custom_rule", validate_custom_rule)
```

## Performance Targets
- < 100ms for typical spec (from constitution)
- < 1s for 1000-entity spec
- Memory: < 50MB

## Extensibility
- Plugin system via entry points
- Custom rule registration
- Domain-specific validator subclasses
```

#### 8. Re-evaluate Constitution Check

After Phase 1 design:
- Review architecture against constitution
- Check Entity-First: Is entity model minimal?
- Check Validator Extensibility: Are extension points clear?
- Check AI-Agent Friendly: Are error messages actionable?
- Document any justified deviations

#### 9. Output and Report

**Files Created**:
- `specs/toolkit/XXX-name/plan.md` (this file with all sections filled)
- `specs/toolkit/XXX-name/research.md` (Phase 0 output)
- `specs/toolkit/XXX-name/architecture.md` (Phase 1 output)
- `specs/toolkit/XXX-name/parser-design.md` (Phase 1 output)
- `specs/toolkit/XXX-name/validator-design.md` (Phase 1 output)

**Report**:
```
âœ… Implementation plan complete

ðŸ“‹ Summary:
- Toolkit: [name]
- Domain: [domain]
- Primary Entity: [entity]
- Architecture: Parser â†’ Models â†’ Validator â†’ CLI

ðŸ“ Generated files:
- specs/toolkit/XXX-name/plan.md
- specs/toolkit/XXX-name/research.md
- specs/toolkit/XXX-name/architecture.md
- specs/toolkit/XXX-name/parser-design.md
- specs/toolkit/XXX-name/validator-design.md

âœ… Constitution check: PASSED
   - Entity-First Design: âœ…
   - Validator Extensibility: âœ…
   - AI-Agent Friendly: âœ…
   - Progressive Enhancement: âœ…
   - Domain Specificity: âœ…

ðŸ“Š Project Structure:
- Source: src/[toolkit_name]/
- Tests: tests/
- Templates: templates/
- Docs: specs/

ðŸ”„ Next step: /metaspec:tasks (break down into implementable tasks)

ðŸ’¡ Suggested commit message:
   docs: complete implementation plan for [toolkit-name]
```

## Key Rules

1. **No implementation code**
   - This is design, not implementation
   - Show interfaces, not full code
   - Use pseudocode when helpful

2. **Follow constitution**
   - Every design decision must align
   - Document justified exceptions
   - Re-check after Phase 1

3. **Research first**
   - Phase 0 must complete before Phase 1
   - Resolve all NEEDS CLARIFICATION
   - Base design on research findings

4. **Concrete, not vague**
   - Specific file names
   - Clear component responsibilities
   - Actionable error message examples

5. **Extension points first**
   - Design for extensibility from start
   - Clear plugin interfaces
   - Documented registration mechanisms

## Example: API Test Kit Plan

### Technical Context
```markdown
**Domain**: API Testing
**Primary Entity**: APITest
**Validation Strategy**: JSON Schema (structure) + custom rules (assertions)
**Performance Targets**: < 100ms validation, < 5s for 100 tests
**Extensibility**: Custom assertion types, request/response hooks
```

### Research Phase (research.md)
```markdown
## Domain Standards
- RFC 7231 (HTTP/1.1 Semantics): HTTP methods, status codes
- OpenAPI 3.1: API specification format
- JSON Schema: Data validation

## Reference Implementations
- Postman: Collection runner, assertion library
- Rest Assured: Fluent assertion API
- Tavern: YAML-based test definitions

## Validation Patterns
- Assertion library: status, body, headers, performance
- Matcher syntax: exact, contains, regex, jsonpath
- Chaining: multiple assertions per test

## Recommendations
- Adopt HTTP method enum from RFC 7231
- Use JSONPath for body assertions
- Support custom assertion registration
```

### Architecture (architecture.md)
```markdown
## Component Diagram
User writes test.yaml
  â†“
CLI reads file
  â†“
Parser loads YAML
  â†“
APITestModel validates structure
  â†“
Validator checks HTTP semantics
  â†“
CLI outputs validation result / executes test

## Extension Points
1. Custom Assertions: register_assertion("custom", func)
2. Request Hooks: before_request(request) â†’ request
3. Response Hooks: after_response(response) â†’ response
```

## Important Notes

1. **This is toolkit architecture, not end-user app**
   - Design parser/validator, not API endpoints
   - Design CLI commands, not web UI
   - Design entity models, not database schemas

2. **Research is critical**
   - Don't guess domain standards
   - Find and reference actual specs (RFCs, W3C, etc.)
   - Learn from existing tools in the domain

3. **Extension points are mandatory**
   - Constitution requires extensibility
   - Design plugin interfaces early
   - Document registration mechanisms

4. **Error messages are design artifacts**
   - Not implementation details
   - Must be AI-friendly (constitution requirement)
   - Include examples in design docs

5. **Performance targets from constitution**
   - Validation: < 100ms (standard)
   - Memory: < 50MB (standard)
   - Document if different targets needed

