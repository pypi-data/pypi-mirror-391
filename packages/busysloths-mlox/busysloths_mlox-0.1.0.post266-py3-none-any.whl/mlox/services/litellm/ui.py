"""UI components for LiteLLM + Ollama service configuration."""

from typing import Dict

import streamlit as st

from mlox.infra import Bundle, Infrastructure
from mlox.services.litellm.docker import LiteLLMDockerService

# Curated list of popular Ollama models organized by size
OLLAMA_MODELS = {
    "Tiny (< 2GB)": [
        "tinyllama",
        "qwen2.5:0.5b",
        "deepseek-r1:1.5b",
        "llama3.2:1b",
        "phi3.5:3.8b",
    ],
    "Small (2-5GB)": [
        "llama3.2:3b",
        "qwen2.5:3b",
        "phi3:3.8b",
        "gemma2:2b",
        "mistral:7b",
    ],
    "Medium (5-10GB)": [
        "qwen2.5:7b",
        "llama3.1:8b",
        "mistral-nemo:12b",
        "gemma2:9b",
        "phi3:14b",
    ],
    "Large (> 10GB)": [
        "qwen2.5:14b",
        "llama3.1:70b",
        "deepseek-r1:70b",
        "qwen2.5:32b",
        "mixtral:8x7b",
    ],
}


def setup(infra: Infrastructure, bundle: Bundle) -> Dict:  # noqa: ARG001
    """Configure LiteLLM + Ollama service during setup.

    Args:
        infra: Infrastructure instance (unused but required by interface)
        bundle: Bundle instance (unused but required by interface)

    Returns:
        Dictionary of configuration parameters for the service
    """
    params: Dict = {}
    st.write("LiteLLM + Ollama Configuration")

    # OpenAI key input
    c1, _c2 = st.columns(2)
    openai_key = c1.text_input("OpenAI Key (optional)", key="openai_key")
    params["${OPENAI_KEY}"] = openai_key

    # Ollama model selection
    st.markdown("### Ollama Model Selection")
    st.markdown(
        "Select which Ollama models to pre-install. "
        "Models will be downloaded when the service starts. "
        "You can always add more models later via the Ollama CLI."
    )

    # Flatten model list for multiselect
    all_models = []
    for category, models in OLLAMA_MODELS.items():
        all_models.extend(models)

    # Create expandable sections for each category
    with st.expander("üì¶ Browse models by size", expanded=True):
        for category, models in OLLAMA_MODELS.items():
            st.markdown(f"**{category}**")
            st.markdown("- " + "\n- ".join(models))

    # Default selection (tiny models)
    default_models = ["tinyllama", "llama3.2:1b", "deepseek-r1:1.5b"]

    selected_models = st.multiselect(
        "Select models to pre-install",
        options=all_models,
        default=default_models,
        help="Choose one or more models. We recommend starting with smaller models for faster setup.",
    )

    # Warning if no models selected
    if not selected_models:
        st.warning(
            "‚ö†Ô∏è No models selected. Ollama will start without any pre-installed models."
        )

    # Show estimated disk usage
    if selected_models:
        st.info(f"‚úÖ {len(selected_models)} model(s) selected")

    # Store selected models in params for the service
    params["${OLLAMA_MODELS}"] = selected_models

    return params


def settings(infra: Infrastructure, bundle: Bundle, service: LiteLLMDockerService):  # noqa: ARG001
    """Display settings for an installed LiteLLM + Ollama service.

    Args:
        infra: Infrastructure instance (unused but required by interface)
        bundle: Bundle instance containing server information
        service: The LiteLLM service instance
    """
    st.header(f"LiteLLM + Ollama ¬∑ {service.name}")

    ui_url = service.service_urls.get("Login", "")
    api_url = service.service_urls.get("Service", "")
    service_ip = bundle.server.ip

    status_col, models_col, port_col = st.columns(3)
    status_col.metric("Status", service.state.title())
    models_col.metric("Configured Models", len(service.ollama_models))
    port_col.metric("UI Port", service.service_ports.get("LiteLLM UI", "‚Äî"))

    st.markdown("---")

    link_cols = st.columns(3)
    link_cols[0].link_button(
        "Open LiteLLM UI", ui_url or "https://github.com/BerriAI/litellm"
    )
    link_cols[1].link_button(
        "LiteLLM Docs",
        "https://docs.litellm.ai",  # Official documentation
    )
    link_cols[2].link_button("Ollama Docs", "https://github.com/ollama/ollama")

    st.subheader("Credentials")
    cred_user_col, cred_pw_col = st.columns(2)
    cred_user_col.text_input("UI Username", value=service.ui_user, disabled=True)
    cred_pw_col.text_input(
        "UI Password",
        value=service.ui_pw,
        type="password",
        disabled=True,
        help="Use these credentials to sign in to the LiteLLM web UI.",
    )
    st.text_input(
        "LiteLLM Master Token",
        value=service.api_key,
        type="password",
        disabled=True,
        help="Shared secret set as MY_LITELLM_MASTER_KEY for authenticating API requests.",
    )

    st.subheader("Connection Details")
    st.markdown(
        f"""
        - IP / Host: `{service_ip}`
        - API Endpoint: `{api_url or "https://<host>:<service_port>"}`
        - Config Path: `{service.target_path}/litellm-config.yaml`
        """
    )

    st.subheader("Configured Ollama Models")
    if service.ollama_models:
        st.markdown("The following models will be pulled on startup:")
        st.markdown("- " + "\n- ".join(service.ollama_models))
    else:
        st.info(
            "No models pre-configured. Add models by editing the service and adding entries like "
            "`tinyllama` or `qwen2.5:3b`."
        )

    st.subheader("Tips")
    st.markdown(
        "- Use `ollama ls` on the server to inspect installed models.\n"
        "- Additional LiteLLM routes can be configured by editing the generated YAML above."
    )
