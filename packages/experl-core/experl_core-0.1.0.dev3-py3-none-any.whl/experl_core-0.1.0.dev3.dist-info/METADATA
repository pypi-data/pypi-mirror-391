Metadata-Version: 2.4
Name: experl-core
Version: 0.1.0.dev3
Summary: Train large language models using reinforcement learning with human feedback.
Home-page: https://github.com/vjkhambe/experl
Author: Vijay Khambe
Author-email: vj.khambe@gmail.com
Keywords: transformers,huggingface,language modeling,post-training,rlhf,sft,dpo,ppo
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Natural Language :: English
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Python: >=3.12
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: accelerate==1.11.0
Requires-Dist: datasets==4.3.0
Requires-Dist: transformers==4.57.1
Requires-Dist: trl==0.24.0
Requires-Dist: mlflow==3.5.1
Requires-Dist: hydra-core==1.3.2
Provides-Extra: cuda
Requires-Dist: bitsandbytes==0.48.2; extra == "cuda"
Requires-Dist: torchvision==0.24.0; extra == "cuda"
Dynamic: license-file

# üß† Experl ‚Äî RLHF Framework for Post-Training Large Language Models

---

## üìú Overview

Experl is a framework designed for post-training large language and foundation models using Reinforcement Learning from Human Feedback (RLHF) techniques such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO).

Built on top of Hugging Face TRL and Transformers, Experl provides a flexible and extensible platform for training, evaluation, and experimentation in RLHF-based fine-tuning workflows.
It integrates seamlessly with:

- ü§ó TRL ‚Äì provides trainer abstractions for RL pipelines
- ü§ó Transformers ‚Äì for model and tokenizer management
- ‚öôÔ∏è Hydra ‚Äì for structured and hierarchical configuration management
- üìä MLflow ‚Äì used for training experiment tracking, metrics logging, datasets and model versioning


Experl simplifies the complex process of RLHF post-training, allowing researchers and engineers to focus on experimentation, comparison, and innovation ‚Äî rather than boilerplate code.

---
## üöÄ Getting Started

## Installation

### Python Package

Install the library using `pip`:

```bash
pip install experl
```

### From source

If you want to use the latest features before an official release, you can install Experl from source:

```bash
pip install git+https://github.com/vjkhambe/experl.git
```

### Repository

If you want to use the examples you can clone the repository with the following command:

```bash
git clone https://github.com/vjkhambe/experl.git
```
---

## Command Line Interface (CLI)

You can use the Experl Command Line Interface (CLI) to quickly get started with post-training methods like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO):

**PPO:**

```bash
experl ppo \
++model_name_or_path=google/gemma-3-270m-it \
++project.max_seq_length=128 \
++ppo.reward_model_name_or_path=google/gemma-3-270m-it
```

**DPO:**

```bash
experl dpo \
++model_name_or_path=google/gemma-3-270m-it \
++project.max_seq_length=128
```

## Citation

```bibtex
@misc{vjkhambe2025experl,
  author = {Vijay Khambe},
  title = {Experl: Reinforcement Learning from Human Feedback Framework},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/vjkhambe/experl}}
}
```

## License

This repository's source code is available under the [Apache-2.0 License](LICENSE).
