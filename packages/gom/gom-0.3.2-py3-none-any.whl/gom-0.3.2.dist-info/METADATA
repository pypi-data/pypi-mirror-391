Metadata-Version: 2.4
Name: gom
Version: 0.3.2
Summary: Live GPU utilization monitor, inspired by nvidia-smi. Shows per-container GPU usage as well as total GPU usage.
License-File: LICENSE
Author: Ben Gubler
Author-email: nebrelbug@gmail.com
Requires-Python: >=3.8,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Requires-Dist: click (>=8.0.0,<9.0.0)
Requires-Dist: docker (>=7.1.0,<8.0.0)
Requires-Dist: pynvml (>=11.0.0,<12.0.0)
Requires-Dist: tabulate (>=0.9.0,<0.10.0)
Description-Content-Type: text/markdown

# gom

`gom` is a CLI tool that displays a human-readable table with GPU usage information. Think `nvidia-smi`, but minimalist and pretty.

It also shows per-container GPU usage information if Docker is installed.

## Installation

Use the package manager [pip](https://pip.pypa.io/en/stable/) to install `gom`.

## Usage

`gom show` displays a table with GPU usage information.

`gom watch` displays a table with GPU usage information and updates it every second.

## Screenshots

Compare the output of `gom show` and `nvidia-smi`. I hope you'll agree that `gom` produces more clear and helpful output (ex. it breaks usage down across the 4 running Docker containers), while `nvidia-smi` is long and complex (I couldn't even screenshot the whole thing).

![gom show image](https://github.com/nebrelbug/gom/assets/25597854/367004c2-8729-491d-bff4-783a145fa7bb)

![nvidia-smi image](https://github.com/nebrelbug/gom/assets/25597854/80380be6-b7d2-43c0-b10c-07267b85613e)

## Troubleshooting

You may need to install a different version of `pynvml` depending on your CUDA version.

