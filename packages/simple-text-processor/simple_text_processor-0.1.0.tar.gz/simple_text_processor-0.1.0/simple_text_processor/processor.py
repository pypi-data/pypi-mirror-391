# -*- coding: utf-8 -*-
"""Core Processor Logic

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a3XuJpX7a2FGYD2uH4q50De-Yx5rDQgl
"""

import re
import numpy as np

def clean_text(text: str) -> str:
    """
    Cleans a single string of text by:
    1. Converting to lowercase.
    2. Removing all punctuation.
    3. Removing numerical digits (optional, but common for simple models).
    4. Removing extra whitespace.

    Args:
        text: The raw input string.

    Returns:
        The cleaned string.
    """
    # 1. Convert to lowercase
    text = text.lower()

    # 2. Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # 3. Remove numerical digits (e.g., '123' becomes '')
    text = re.sub(r'\d+', '', text)

    # 4. Remove extra whitespace and trim leading/trailing spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def simple_vectorize(documents: list[str]) -> tuple[np.ndarray, dict]:
    """
    Creates a simple Bag-of-Words (BoW) matrix from a list of documents.

    Steps:
    1. Clean each document using clean_text().
    2. Build a vocabulary of all unique words.
    3. Create a count vector for each document based on the vocabulary.

    Args:
        documents: A list of strings, where each string is a document.

    Returns:
        A tuple containing:
        - A NumPy array (N x V) where N is docs and V is vocab size.
        - A dictionary mapping words to their column index (vocabulary).
    """
    cleaned_docs = [clean_text(doc) for doc in documents]

    # Build vocabulary
    vocabulary = {}
    word_index = 0
    for doc in cleaned_docs:
        words = doc.split()
        for word in words:
            if word not in vocabulary:
                vocabulary[word] = word_index
                word_index += 1

    vocab_size = len(vocabulary)
    num_documents = len(documents)

    # Create the BoW matrix
    vector_matrix = np.zeros((num_documents, vocab_size), dtype=np.int32)

    for i, doc in enumerate(cleaned_docs):
        words = doc.split()
        for word in words:
            if word in vocabulary:
                j = vocabulary[word]
                vector_matrix[i, j] += 1

    return vector_matrix, vocabulary

# Example usage (will not run when imported as a library, only when run directly)
if __name__ == '__main__':
    # Test clean_text
    raw_text = "This is a great test, 2024 is the year!"
    print(f"Original: {raw_text}")
    print(f"Cleaned: {clean_text(raw_text)}")

    # Test simple_vectorize
    data = [
        "The cat sat on the mat.",
        "A dog ran over the rug.",
        "The cat and dog are friends."
    ]

    vectors, vocab = simple_vectorize(data)
    print("\n--- Vectorization Test ---")
    print("Vocabulary:", vocab)
    print("BoW Matrix:\n", vectors)