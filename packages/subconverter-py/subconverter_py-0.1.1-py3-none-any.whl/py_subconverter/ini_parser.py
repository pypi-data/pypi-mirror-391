#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
INI Configuration Parser for Subconverter
è§£æ subconverter çš„ INI é…ç½®æ–‡ä»¶
"""

import re
import urllib.request
import urllib.error
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass


@dataclass
class RuleSet:
    """è§„åˆ™é›†"""
    group: str  # ç­–ç•¥ç»„åç§°
    url: str    # è§„åˆ™ URL æˆ–è§„åˆ™å†…å®¹


@dataclass
class ProxyGroup:
    """ç­–ç•¥ç»„"""
    name: str           # ç»„åç§°
    type: str           # ç±»å‹: select, url-test, fallback, load-balance
    proxies: List[str]  # ä»£ç†åˆ—è¡¨
    url: Optional[str] = None      # å¥åº·æ£€æŸ¥ URL
    interval: Optional[int] = None # æ£€æŸ¥é—´éš”


class INIConfigParser:
    """INI é…ç½®è§£æå™¨"""

    # Clash æ”¯æŒçš„è§„åˆ™ç±»å‹
    # å‚è€ƒ: https://github.com/Dreamacro/clash/wiki/configuration
    SUPPORTED_RULE_TYPES = {
        'DOMAIN',
        'DOMAIN-SUFFIX',
        'DOMAIN-KEYWORD',
        'IP-CIDR',
        'IP-CIDR6',
        'GEOIP',
        'MATCH',
        'PROCESS-NAME',
        # 'SRC-IP-CIDR',  # æŸäº›ç‰ˆæœ¬æ”¯æŒ
        # 'SRC-PORT',
        # 'DST-PORT',
    }

    def __init__(self):
        self.rulesets: List[RuleSet] = []
        self.proxy_groups: List[ProxyGroup] = []
        self.custom_rules: List[str] = []

    def fetch_content(self, url: str) -> str:
        """è·å– URL å†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=30) as response:
                return response.read().decode('utf-8')
        except Exception as e:
            raise Exception(f"Failed to fetch {url}: {e}")

    def parse_ini_file(self, content: str) -> None:
        """è§£æ INI æ–‡ä»¶å†…å®¹"""
        current_section = None

        for line in content.split('\n'):
            line = line.strip()

            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not line or line.startswith('#') or line.startswith(';'):
                continue

            # æ£€æµ‹æ®µè½
            if line.startswith('[') and line.endswith(']'):
                current_section = line[1:-1].lower()
                continue

            # è§£æ custom æ®µè½
            if current_section == 'custom':
                self._parse_custom_line(line)

    def _parse_custom_line(self, line: str) -> None:
        """è§£æ custom æ®µè½çš„ä¸€è¡Œ"""
        # è§£æ ruleset
        if line.startswith('ruleset='):
            self._parse_ruleset(line[8:])  # ç§»é™¤ "ruleset="

        # è§£æ custom_proxy_group
        elif line.startswith('custom_proxy_group='):
            self._parse_proxy_group(line[19:])  # ç§»é™¤ "custom_proxy_group="

    def _parse_ruleset(self, content: str) -> None:
        """è§£æ ruleset è¡Œ

        æ ¼å¼: ruleset=ç­–ç•¥ç»„å,è§„åˆ™URLæˆ–å†…å®¹
        ç¤ºä¾‹:
          ruleset=ğŸ¯ å…¨çƒç›´è¿,https://example.com/rules.list
          ruleset=ğŸ¯ å…¨çƒç›´è¿,[]GEOIP,CN
        """
        parts = content.split(',', 1)
        if len(parts) != 2:
            return

        group = parts[0].strip()
        rule_content = parts[1].strip()

        self.rulesets.append(RuleSet(group=group, url=rule_content))

    def _parse_proxy_group(self, content: str) -> None:
        """è§£æ custom_proxy_group è¡Œ

        æ ¼å¼: ç»„å`ç±»å‹`å‚æ•°1`å‚æ•°2...
        ç¤ºä¾‹:
          ğŸš€ èŠ‚ç‚¹é€‰æ‹©`select`[]â™»ï¸ è‡ªåŠ¨é€‰æ‹©`[]ğŸ‡­ğŸ‡° é¦™æ¸¯èŠ‚ç‚¹`[]DIRECT
          â™»ï¸ è‡ªåŠ¨é€‰æ‹©`url-test`.*`http://www.gstatic.com/generate_204`300
        """
        # ä½¿ç”¨åå¼•å·åˆ†å‰²
        parts = content.split('`')
        if len(parts) < 2:
            return

        name = parts[0].strip()
        group_type = parts[1].strip()

        # è§£æä»£ç†åˆ—è¡¨
        proxies = []
        url = None
        interval = None

        for i in range(2, len(parts)):
            part = parts[i].strip()

            # è·³è¿‡ç©ºéƒ¨åˆ†
            if not part:
                continue

            # URL å¥åº·æ£€æŸ¥åœ°å€
            if part.startswith('http://') or part.startswith('https://'):
                url = part
            # é—´éš”æ—¶é—´ï¼ˆæ•°å­—ï¼‰
            elif part.isdigit():
                interval = int(part)
            # ä»£ç†æˆ–ä»£ç†ç»„å¼•ç”¨
            else:
                # ä¿æŒåŸæ ·ï¼ŒåŒ…æ‹¬ [] å‰ç¼€ï¼ˆç”¨äºåç»­è¯†åˆ«ç­–ç•¥ç»„å¼•ç”¨ï¼‰
                proxies.append(part)

        group = ProxyGroup(
            name=name,
            type=group_type,
            proxies=proxies,
            url=url,
            interval=interval
        )

        self.proxy_groups.append(group)

    def download_rulesets(self, verbose: bool = False) -> List[Tuple[str, List[str]]]:
        """ä¸‹è½½æ‰€æœ‰è§„åˆ™é›†

        Returns:
            List of (group_name, rules)
        """
        results = []

        for ruleset in self.rulesets:
            if verbose:
                print(f"  Downloading ruleset: {ruleset.group}")

            rules = []

            # å¦‚æœæ˜¯å†…è”è§„åˆ™ï¼ˆ[]å¼€å¤´ï¼‰
            if ruleset.url.startswith('[]'):
                # æ ¼å¼: []GEOIP,CN æˆ– []FINAL
                inline_rule = ruleset.url[2:]  # ç§»é™¤ []

                # è½¬æ¢ FINAL â†’ MATCH (Clash å…¼å®¹æ€§)
                if inline_rule.upper() == 'FINAL' or inline_rule.upper().startswith('FINAL,'):
                    inline_rule = inline_rule.replace('FINAL', 'MATCH', 1)
                    inline_rule = inline_rule.replace('final', 'MATCH', 1)

                if ',' in inline_rule:
                    rules.append(f"{inline_rule},{ruleset.group}")
                else:
                    rules.append(f"{inline_rule},{ruleset.group}")

            # å¦‚æœæ˜¯ URL
            elif ruleset.url.startswith('http://') or ruleset.url.startswith('https://'):
                try:
                    content = self.fetch_content(ruleset.url)
                    # è§£æè§„åˆ™æ–‡ä»¶
                    for line in content.split('\n'):
                        line = line.strip()
                        # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                        if not line or line.startswith('#') or line.startswith(';'):
                            continue

                        # æ£€æŸ¥è§„åˆ™ç±»å‹æ˜¯å¦æ”¯æŒ
                        parts = line.split(',')
                        if not parts:
                            continue

                        rule_type = parts[0].strip().upper()

                        # å¤„ç†è§„åˆ™ç±»å‹è½¬æ¢
                        # FINAL -> MATCH (Clash å…¼å®¹æ€§)
                        if rule_type == 'FINAL':
                            rule_type = 'MATCH'
                            parts[0] = 'MATCH'
                            line = ','.join(parts)

                        # è¿‡æ»¤ä¸æ”¯æŒçš„è§„åˆ™ç±»å‹
                        if rule_type not in self.SUPPORTED_RULE_TYPES:
                            if verbose:
                                print(f"    Skipped unsupported rule: {line[:60]}...")
                            continue

                        # æ£€æŸ¥è§„åˆ™æ˜¯å¦å·²ç»æœ‰ç­–ç•¥ç»„ï¼Œå¹¶åœ¨æ­£ç¡®ä½ç½®æ’å…¥
                        # Clashè§„åˆ™æ ¼å¼: TYPE,VALUE,GROUP[,OPTIONS]
                        # ä¾‹å¦‚: DOMAIN-SUFFIX,google.com,PROXY
                        #       IP-CIDR,1.1.1.1/32,PROXY,no-resolve
                        # parts å·²ç»åœ¨ä¸Šé¢ split è¿‡äº†

                        # å¸¸è§çš„é€‰é¡¹å…³é”®å­—ï¼ˆä¸æ˜¯ç­–ç•¥ç»„åç§°ï¼‰
                        known_options = {'no-resolve'}

                        # åˆ¤æ–­æ˜¯å¦å·²æœ‰ç­–ç•¥ç»„ï¼š
                        # - è‡³å°‘3éƒ¨åˆ†
                        # - ç¬¬3éƒ¨åˆ†ä¸æ˜¯known_optionsä¸­çš„é€‰é¡¹
                        has_group = False
                        if len(parts) >= 3:
                            third_part = parts[2].strip()
                            if third_part not in known_options:
                                has_group = True

                        if has_group:
                            # å·²ç»æœ‰ç­–ç•¥ç»„ï¼Œä¿æŒåŸæ ·
                            rules.append(line)
                        else:
                            # æ²¡æœ‰ç­–ç•¥ç»„ï¼Œéœ€è¦åœ¨æ­£ç¡®ä½ç½®æ’å…¥
                            # æ ¼å¼ï¼šTYPE,VALUE,GROUP[,OPTIONS]
                            # ç­–ç•¥ç»„åº”è¯¥åœ¨ç¬¬3ä½ï¼ˆindex 2ï¼‰ï¼Œé€‰é¡¹åœ¨ç¬¬4ä½åŠä¹‹å
                            if len(parts) == 2:
                                # TYPE,VALUE â†’ TYPE,VALUE,GROUP
                                rules.append(f"{line},{ruleset.group}")
                            elif len(parts) >= 3 and parts[2].strip() in known_options:
                                # TYPE,VALUE,no-resolve â†’ TYPE,VALUE,GROUP,no-resolve
                                # æ’å…¥ç­–ç•¥ç»„åœ¨é€‰é¡¹ä¹‹å‰
                                new_parts = parts[:2] + [ruleset.group] + parts[2:]
                                rules.append(','.join(new_parts))
                            else:
                                # å…¶ä»–æƒ…å†µï¼Œæ·»åŠ åˆ°æœ«å°¾
                                rules.append(f"{line},{ruleset.group}")

                    if verbose:
                        print(f"    Loaded {len(rules)} rules")
                except Exception as e:
                    if verbose:
                        print(f"    Failed to load: {e}")

            if rules:
                results.append((ruleset.group, rules))

        return results

    def resolve_proxy_groups(self, proxy_names: List[str]) -> List[ProxyGroup]:
        """è§£æä»£ç†ç»„ï¼Œå¤„ç†å¼•ç”¨å’Œæ­£åˆ™åŒ¹é…

        Args:
            proxy_names: å®é™…çš„ä»£ç†èŠ‚ç‚¹åç§°åˆ—è¡¨

        Returns:
            è§£æåçš„ä»£ç†ç»„åˆ—è¡¨
        """
        resolved_groups = []

        for group in self.proxy_groups:
            resolved_proxies = []

            for proxy_ref in group.proxies:
                # ç­–ç•¥ç»„å¼•ç”¨ï¼ˆ[]å¼€å¤´ï¼‰
                if proxy_ref.startswith('[]'):
                    # ç§»é™¤ [] å‰ç¼€ï¼Œå¾—åˆ°ç­–ç•¥ç»„åç§°
                    group_name = proxy_ref[2:]
                    resolved_proxies.append(group_name)
                # ç‰¹æ®Šå€¼ï¼ˆDIRECT, REJECTï¼‰
                elif proxy_ref in ['DIRECT', 'REJECT']:
                    resolved_proxies.append(proxy_ref)
                # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…èŠ‚ç‚¹
                else:
                    # å°è¯•ä½œä¸ºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…èŠ‚ç‚¹
                    try:
                        pattern = re.compile(proxy_ref)
                        matched = [name for name in proxy_names if pattern.search(name)]
                        resolved_proxies.extend(matched)
                    except re.error:
                        # ä¸æ˜¯æœ‰æ•ˆçš„æ­£åˆ™ï¼Œå½“ä½œæ™®é€šä»£ç†å
                        if proxy_ref in proxy_names:
                            resolved_proxies.append(proxy_ref)

            # å»é‡ä½†ä¿æŒé¡ºåº
            seen = set()
            unique_proxies = []
            for p in resolved_proxies:
                if p not in seen:
                    seen.add(p)
                    unique_proxies.append(p)

            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•ä»£ç†ï¼Œæ·»åŠ  DIRECT ä½œä¸ºé»˜è®¤å€¼
            # Clash è¦æ±‚æ¯ä¸ªç­–ç•¥ç»„è‡³å°‘æœ‰ä¸€ä¸ª proxy
            if not unique_proxies:
                unique_proxies = ['DIRECT']

            resolved_group = ProxyGroup(
                name=group.name,
                type=group.type,
                proxies=unique_proxies,
                url=group.url,
                interval=group.interval
            )
            resolved_groups.append(resolved_group)

        return resolved_groups

    def to_clash_proxy_groups(self, proxy_names: List[str]) -> List[Dict]:
        """è½¬æ¢ä¸º Clash proxy-groups æ ¼å¼"""
        resolved_groups = self.resolve_proxy_groups(proxy_names)
        clash_groups = []

        for group in resolved_groups:
            clash_group = {
                'name': group.name,
                'type': group.type,
                'proxies': group.proxies
            }

            # æ·»åŠ å¥åº·æ£€æŸ¥å‚æ•°
            if group.type in ['url-test', 'fallback', 'load-balance']:
                if group.url:
                    clash_group['url'] = group.url
                else:
                    clash_group['url'] = 'http://www.gstatic.com/generate_204'

                if group.interval:
                    clash_group['interval'] = group.interval
                else:
                    clash_group['interval'] = 300

            clash_groups.append(clash_group)

        return clash_groups


def parse_ini_config(ini_url: str, verbose: bool = False) -> INIConfigParser:
    """è§£æ INI é…ç½®æ–‡ä»¶

    Args:
        ini_url: INI é…ç½®æ–‡ä»¶çš„ URL
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        INIConfigParser å®ä¾‹
    """
    parser = INIConfigParser()

    if verbose:
        print(f"Fetching INI config from: {ini_url}")

    # ä¸‹è½½ INI æ–‡ä»¶
    content = parser.fetch_content(ini_url)

    if verbose:
        print(f"INI config size: {len(content)} bytes")

    # è§£æ INI æ–‡ä»¶
    parser.parse_ini_file(content)

    if verbose:
        print(f"Parsed {len(parser.rulesets)} rulesets")
        print(f"Parsed {len(parser.proxy_groups)} proxy groups")

    return parser


if __name__ == "__main__":
    # æµ‹è¯•
    test_ini = """
[custom]
ruleset=ğŸ¯ å…¨çƒç›´è¿,https://raw.githubusercontent.com/ACL4SSR/ACL4SSR/master/Clash/LocalAreaNetwork.list
ruleset=ğŸ›‘ å¹¿å‘Šæ‹¦æˆª,https://raw.githubusercontent.com/ACL4SSR/ACL4SSR/master/Clash/BanAD.list
ruleset=ğŸ¯ å…¨çƒç›´è¿,[]GEOIP,CN
ruleset=ğŸŸ æ¼ç½‘ä¹‹é±¼,[]FINAL

custom_proxy_group=ğŸš€ èŠ‚ç‚¹é€‰æ‹©`select`[]â™»ï¸ è‡ªåŠ¨é€‰æ‹©`[]ğŸ‡­ğŸ‡° é¦™æ¸¯èŠ‚ç‚¹`[]DIRECT
custom_proxy_group=â™»ï¸ è‡ªåŠ¨é€‰æ‹©`url-test`.*`http://www.gstatic.com/generate_204`300
custom_proxy_group=ğŸ‡­ğŸ‡° é¦™æ¸¯èŠ‚ç‚¹`url-test`é¦™æ¸¯|HK`http://www.gstatic.com/generate_204`300
custom_proxy_group=ğŸ¯ å…¨çƒç›´è¿`select`[]DIRECT
custom_proxy_group=ğŸ›‘ å¹¿å‘Šæ‹¦æˆª`select`[]REJECT
custom_proxy_group=ğŸŸ æ¼ç½‘ä¹‹é±¼`select`[]ğŸš€ èŠ‚ç‚¹é€‰æ‹©`[]DIRECT
"""

    parser = INIConfigParser()
    parser.parse_ini_file(test_ini)

    print(f"\nParsed {len(parser.rulesets)} rulesets:")
    for rs in parser.rulesets:
        print(f"  {rs.group}: {rs.url[:50]}...")

    print(f"\nParsed {len(parser.proxy_groups)} proxy groups:")
    for pg in parser.proxy_groups:
        print(f"  {pg.name} ({pg.type}): {len(pg.proxies)} proxies")

    # æµ‹è¯•èŠ‚ç‚¹åŒ¹é…
    test_proxies = ["é¦™æ¸¯ 01", "é¦™æ¸¯ 02", "ç¾å›½ 01", "æ—¥æœ¬ 01"]
    resolved = parser.resolve_proxy_groups(test_proxies)

    print(f"\nResolved proxy groups with test proxies:")
    for pg in resolved:
        print(f"  {pg.name}: {pg.proxies}")
