# H-Matrix MatVec Performance Optimization

**Date**: 2025-11-13
**Version**: 1.1.1 (Performance Fix)
**File**: `src/ext/HACApK_LH-Cimplm/hacapk.cpp`

## Problem Discovery

### Performance Issue

Benchmark results showed H-matrix providing **minimal performance improvement** over Gauss-Seidel:

```
Problem Size: N=10,648 elements
------------------------------
Gauss-Seidel:  509.16 ms/iteration
H-matrix:      501.20 ms/iteration (1.02x speedup)

Expected: 5-10x speedup
Actual:   1.02x speedup (1.6% improvement)
```

**Scaling exponents**:
- Gauss-Seidel: alpha = 1.735 (expected ~2.0)
- H-matrix: alpha = 1.645 (expected ~1.3-1.5)

Both methods showed nearly identical scaling, indicating **H-matrix was not providing algorithmic advantage**.

## Root Cause Analysis

### Code Inspection

File: `src/ext/HACApK_LH-Cimplm/hacapk.cpp` lines 598-662

```cpp
void hmatrix_matvec(...) {
    #pragma omp parallel for
    for (int b = 0; b < nblocks; b++) {
        // ... compute block contribution to y_local ...

        // PROBLEM: Critical section serializes all threads!
        #pragma omp critical
        {
            for (size_t i = 0; i < y.size(); i++) {
                y[i] += y_local[i];
            }
        }
    }
}
```

### Performance Impact

**Critical section bottleneck**:
- **nblocks = 2,214,144** for N=10,648
- Each block computation requires entering critical section
- All threads wait for access to shared `y` array
- **Effective parallelization: ~1.0x** (serialized)

**Memory waste**:
- `y_local(y.size())` allocated for every block
- N=10,648: 2.2M blocks × 10,648 elements × 8 bytes = **178 GB** temporary storage!
- Constant allocation/deallocation overhead

## Solution

### Optimized Implementation

**Key changes**:
1. **Eliminate critical sections**: Replace with atomic operations
2. **Direct accumulation**: No temporary full-size arrays
3. **Dynamic scheduling**: Better load balancing for variable block sizes

```cpp
void hmatrix_matvec(...) {
    // OPTIMIZED: Use atomic operations instead of critical sections
    #pragma omp parallel for schedule(dynamic, 64)
    for (int b = 0; b < nblocks; b++) {
        const LowRankBlock& block = hmat.blocks[b];

        if (block.is_lowrank()) {
            // Compute low-rank: y = U * (V^T * x)
            // ... compute contribution 'sum' ...

            // Atomic accumulation - no waiting!
            #pragma omp atomic
            y[block.nstrtl + i] += sum;
        }
        else if (block.is_full()) {
            // Full block
            // ... compute contribution 'sum' ...

            // Atomic accumulation - no waiting!
            #pragma omp atomic
            y[block.nstrtl + i] += sum;
        }
    }
}
```

### Benefits

1. **Lock-free parallelization**: Threads don't wait for each other
2. **Memory efficiency**: No y_local arrays needed
3. **Cache-friendly**: Direct writes to output vector
4. **Scalable**: Performance scales with thread count

## Expected Performance Improvement

### Per-Iteration Time (N=10,648)

| Implementation | Time | Speedup |
|----------------|------|---------|
| **Before (Critical Section)** | 501 ms | 1.02x vs GS |
| **After (Atomic Ops)** | 50-100 ms | **5-10x vs GS** |

### Scaling Exponent

| Implementation | Alpha | Complexity |
|----------------|-------|------------|
| **Before** | 1.645 | ~O(N^1.65) - same as GS |
| **After** | 1.3-1.5 | ~O(N^1.3-1.5) - true H-matrix |

### Overall Impact

**For typical simulation (N=10,648)**:
- Construction time: ~600s (unchanged)
- Per-iteration time: 501ms → **50-100ms** (5-10x faster)
- 100 iterations: 50s → **5-10s** (overall 5-10x speedup)

## Verification

### Quick Test (N=512)

Run crossover-point test to verify optimization:

```bash
cd S:/Radia/01_GitHub
python examples/solver_benchmarks/benchmark_solver_comparison.py
```

**Expected results**:
- N=512: H-matrix should be **2-3x faster** than Gauss-Seidel
- N=1000: H-matrix should be **3-5x faster**
- N=10648: H-matrix should be **5-10x faster**

### Full Benchmark

Re-run comprehensive benchmark:

```bash
python examples/solver_benchmarks/benchmark_large_scale_comparison.py
```

**Expected improvements**:
- Scaling exponent: alpha = 1.3-1.5 (was 1.645)
- Crossover point: N=200-300 (was N=512)
- Large problem speedup: 5-10x (was 1.02x)

## Technical Details

### Why Atomic Operations Work

**Atomic operation characteristics**:
- **Lock-free**: No thread synchronization overhead
- **Hardware support**: Modern CPUs have atomic ADD instructions
- **Fine-grained**: Per-element locking, not whole-array
- **Scalable**: Overhead constant regardless of array size

**Comparison**:

| Method | Synchronization | Overhead | Scalability |
|--------|----------------|----------|-------------|
| Critical section | Coarse-grained lock | O(N × blocks) | Poor |
| Atomic ops | Per-element lock-free | O(blocks) | Excellent |

### Trade-offs

**Advantages**:
- ✅ Eliminates serialization bottleneck
- ✅ Reduces memory footprint (no y_local arrays)
- ✅ Better cache locality
- ✅ Scales with thread count

**Potential concerns**:
- ⚠️ Atomic operations have slight overhead vs non-atomic
- ⚠️ False sharing possible (mitigated by dynamic scheduling)
- ✅ Overall: Benefits >> Costs for large problems

## Files Modified

- `src/ext/HACApK_LH-Cimplm/hacapk.cpp` (lines 598-662)
  - Function: `hmatrix_matvec()`
  - Changes: Critical sections → Atomic operations
  - Impact: 5-10x speedup for large problems

## References

- [H-Matrix Implementation History](HMATRIX_IMPLEMENTATION_HISTORY.md)
- [H-Matrix Serialization Guide](HMATRIX_SERIALIZATION.md)
- [Benchmark Results](HMATRIX_BENCHMARKS_RESULTS.md)
- OpenMP Atomic Operations: https://www.openmp.org/spec-html/5.0/openmpsu95.html

## Version History

- **v1.1.0** (2025-11-09): H-matrix serialization (Phase 3B)
- **v1.1.1** (2025-11-13): H-matrix MatVec optimization (this document)

---

**Author**: Claude Code
**Discovered by**: User benchmarking analysis
**Root cause**: Critical section serialization in `hmatrix_matvec()`
**Fix**: Atomic operations for lock-free parallelization
