"""
ğŸš€ Apple MLX ìµœì í™”ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°

ë¹ ë¥¸ ì¶”ì¶œì  ìš”ì•½ê³¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¡œ embedding ì„±ëŠ¥ ê·¹ëŒ€í™”! ğŸ”¥
Apple Siliconì˜ unified memory architectureë¥¼ í™œìš©í•œ ì´ˆê³ ì† ì²˜ë¦¬!
"""

import logging
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


@dataclass
class TextProcessingResult:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê²°ê³¼ ì •ë³´"""

    original_text: str
    processed_text: str
    original_tokens: int
    processed_tokens: int
    truncated: bool
    strategy_used: str
    warnings: List[str]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "original_tokens": self.original_tokens,
            "processed_tokens": self.processed_tokens,
            "truncated": self.truncated,
            "strategy": self.strategy_used,
            "warnings": self.warnings,
            "token_reduction": self.original_tokens - self.processed_tokens,
            "reduction_percentage": (
                round((self.original_tokens - self.processed_tokens) / self.original_tokens * 100, 1)
                if self.original_tokens > 0
                else 0
            ),
        }


class TextSummarizer:
    """ğŸš€ Apple MLX ìµœì í™”ëœ í…ìŠ¤íŠ¸ ìš”ì•½ ìœ í‹¸ë¦¬í‹°

    ë¹ ë¥¸ ì¶”ì¶œì  ìš”ì•½ìœ¼ë¡œ embedding ì„±ëŠ¥ ê·¹ëŒ€í™”! ğŸ”¥
    Apple Siliconì˜ ê°•ë ¥í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„±ëŠ¥ì„ í™œìš©í•©ë‹ˆë‹¤.
    """

    @staticmethod
    def truncate_by_tokens(text: str, max_tokens: int = 512) -> str:
        """í† í° ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìë¥´ê¸° - ì´ˆê³ ì† ì²˜ë¦¬! âš¡

        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜

        Returns:
            ìë¥¸ í…ìŠ¤íŠ¸ (ë‹¨ì–´ ê²½ê³„ ë³´ì¡´)
        """
        # ëŒ€ëµì ì¸ í† í° ê³„ì‚° (1 í† í° â‰ˆ 4 ê¸€ì)
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text

        # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        truncated = text[:max_chars]
        # ë§ˆì§€ë§‰ ê³µë°±ì—ì„œ ìë¥´ê¸° (ë‹¨ì–´ ì¤‘ê°„ì—ì„œ ëŠì–´ì§€ì§€ ì•Šë„ë¡)
        last_space = truncated.rfind(' ')
        if last_space > max_chars * 0.8:  # ë„ˆë¬´ ë§ì´ ìë¥´ì§€ ì•Šë„ë¡
            truncated = truncated[:last_space]

        return truncated + "..."

    @staticmethod
    def extract_key_sentences(text: str, max_sentences: int = 3) -> str:
        """í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ - MLX ë°±ì—”ë“œ ì¹œí™”ì ! ğŸ¯

        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            max_sentences: ìµœëŒ€ ë¬¸ì¥ ìˆ˜

        Returns:
            ì¶”ì¶œëœ í•µì‹¬ ë¬¸ì¥ë“¤
        """
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) <= max_sentences:
            return text

        # ì²« ë¬¸ì¥ + ì¤‘ê°„ ë¬¸ì¥ë“¤ ì„ íƒ
        selected = [sentences[0]]
        if max_sentences > 1:
            mid_start = len(sentences) // 3
            selected.extend(sentences[mid_start : mid_start + max_sentences - 1])

        return '. '.join(selected) + '.'

    @staticmethod
    def smart_truncate(text: str, max_tokens: int = 512) -> str:
        """ğŸ§  ìŠ¤ë§ˆíŠ¸ ìš”ì•½ - ë¬¸ì¥ ê²½ê³„ ë³´ì¡´í•˜ë©° ìë¥´ê¸°

        Apple MLX ì„±ëŠ¥ì„ ìœ„í•´ ìµœì í™”ëœ ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ìë¥´ê¸°!
        ë¬¸ì¥ ì™„ì„±ë„ë¥¼ ë³´ì¥í•˜ë©´ì„œ í† í° ì œí•œì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤.

        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜

        Returns:
            ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ìë¥¸ í…ìŠ¤íŠ¸
        """
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° ì‹œë„
        sentences = re.split(r'[.!?]+', text)
        result = ""
        char_count = 0

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ ì²´í¬
            if char_count + len(sentence) + 2 > max_chars:  # +2 for ". "
                break

            result += sentence + ". "
            char_count += len(sentence) + 2

        # ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´ ê°•ì œë¡œ ìë¥´ê¸°
        if not result.strip():
            return TextSummarizer.truncate_by_tokens(text, max_tokens)

        return result.strip()

    @staticmethod
    def validate_and_process_text(
        text: str, max_tokens: int, strategy: str = "smart_truncate", return_processing_info: bool = False
    ) -> str | Tuple[str, TextProcessingResult]:
        """ğŸ“ í…ìŠ¤íŠ¸ ê²€ì¦ ë° ì²˜ë¦¬ - ê°œì„ ëœ All-in-One ì†”ë£¨ì…˜! ğŸš€

        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            strategy: ì²˜ë¦¬ ì „ëµ ("truncate", "extract", "smart_truncate", "error")
            return_processing_info: ì²˜ë¦¬ ì •ë³´ ë°˜í™˜ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” (ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸, ì²˜ë¦¬ ì •ë³´) íŠœí”Œ

        Raises:
            ValueError: ë¹ˆ í…ìŠ¤íŠ¸ì´ê±°ë‚˜ strategyê°€ "error"ì´ê³  í† í° í•œê³„ ì´ˆê³¼ì¸ ê²½ìš°
        """
        if not text or not text.strip():
            raise ValueError("Text cannot be empty")

        text = text.strip()

        # í† í° ê¸¸ì´ ì¶”ì • (1 í† í° â‰ˆ 4 ê¸€ì)
        original_tokens = len(text) // 4
        warnings = []

        # ì²˜ë¦¬ ê²°ê³¼ ì´ˆê¸°í™”
        result = TextProcessingResult(
            original_text=text,
            processed_text=text,
            original_tokens=original_tokens,
            processed_tokens=original_tokens,
            truncated=False,
            strategy_used=strategy,
            warnings=warnings,
        )

        if original_tokens <= max_tokens:
            # í† í° ì œí•œ ë‚´ - ê·¸ëŒ€ë¡œ ë°˜í™˜
            if return_processing_info:
                return text, result
            return text

        # í† í° ì œí•œ ì´ˆê³¼ ì²˜ë¦¬
        logger.info(f"ğŸ”§ Text too long ({original_tokens} > {max_tokens} tokens), applying {strategy}")

        if strategy == "error":
            raise ValueError(f"Text exceeds maximum token limit: {original_tokens} > {max_tokens} tokens")

        # ì „ëµì— ë”°ë¥¸ ì²˜ë¦¬
        if strategy == "truncate":
            processed_text = TextSummarizer.truncate_by_tokens(text, max_tokens)
        elif strategy == "extract":
            # ë¬¸ì¥ ìˆ˜ë¥¼ í† í° ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
            max_sentences = max(1, max_tokens // 100)
            processed_text = TextSummarizer.extract_key_sentences(text, max_sentences)
        else:  # smart_truncate (default)
            processed_text = TextSummarizer.smart_truncate(text, max_tokens)

        # ì²˜ë¦¬ ê²°ê³¼ ì—…ë°ì´íŠ¸
        result.processed_text = processed_text
        result.processed_tokens = len(processed_text) // 4
        result.truncated = True

        # ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
        if original_tokens > max_tokens * 2:
            warnings.append(f"Text was significantly longer than recommended ({original_tokens} tokens)")

        if result.processed_tokens < original_tokens * 0.5:
            warnings.append("More than 50% of original text was removed")

        if return_processing_info:
            return processed_text, result
        return processed_text

    @staticmethod
    def process_texts_with_options(
        texts: List[str],
        max_tokens: int,
        absolute_max_tokens: int,
        strategy: str = "smart_truncate",
        auto_truncate: bool = True,
        return_processing_info: bool = False,
    ) -> Tuple[List[str], Optional[List[TextProcessingResult]]]:
        """ğŸš€ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì¼ê´„ ì²˜ë¦¬ - ê³ ê¸‰ ì˜µì…˜ ì§€ì›! âš¡

        Args:
            texts: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_tokens: ê¶Œì¥ ìµœëŒ€ í† í° ìˆ˜
            absolute_max_tokens: ì ˆëŒ€ ìµœëŒ€ í† í° ìˆ˜ (ì´ˆê³¼ì‹œ ì—ëŸ¬)
            strategy: ì²˜ë¦¬ ì „ëµ
            auto_truncate: ìë™ ì¶•ì†Œ í™œì„±í™” ì—¬ë¶€
            return_processing_info: ì²˜ë¦¬ ì •ë³´ ë°˜í™˜ ì—¬ë¶€

        Returns:
            (ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸, ì²˜ë¦¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸)

        Raises:
            ValueError: ì ˆëŒ€ ìµœëŒ€ í† í° ì´ˆê³¼ ë˜ëŠ” auto_truncate=Falseì¸ë° í† í° í•œê³„ ì´ˆê³¼
        """
        processed_texts = []
        processing_infos = [] if return_processing_info else None

        for i, text in enumerate(texts):
            try:
                # ì ˆëŒ€ ìµœëŒ€ ê¸¸ì´ ì²´í¬
                estimated_tokens = len(text) // 4

                if estimated_tokens > absolute_max_tokens:
                    raise ValueError(
                        f"Text at index {i} exceeds absolute maximum token limit: "
                        f"{estimated_tokens} > {absolute_max_tokens} tokens. "
                        f"Please split the text into smaller chunks."
                    )

                # ê¶Œì¥ ê¸¸ì´ ì²´í¬
                if estimated_tokens > max_tokens:
                    if not auto_truncate:
                        raise ValueError(
                            f"Text at index {i} exceeds recommended token limit: "
                            f"{estimated_tokens} > {max_tokens} tokens. "
                            f"Enable auto_truncate or reduce text length."
                        )

                    # ìë™ ì¶•ì†Œ ì²˜ë¦¬
                    if return_processing_info:
                        processed_text, processing_info = TextSummarizer.validate_and_process_text(
                            text, max_tokens, strategy, return_processing_info=True
                        )
                        processing_infos.append(processing_info)
                    else:
                        processed_text = TextSummarizer.validate_and_process_text(
                            text, max_tokens, strategy, return_processing_info=False
                        )

                    processed_texts.append(processed_text)
                else:
                    # í† í° ì œí•œ ë‚´ - ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                    processed_texts.append(text)

                    if return_processing_info:
                        processing_infos.append(
                            TextProcessingResult(
                                original_text=text,
                                processed_text=text,
                                original_tokens=estimated_tokens,
                                processed_tokens=estimated_tokens,
                                truncated=False,
                                strategy_used="none",
                                warnings=[],
                            )
                        )

            except Exception as e:
                logger.error(f"ğŸ’¥ Text processing failed for text {i}: {e}")
                raise

        return processed_texts, processing_infos
