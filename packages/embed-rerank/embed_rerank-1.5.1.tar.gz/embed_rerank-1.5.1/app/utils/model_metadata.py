"""
ğŸ§  Apple MLX ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹°

MLX í”„ë ˆì„ì›Œí¬ì˜ unified memory architectureë¥¼ í™œìš©í•œ
ì´ˆê³ ì† ëª¨ë¸ ì •ë³´ ë¡œë”© ë° ë™ì  ì„œë¹„ìŠ¤ êµ¬ì„±! ğŸš€
"""

import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, Optional

logger = logging.getLogger(__name__)


class ModelMetadataExtractor:
    """ğŸ” Apple MLX ìµœì í™”ëœ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°

    Apple Siliconì˜ ê°•ë ¥í•œ I/O ì„±ëŠ¥ì„ í™œìš©í•˜ì—¬
    ëª¨ë¸ ì„¤ì •ì„ sub-millisecondë¡œ ë¡œë”©! âš¡
    """

    @staticmethod
    def get_model_cache_path(model_name: str) -> Optional[Path]:
        """ğŸ—‚ï¸ Hugging Face ìºì‹œì—ì„œ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° - Apple Silicon ìµœì í™”!"""
        try:
            # HF ìºì‹œ ê²½ë¡œ í™•ì¸
            cache_dir = Path.home() / ".cache" / "huggingface" / "hub"

            # ëª¨ë¸ ì´ë¦„ì„ íŒŒì¼ì‹œìŠ¤í…œ ì•ˆì „í•œ í˜•íƒœë¡œ ë³€í™˜
            safe_model_name = model_name.replace("/", "--")

            # ìºì‹œì—ì„œ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
            model_dirs = list(cache_dir.glob(f"models--{safe_model_name}"))

            if model_dirs:
                # ê°€ì¥ ìµœì‹  ìŠ¤ëƒ…ìƒ· ê²½ë¡œ ë°˜í™˜
                snapshots_dir = model_dirs[0] / "snapshots"
                if snapshots_dir.exists():
                    latest_snapshot = max(snapshots_dir.iterdir(), key=os.path.getctime)
                    logger.info(f"ğŸš€ Found MLX model cache: {latest_snapshot}")
                    return latest_snapshot

        except Exception as e:
            logger.warning(f"âš ï¸ Cache path detection failed: {e}")

        return None

    @staticmethod
    async def extract_metadata(model_path_or_name: str) -> Dict[str, Any]:
        """âš¡ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ - Apple MLX ìµœì í™”!

        ëª¨ë¸ ì„¤ì •ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ì„œë¹„ìŠ¤ ë™ì  êµ¬ì„±ì— í™œìš©í•©ë‹ˆë‹¤.

        Args:
            model_path_or_name: ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HF ëª¨ë¸ ì´ë¦„

        Returns:
            ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        metadata = {
            "embedding_dimension": 4096,  # ê¸°ë³¸ê°’
            "max_position_embeddings": 32768,  # ê¸°ë³¸ê°’
            "recommended_max_tokens": 2048,  # ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ê°’
            "vocab_size": 151665,  # ê¸°ë³¸ê°’
            "model_type": "unknown",
            "hidden_size": 4096,  # ê¸°ë³¸ê°’
            "source": "default",
            # Reranker-specific defaults (will be used when task == 'rerank')
            "task": "embed",  # embed | rerank
            "max_seq_len_pair": 1024,
            "truncation": {"strategy": "pairwise_head+tail", "query_priority": True},
            "score_range": "raw",  # raw | sigmoid | softmax_pair
        }

        try:
            # 1. ë¡œì»¬ ê²½ë¡œì—ì„œ config.json ì°¾ê¸°
            config_path = None

            if os.path.isdir(model_path_or_name):
                config_path = Path(model_path_or_name) / "config.json"
            else:
                # 2. HF ìºì‹œì—ì„œ ì°¾ê¸°
                cache_path = ModelMetadataExtractor.get_model_cache_path(model_path_or_name)
                if cache_path:
                    config_path = cache_path / "config.json"

            # 3. config.json íŒŒì‹±
            if config_path and config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)

                logger.info(f"âœ… Loaded config.json from {config_path}")

                # í•µì‹¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata.update(
                    {
                        "hidden_size": config.get("hidden_size", 4096),
                        "max_position_embeddings": config.get("max_position_embeddings", 32768),
                        "vocab_size": config.get("vocab_size", 151665),
                        "model_type": config.get("model_type", "unknown"),
                        "source": "config.json",
                    }
                )

                # Detect reranker models heuristically by name/type
                name_l = str(model_path_or_name).lower()
                is_reranker = (
                    "rerank" in name_l
                    or "cross-encoder" in name_l
                    or config.get("architectures", ["unknown"])[0].lower().find("cross") >= 0
                )
                if is_reranker:
                    metadata["task"] = "rerank"
                    # Pair length defaults; prefer tokenizer_config override below
                    metadata["max_seq_len_pair"] = min(int(metadata.get("max_position_embeddings", 2048)), 4096)

                # ì„ë² ë”© ì°¨ì› ê²°ì • (ìš°ì„ ìˆœìœ„: hidden_size > d_model > embedding_size > model_dim > dim)
                candidate_keys = [
                    "hidden_size",
                    "d_model",
                    "embedding_size",
                    "model_dim",
                    "dim",
                ]
                for k in candidate_keys:
                    if k in config and isinstance(config[k], int):
                        metadata["embedding_dimension"] = int(config[k])
                        break

                # ê¶Œì¥ ìµœëŒ€ í† í° ê³„ì‚° (ì„±ëŠ¥ ìµœì í™”)
                max_pos = metadata["max_position_embeddings"]
                hidden_size = metadata["hidden_size"]

                # ğŸš€ Apple MLX ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ê¶Œì¥ê°’ ê³„ì‚°
                if max_pos >= 32768:  # ê¸´ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ (Qwen3ì²˜ëŸ¼)
                    if hidden_size >= 4096:
                        metadata["recommended_max_tokens"] = 2048  # ëŒ€í˜• ëª¨ë¸
                    elif hidden_size >= 2048:
                        metadata["recommended_max_tokens"] = 2048  # ì¤‘í˜• ëª¨ë¸ë„ 2048 ì§€ì›
                    else:
                        metadata["recommended_max_tokens"] = 1024  # ì†Œí˜• ëª¨ë¸
                elif max_pos >= 8192:
                    metadata["recommended_max_tokens"] = 1024  # ì¤‘ê°„ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸
                else:
                    metadata["recommended_max_tokens"] = 512  # ì§§ì€ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸

                logger.info(
                    f"ğŸ¯ MLX Model Metadata Extracted - "
                    f"dimension={metadata['embedding_dimension']}, "
                    f"max_tokens={metadata['recommended_max_tokens']}, "
                    f"model_type={metadata['model_type']}"
                )

            # 4. tokenizer_config.jsonì—ì„œ ì¶”ê°€ ì •ë³´ í™•ì¸
            if config_path:
                tokenizer_config_path = config_path.parent / "tokenizer_config.json"
                if tokenizer_config_path.exists():
                    with open(tokenizer_config_path, 'r', encoding='utf-8') as f:
                        tokenizer_config = json.load(f)

                    # í† í¬ë‚˜ì´ì € ìµœëŒ€ ê¸¸ì´ í™•ì¸
                    model_max_length = tokenizer_config.get("model_max_length")
                    if model_max_length and isinstance(model_max_length, int):
                        # í† í¬ë‚˜ì´ì € ì œí•œì´ ë” ì‘ë‹¤ë©´ ê·¸ê²ƒì„ ìš°ì„ 
                        if model_max_length < metadata["max_position_embeddings"]:
                            metadata["max_position_embeddings"] = model_max_length
                            logger.info(f"ğŸ“ Updated max tokens from tokenizer: {model_max_length}")

                        # If reranker, set pair length accordingly
                        if metadata.get("task") == "rerank":
                            metadata["max_seq_len_pair"] = min(model_max_length, 4096)

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract metadata, using defaults: {e}")

        return metadata

    def extract_metadata_from_path(self, model_path: str) -> Dict[str, Any]:
        """
        ëª¨ë¸ ê²½ë¡œì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            model_path: ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œ

        Returns:
            ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            # pathlib.Path ê°ì²´ë¡œ ë³€í™˜
            from pathlib import Path

            path = Path(model_path)

            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° êµ¬ì¡°
            metadata = {
                "embedding_dimension": 4096,  # ê¸°ë³¸ê°’
                "max_position_embeddings": 8192,
                "recommended_max_tokens": 2048,
                "absolute_max_tokens": 8192,
                "warning_threshold": 4096,
                "model_type": "unknown",
                "architecture": "unknown",
                "vocab_size": 32000,
            }

            # config.json íŒŒì¼ í™•ì¸
            config_file = path / "config.json"
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)

                # ì„ë² ë”© ì°¨ì› ì¶”ì¶œ (ì—¬ëŸ¬ í‚¤ ì§€ì›)
                for k in ["hidden_size", "d_model", "embedding_size", "model_dim", "dim"]:
                    if k in config and isinstance(config[k], int):
                        metadata["embedding_dimension"] = int(config[k])
                        break

                # ìµœëŒ€ í¬ì§€ì…˜ ì„ë² ë”© ì¶”ì¶œ
                if "max_position_embeddings" in config:
                    metadata["max_position_embeddings"] = config["max_position_embeddings"]
                elif "n_positions" in config:
                    metadata["max_position_embeddings"] = config["n_positions"]

                # ëª¨ë¸ íƒ€ì…ê³¼ ì•„í‚¤í…ì²˜
                metadata["model_type"] = config.get("model_type", "unknown")
                metadata["architecture"] = (
                    config.get("architectures", ["unknown"])[0] if "architectures" in config else "unknown"
                )
                metadata["vocab_size"] = config.get("vocab_size", 32000)

            # tokenizer_config.jsonì—ì„œ ì¶”ê°€ ì •ë³´ í™•ì¸
            tokenizer_config_file = path / "tokenizer_config.json"
            if tokenizer_config_file.exists():
                with open(tokenizer_config_file, 'r', encoding='utf-8') as f:
                    tokenizer_config = json.load(f)

                model_max_length = tokenizer_config.get("model_max_length")
                if model_max_length and isinstance(model_max_length, int):
                    if model_max_length < metadata["max_position_embeddings"]:
                        metadata["max_position_embeddings"] = model_max_length

            # ê¶Œì¥ ë° ì ˆëŒ€ ìµœëŒ€ í† í° ê³„ì‚°
            max_tokens = metadata["max_position_embeddings"]
            metadata["absolute_max_tokens"] = max_tokens
            metadata["recommended_max_tokens"] = min(max_tokens // 4, 2048)  # 25% ë˜ëŠ” 2048 ì¤‘ ì‘ì€ ê°’
            metadata["warning_threshold"] = max_tokens // 2

            logger.info(
                f"ğŸ¯ Model metadata extracted from {model_path}: "
                f"dim={metadata['embedding_dimension']}, "
                f"max_tokens={metadata['recommended_max_tokens']}/{metadata['absolute_max_tokens']}"
            )

            return metadata

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract metadata from {model_path}: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "embedding_dimension": 4096,
                "max_position_embeddings": 8192,
                "recommended_max_tokens": 2048,
                "absolute_max_tokens": 8192,
                "warning_threshold": 4096,
                "model_type": "unknown",
                "architecture": "unknown",
                "vocab_size": 32000,
            }

    @staticmethod
    def calculate_performance_limits(metadata: Dict[str, Any]) -> Dict[str, int]:
        """ğŸš€ Apple MLX ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì œí•œê°’ ê³„ì‚°

        Apple Siliconì˜ unified memoryì™€ Metal accelerationì„ ê³ ë ¤í•œ
        ìµœì  ì„±ëŠ¥ íŒŒë¼ë¯¸í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            metadata: ëª¨ë¸ ë©”íƒ€ë°ì´í„°

        Returns:
            ì„±ëŠ¥ ìµœì í™”ëœ ì œí•œê°’ë“¤
        """
        max_pos = metadata.get("max_position_embeddings", 32768)
        hidden_size = metadata.get("hidden_size", 4096)

        # Apple MLX ì„±ëŠ¥ ìµœì í™” ê¸°ì¤€
        # - Unified Memory íš¨ìœ¨ì„±
        # - Metal shader ë³‘ë ¬ì²˜ë¦¬ í•œê³„
        # - ì‹¤ì‹œê°„ ì‘ë‹µì„± ë³´ì¥

        if hidden_size >= 4096:  # ëŒ€í˜• ëª¨ë¸
            if max_pos >= 32768:
                recommended_max = 2048  # ğŸš€ Qwen3-4B ë“± ëŒ€í˜• ëª¨ë¸ì˜ ìµœì  ì„±ëŠ¥
                warning_threshold = 4096  # ê²½ê³  ì„ê³„ê°’
            else:
                recommended_max = 1024
                warning_threshold = 2048
        elif hidden_size >= 2048:  # ì¤‘í˜• ëª¨ë¸ (Qwen3-4BëŠ” 2560 hidden_size)
            if max_pos >= 32768:
                recommended_max = 2048  # ğŸ¯ 2560 hidden_sizeë„ 2048 í† í° ì§€ì›
                warning_threshold = 4096
            else:
                recommended_max = 1024
                warning_threshold = 2048
        else:  # ì†Œí˜• ëª¨ë¸
            if max_pos >= 16384:
                recommended_max = 1024
                warning_threshold = 2048
            else:
                recommended_max = 512
                warning_threshold = 1024

        return {
            "recommended_max_tokens": recommended_max,
            "warning_threshold": warning_threshold,
            "absolute_max_tokens": min(max_pos, 8192),  # ì ˆëŒ€ ìµœëŒ€ê°’
            "optimal_batch_size": 32 if hidden_size >= 4096 else 64,
        }


class DynamicServiceConfig:
    """âš™ï¸ ë™ì  ì„œë¹„ìŠ¤ êµ¬ì„± ê´€ë¦¬ì

    ëª¨ë¸ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.
    Apple MLX ì„±ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ì„¤ì •ì„ ì œê³µí•©ë‹ˆë‹¤! ğŸš€
    """

    def __init__(self):
        self.metadata: Dict[str, Any] = {}
        self.performance_limits: Dict[str, int] = {}
        self.is_configured = False

    async def configure_from_model(self, model_name: str, model_path: Optional[str] = None) -> None:
        """ğŸ”§ ëª¨ë¸ ê¸°ë°˜ ì„œë¹„ìŠ¤ ìë™ êµ¬ì„±

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            model_path: ëª¨ë¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        logger.info(f"ğŸš€ Configuring service from model: {model_name}")

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        model_identifier = model_path if model_path else model_name
        self.metadata = await ModelMetadataExtractor.extract_metadata(model_identifier)

        # ì„±ëŠ¥ ì œí•œê°’ ê³„ì‚°
        self.performance_limits = ModelMetadataExtractor.calculate_performance_limits(self.metadata)

        self.is_configured = True

        logger.info(
            f"âœ… Dynamic service configuration completed - "
            f"embedding_dim={self.get_embedding_dimension()}, "
            f"max_tokens={self.get_max_tokens()}, "
            f"recommended_tokens={self.get_recommended_max_tokens()}"
        )

    def get_embedding_dimension(self) -> int:
        """ğŸ“ ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.metadata.get("embedding_dimension", 4096)

    def get_max_tokens(self) -> int:
        """ğŸ¯ ì ˆëŒ€ ìµœëŒ€ í† í° ìˆ˜ ë°˜í™˜"""
        return self.performance_limits.get("absolute_max_tokens", 8192)

    def get_recommended_max_tokens(self) -> int:
        """âš¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ ìµœëŒ€ í† í° ìˆ˜ ë°˜í™˜"""
        return self.performance_limits.get("recommended_max_tokens", 2048)

    def get_warning_threshold(self) -> int:
        """âš ï¸ ê²½ê³  ì„ê³„ê°’ ë°˜í™˜"""
        return self.performance_limits.get("warning_threshold", 4096)

    def get_optimal_batch_size(self) -> int:
        """ğŸš€ ìµœì  ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        return self.performance_limits.get("optimal_batch_size", 32)

    def get_service_info(self) -> Dict[str, Any]:
        """ğŸ“Š ì„œë¹„ìŠ¤ ì •ë³´ ë°˜í™˜"""
        if not self.is_configured:
            return {"status": "not_configured"}

        return {
            "status": "configured",
            "model_metadata": {
                "model_type": self.metadata.get("model_type"),
                "hidden_size": self.metadata.get("hidden_size"),
                "max_position_embeddings": self.metadata.get("max_position_embeddings"),
                "vocab_size": self.metadata.get("vocab_size"),
                "source": self.metadata.get("source"),
            },
            "service_limits": {
                "embedding_dimension": self.get_embedding_dimension(),
                "recommended_max_tokens": self.get_recommended_max_tokens(),
                "warning_threshold": self.get_warning_threshold(),
                "absolute_max_tokens": self.get_max_tokens(),
                "optimal_batch_size": self.get_optimal_batch_size(),
            },
            "apple_mlx_optimized": True,
        }
