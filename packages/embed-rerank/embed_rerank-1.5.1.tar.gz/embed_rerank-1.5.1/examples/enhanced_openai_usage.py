#!/usr/bin/env python3
"""
ğŸš€ Enhanced OpenAI-Compatible API Usage Examples

This script demonstrates how to use the enhanced OpenAI-compatible embeddings
endpoint with configurable Apple MLX arguments while maintaining full
OpenAI SDK compatibility.

âœ¨ Features Demonstrated:
- ğŸ”§ Configurable batch sizes
- ğŸ¯ Normalization control
- ğŸ§  Backend preferences
- âš¡ Device preferences
- â±ï¸ Detailed timing metrics
- ğŸŒ Custom header support
"""

import asyncio
import time
from openai import OpenAI
import httpx
import json


# ğŸ”— Configure client for local Apple MLX service
BASE_URL = "http://localhost:9000/v1"


def basic_openai_compatibility():
    """
    ğŸ¯ Basic OpenAI SDK Usage (No Changes Needed)

    Your existing OpenAI code works unchanged - just point to our endpoint!
    """
    print("ğŸ”„ Testing Basic OpenAI Compatibility...")

    client = OpenAI(base_url=BASE_URL, api_key="dummy-key")  # Not needed for local service

    response = client.embeddings.create(
        model="text-embedding-ada-002", input=["Hello Apple MLX!", "Blazing fast embeddings"]
    )

    print(f"âœ… Basic embeddings generated: {len(response.data)} vectors")
    print(f"ğŸ“ Vector dimension: {len(response.data[0].embedding)}")
    print(f"ğŸ”¢ Tokens used: {response.usage.total_tokens}")
    print()


def enhanced_openai_with_args():
    """
    ğŸš€ Enhanced OpenAI Usage with Apple MLX Configuration

    Use additional fields in the request body to control MLX behavior
    while maintaining OpenAI SDK compatibility.
    """
    print("ğŸ”§ Testing Enhanced OpenAI with MLX Arguments...")

    client = OpenAI(base_url=BASE_URL, api_key="dummy-key")

    # ğŸŒŸ Enhanced request with MLX-specific options
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=[
            "Apple Silicon delivers incredible performance",
            "MLX framework revolutionizes on-device AI",
            "Unified memory architecture enables fast inference",
        ],
        # ğŸš€ Enhanced MLX Arguments (Optional, Non-Breaking)
        extra_body={
            "batch_size": 16,  # ğŸ“¦ Custom batch size
            "normalize": True,  # ğŸ¯ Normalization control
            "backend_preference": "mlx",  # ğŸ§  Prefer MLX backend
            "device_preference": "mps",  # âš¡ Prefer Apple Silicon
            "return_timing": True,  # â±ï¸ Include timing metrics
            "max_tokens_per_text": 512,  # ğŸ“ Token limit per text
        },
    )

    print(f"âœ… Enhanced embeddings generated: {len(response.data)} vectors")
    print(f"ğŸ“ Vector dimension: {len(response.data[0].embedding)}")
    print(f"ğŸ”¢ Tokens used: {response.usage.total_tokens}")

    # ğŸš€ Check for enhanced metrics (if return_timing=True)
    if hasattr(response.usage, 'mlx_processing_time'):
        print(f"âš¡ MLX processing time: {response.usage.mlx_processing_time:.4f}s")
        print(f"ğŸ• Total processing time: {response.usage.total_processing_time:.4f}s")
        print(f"ğŸ§  Backend used: {response.usage.backend_used}")
        print(f"ğŸ’» Device used: {response.usage.device_used}")
        print(f"ğŸ“¦ Batch size used: {response.usage.batch_size_used}")
    print()


def custom_headers_approach():
    """
    ğŸŒ Using Custom Headers for MLX Configuration

    Alternative approach using X-MLX-* headers for enterprise integration
    while keeping request body perfectly OpenAI-compatible.
    """
    print("ğŸŒ Testing Custom Headers Approach...")

    # ğŸ”— Use httpx for direct HTTP control
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer dummy-key",
        # ğŸš€ MLX-specific configuration headers
        "X-MLX-Batch-Size": "64",
        "X-MLX-Normalize": "false",
        "X-MLX-Backend": "mlx",
        "X-MLX-Device": "mps",
    }

    payload = {
        "model": "text-embedding-ada-002",
        "input": ["Custom headers enable enterprise integration"],
        "return_timing": True,
    }

    response = httpx.post(f"{BASE_URL}/embeddings", headers=headers, json=payload, timeout=30.0)

    if response.status_code == 200:
        data = response.json()
        print(f"âœ… Custom headers embeddings: {len(data['data'])} vectors")
        print(f"ğŸ“ Vector dimension: {len(data['data'][0]['embedding'])}")

        usage = data['usage']
        if 'backend_used' in usage:
            print(f"ğŸ§  Backend used: {usage['backend_used']}")
            print(f"ğŸ“¦ Batch size used: {usage['batch_size_used']}")
    else:
        print(f"âŒ Request failed: {response.status_code}")
    print()


def performance_comparison():
    """
    âš¡ Performance Comparison with Different Configurations

    Compare performance with different batch sizes and configurations
    to find optimal settings for your use case.
    """
    print("âš¡ Testing Performance with Different Configurations...")

    client = OpenAI(base_url=BASE_URL, api_key="dummy-key")

    # ğŸ“ Test data
    texts = [f"Performance test text number {i} for Apple MLX benchmarking" for i in range(50)]

    configs = [
        {"batch_size": 8, "name": "Small Batch"},
        {"batch_size": 32, "name": "Medium Batch"},
        {"batch_size": 64, "name": "Large Batch"},
    ]

    for config in configs:
        start_time = time.time()

        response = client.embeddings.create(
            model="text-embedding-ada-002",
            input=texts,
            extra_body={"batch_size": config["batch_size"], "return_timing": True},
        )

        total_time = time.time() - start_time

        print(f"ğŸ”§ {config['name']} (batch_size={config['batch_size']}):")
        print(f"   ğŸ“Š Total time: {total_time:.4f}s")
        print(f"   âš¡ MLX time: {response.usage.mlx_processing_time:.4f}s")
        print(f"   ğŸ“ˆ Throughput: {len(texts)/total_time:.1f} texts/sec")
    print()


async def async_usage_example():
    """
    ğŸ”„ Async Usage Example

    Demonstrate async usage for high-throughput applications.
    """
    print("ğŸ”„ Testing Async Usage...")

    async with httpx.AsyncClient() as client:
        tasks = []

        for i in range(5):
            payload = {
                "model": "text-embedding-ada-002",
                "input": [f"Async request {i} for Apple MLX"],
                "batch_size": 16,
                "return_timing": True,
            }

            task = client.post(f"{BASE_URL}/embeddings", json=payload, timeout=30.0)
            tasks.append(task)

        # âš¡ Execute all requests concurrently
        responses = await asyncio.gather(*tasks)

        print(f"âœ… Completed {len(responses)} async requests")
        for i, response in enumerate(responses):
            if response.status_code == 200:
                data = response.json()
                mlx_time = data['usage'].get('mlx_processing_time', 0)
                print(f"   Request {i}: {mlx_time:.4f}s MLX processing")
    print()


def main():
    """ğŸš€ Run all examples"""
    print("ğŸ Apple MLX Enhanced OpenAI Compatibility Examples")
    print("=" * 60)
    print()

    try:
        # ğŸ”„ Basic compatibility test
        basic_openai_compatibility()

        # ğŸš€ Enhanced features test
        enhanced_openai_with_args()

        # ğŸŒ Custom headers test
        custom_headers_approach()

        # âš¡ Performance comparison
        performance_comparison()

        # ğŸ”„ Async usage
        asyncio.run(async_usage_example())

        print("âœ… All examples completed successfully!")
        print("ğŸ¯ Your OpenAI-compatible endpoint with MLX arguments is working perfectly!")

    except Exception as e:
        print(f"âŒ Error running examples: {e}")
        print("ğŸ”§ Make sure your Apple MLX service is running on localhost:9000")


if __name__ == "__main__":
    main()
