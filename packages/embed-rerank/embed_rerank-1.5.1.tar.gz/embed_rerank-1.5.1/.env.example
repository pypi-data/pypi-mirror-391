############################################################
# Example Environment (.env example)
#
# Copy this file to `.env` and tweak as needed.
############################################################

# =========================
# Backend (Embeddings)
# =========================
# auto | mlx | torch (auto picks MLX on Apple Silicon)
BACKEND=auto
# Default Appleâ€‘Siliconâ€‘friendly embedding model
MODEL_NAME=mlx-community/Qwen3-Embedding-4B-4bit-DWQ
# Optional: local MLX-converted model directory (overrides HF cache)
MODEL_PATH=

# Embedding dimension strategy
# - as_is           â†’ use backend output dimension
# - hidden_size     â†’ use model hidden_size / HF metadata (2560 for Qwen3-Embedding-4B)
# - pad_or_truncate â†’ force OUTPUT_EMBEDDING_DIMENSION (pads or truncates vectors)
DIMENSION_STRATEGY=hidden_size
# OUTPUT_EMBEDDING_DIMENSION=2560  # uncomment + set DIMENSION_STRATEGY=pad_or_truncate to force a fixed size

# =========================
# Performance (Embeddings)
# =========================
BATCH_SIZE=32
MAX_BATCH_SIZE=128
MAX_TEXTS_PER_REQUEST=100
MAX_PASSAGES_PER_RERANK=1000
# Logical max tokens per text; actual limits are inferred from model metadata
MAX_SEQUENCE_LENGTH=8192
DEVICE_MEMORY_FRACTION=0.8
REQUEST_TIMEOUT=300

# =========================
# Reranker (Cross-Encoder) â€“ optional
# =========================
# Enable true cross-encoder reranking by setting a model ID.
# When unset, /api/v1/rerank falls back to embedding-similarity.
RERANKER_BACKEND=auto           # auto | mlx | torch
RERANKER_MODEL_ID=              # e.g. cross-encoder/ms-marco-MiniLM-L-6-v2
# RERANKER_MODEL_NAME=          # alias for RERANKER_MODEL_ID
# CROSS_ENCODER_MODEL=          # legacy alias, also accepted

# Optional reranker overrides
RERANK_MAX_SEQ_LEN=512          # pairwise (query+doc) max tokens
RERANK_BATCH_SIZE=16            # reranker batch size

# MLX-only experimental options:
# - RERANK_POOLING: mean | cls (default: mean)
# - RERANK_SCORE_NORM: none | sigmoid | minmax (default: none)
#   Use sigmoid to bound scores to [0,1] for schema-constrained clients.
RERANK_POOLING=mean
RERANK_SCORE_NORM=none

# =========================
# OpenAI compatibility
# =========================
# Automatically apply sigmoid normalization for OpenAI-compatible rerank scores
# true | false (default true)
OPENAI_RERANK_AUTO_SIGMOID=true

# =========================
# Model Cache & Storage
# =========================
# MODEL_PATH: Custom path for MLX models (overrides auto cache detection)
# If empty, uses Hugging Face cache or environment variables below:
# TRANSFORMERS_CACHE: Override HF transformers cache location
# HF_HOME: Hugging Face cache home directory
# Default cache location: ~/.cache/huggingface/hub/
#
# Examples:
# MODEL_PATH=/path/to/local/models/Qwen3-Embedding-4B-4bit-DWQ
# TRANSFORMERS_CACHE=/custom/cache/transformers
# HF_HOME=/custom/huggingface

# =========================
# Server
# =========================
HOST=0.0.0.0
PORT=9000
RELOAD=false

# =========================
# ðŸš€ Text Processing Defaults
# =========================
# Long text handling is automatic (token-aware). These set the defaults.
DEFAULT_AUTO_TRUNCATE=true
DEFAULT_TRUNCATION_STRATEGY=smart_truncate
# DEFAULT_MAX_TOKENS_OVERRIDE=4096  # up to absolute max (from model metadata)
DEFAULT_RETURN_PROCESSING_INFO=false

# Text processing strategies:
# - smart_truncate: Preserve sentence boundaries while truncating (recommended)
# - truncate: Simple token-based truncation
# - extract: Extract key sentences only
# - error: Raise error when token limit is exceeded

# =========================
# Logging
# =========================
LOG_LEVEL=INFO
LOG_FORMAT=json

# =========================
# Security / CORS (optional)
# =========================
# ALLOWED_HOSTS=["example.com","api.example.com"]
# ALLOWED_ORIGINS=["https://example.com","https://app.example.com"]

# Copy this file to `.env` and adjust values for your deployment.
