"""Tests for frame analysis pipeline."""

import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import cv2
import numpy as np
import pytest

from deep_brief.analysis.frame_analyzer import (
    FrameAnalysisPipeline,
    FrameAnalysisStep,
    PipelineMetrics,
    create_frame_analysis_pipeline,
)
from deep_brief.analysis.visual_analyzer import (
    ExtractedFrame,
    SceneFrameAnalysis,
    VisualAnalysisResult,
)
from deep_brief.core.exceptions import VideoProcessingError
from deep_brief.core.scene_detector import Scene, SceneDetectionResult
from deep_brief.utils.config import DeepBriefConfig, VisualAnalysisConfig


@pytest.fixture
def mock_config():
    """Create mock configuration for testing."""
    config = DeepBriefConfig(
        visual_analysis=VisualAnalysisConfig(
            frames_per_scene=2,
            frame_quality=85,
            blur_threshold=100.0,
            contrast_threshold=20.0,
            brightness_min=50.0,
            brightness_max=200.0,
            blur_weight=0.4,
            contrast_weight=0.3,
            brightness_weight=0.3,
            enable_quality_filtering=False,
            save_extracted_frames=False,
            enable_captioning=True,
            enable_ocr=True,
            enable_object_detection=True,
        )
    )
    return config


@pytest.fixture
def mock_config_minimal():
    """Create minimal configuration with only quality assessment."""
    config = DeepBriefConfig(
        visual_analysis=VisualAnalysisConfig(
            frames_per_scene=1,
            enable_captioning=False,
            enable_ocr=False,
            enable_object_detection=False,
        )
    )
    return config


@pytest.fixture
def pipeline(mock_config):
    """Create FrameAnalysisPipeline instance for testing."""
    return FrameAnalysisPipeline(config=mock_config)


@pytest.fixture
def pipeline_minimal(mock_config_minimal):
    """Create minimal FrameAnalysisPipeline instance."""
    return FrameAnalysisPipeline(config=mock_config_minimal)


@pytest.fixture
def mock_scene_result():
    """Create mock scene detection result."""
    scenes = [
        Scene(
            scene_number=1,
            start_time=0.0,
            end_time=5.0,
            duration=5.0,
            start_frame=0,
            end_frame=150,
            confidence=0.95,
            transition_type="cut",
        ),
        Scene(
            scene_number=2,
            start_time=5.0,
            end_time=10.0,
            duration=5.0,
            start_frame=150,
            end_frame=300,
            confidence=0.90,
            transition_type="fade",
        ),
    ]

    return SceneDetectionResult(
        scenes=scenes,
        total_scenes=2,
        detection_method="threshold",
        threshold_used=0.3,
        video_duration=10.0,
        average_scene_duration=5.0,
    )


@pytest.fixture
def test_video():
    """Create a test video file."""
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
        # Create video writer
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(tmp.name, fourcc, 30.0, (640, 480))

        # Generate frames
        for i in range(300):  # 10 seconds at 30fps
            frame = np.ones((480, 640, 3), dtype=np.uint8) * 255

            # Add some content
            cv2.putText(
                frame, f"Frame {i}", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2
            )

            # Add different content for different scenes
            if i < 150:
                cv2.rectangle(frame, (100, 100), (300, 300), (0, 0, 255), -1)
            else:
                cv2.circle(frame, (320, 240), 100, (255, 0, 0), -1)

            out.write(frame)

        out.release()
        return Path(tmp.name)


@pytest.fixture
def sample_frame():
    """Create a sample frame for testing."""
    frame = np.ones((480, 640, 3), dtype=np.uint8) * 240
    cv2.putText(
        frame, "Test Frame", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 3
    )
    cv2.rectangle(frame, (50, 150), (590, 450), (100, 100, 100), -1)
    return frame


class TestFrameAnalysisStep:
    """Test FrameAnalysisStep model."""

    def test_frame_analysis_step_creation(self):
        """Test creating FrameAnalysisStep."""
        step = FrameAnalysisStep(
            name="test_step",
            enabled=True,
            weight=0.5,
        )

        assert step.name == "test_step"
        assert step.enabled is True
        assert step.weight == 0.5

    def test_frame_analysis_step_defaults(self):
        """Test default values."""
        step = FrameAnalysisStep(
            name="test_step",
            enabled=False,
        )

        assert step.weight == 1.0


class TestPipelineMetrics:
    """Test PipelineMetrics model."""

    def test_pipeline_metrics_creation(self):
        """Test creating PipelineMetrics."""
        metrics = PipelineMetrics(
            total_frames_processed=10,
            successful_frames=9,
            failed_frames=1,
            total_processing_time=5.0,
            average_frame_time=0.5,
            step_timings={"quality": 0.1, "caption": 0.3},
            step_success_rates={"quality": 1.0, "caption": 0.9},
            average_quality_score=0.75,
            quality_distribution={"excellent": 3, "good": 4, "fair": 2, "poor": 1},
            frames_with_captions=8,
            frames_with_ocr=7,
            frames_with_objects=9,
        )

        assert metrics.total_frames_processed == 10
        assert metrics.successful_frames == 9
        assert metrics.average_frame_time == 0.5

    def test_get_summary(self):
        """Test metrics summary generation."""
        metrics = PipelineMetrics(
            total_frames_processed=10,
            successful_frames=9,
            failed_frames=1,
            total_processing_time=5.0,
            average_frame_time=0.5,
            step_timings={},
            step_success_rates={},
            average_quality_score=0.75,
            quality_distribution={},
            frames_with_captions=8,
            frames_with_ocr=7,
            frames_with_objects=9,
        )

        summary = metrics.get_summary()

        assert summary["success_rate"] == 0.9
        assert summary["average_processing_time"] == 0.5
        assert summary["total_processing_time"] == 5.0
        assert summary["quality_score"] == 0.75
        assert summary["analysis_coverage"]["captions"] == 0.8
        assert summary["analysis_coverage"]["ocr"] == 0.7
        assert summary["analysis_coverage"]["objects"] == 0.9

    def test_get_summary_empty_metrics(self):
        """Test summary with no frames processed."""
        metrics = PipelineMetrics(
            total_frames_processed=0,
            successful_frames=0,
            failed_frames=0,
            total_processing_time=0.0,
            average_frame_time=0.0,
            step_timings={},
            step_success_rates={},
            average_quality_score=0.0,
            quality_distribution={},
            frames_with_captions=0,
            frames_with_ocr=0,
            frames_with_objects=0,
        )

        summary = metrics.get_summary()

        assert summary["success_rate"] == 0
        assert summary["analysis_coverage"]["captions"] == 0


class TestFrameAnalysisPipeline:
    """Test FrameAnalysisPipeline class."""

    def test_initialization_with_all_enabled(self, pipeline):
        """Test pipeline initialization with all analyses enabled."""
        assert pipeline.config is not None
        assert pipeline.frame_extractor is not None
        assert len(pipeline.analysis_steps) == 4

        # Check enabled steps
        enabled_steps = [step.name for step in pipeline.analysis_steps if step.enabled]
        assert "quality_assessment" in enabled_steps
        assert "image_captioning" in enabled_steps
        assert "text_extraction" in enabled_steps
        assert "object_detection" in enabled_steps

        assert pipeline.total_weight > 0

    def test_initialization_minimal(self, pipeline_minimal):
        """Test pipeline initialization with minimal config."""
        enabled_steps = [
            step.name for step in pipeline_minimal.analysis_steps if step.enabled
        ]
        assert "quality_assessment" in enabled_steps
        assert "image_captioning" not in enabled_steps
        assert "text_extraction" not in enabled_steps
        assert "object_detection" not in enabled_steps

    def test_get_enabled_analyses(self, pipeline):
        """Test getting list of enabled analyses."""
        enabled = pipeline.get_enabled_analyses()

        assert "quality_assessment" in enabled
        assert "image_captioning" in enabled
        assert "text_extraction" in enabled
        assert "object_detection" in enabled
        assert len(enabled) == 4

    def test_analyze_single_frame(self, pipeline, sample_frame):
        """Test analyzing a single frame."""
        frame_info = {
            "frame_number": 100,
            "timestamp": 3.33,
            "scene_number": 1,
        }

        # Mock the analysis methods
        with (
            patch.object(
                pipeline.frame_extractor, "_assess_frame_quality"
            ) as mock_quality,
            patch.object(pipeline.frame_extractor, "_caption_frame") as mock_caption,
            patch.object(
                pipeline.frame_extractor, "_extract_text_from_frame"
            ) as mock_ocr,
            patch.object(
                pipeline.frame_extractor, "_detect_objects_in_frame"
            ) as mock_detect,
        ):
            # Set up mocks
            from tests.analysis.test_visual_analyzer import (
                create_test_quality_metrics,
            )

            mock_quality.return_value = create_test_quality_metrics()
            mock_caption.return_value = None
            mock_ocr.return_value = None
            mock_detect.return_value = None

            # Analyze frame
            result = pipeline.analyze_single_frame(sample_frame, frame_info)

            assert isinstance(result, ExtractedFrame)
            assert result.frame_number == 100
            assert result.timestamp == 3.33
            assert result.scene_number == 1
            assert result.width == 640
            assert result.height == 480

            # Verify all methods were called
            mock_quality.assert_called_once()
            mock_caption.assert_called_once()
            mock_ocr.assert_called_once()
            mock_detect.assert_called_once()

    def test_analyze_single_frame_minimal(self, pipeline_minimal, sample_frame):
        """Test analyzing frame with minimal pipeline."""
        with patch.object(
            pipeline_minimal.frame_extractor, "_assess_frame_quality"
        ) as mock_quality:
            from tests.analysis.test_visual_analyzer import create_test_quality_metrics

            mock_quality.return_value = create_test_quality_metrics()

            result = pipeline_minimal.analyze_single_frame(sample_frame)

            assert result.caption_result is None
            assert result.ocr_result is None
            assert result.object_detection_result is None

    def test_analyze_video_frames(self, pipeline, test_video, mock_scene_result):
        """Test analyzing video frames through pipeline."""
        with patch.object(
            pipeline.frame_extractor, "extract_frames_from_scenes"
        ) as mock_extract:
            # Create mock visual result
            from tests.analysis.test_visual_analyzer import create_test_quality_metrics

            mock_frames = []
            for i in range(4):  # 2 scenes x 2 frames per scene
                frame = ExtractedFrame(
                    frame_number=i * 75,
                    timestamp=i * 2.5,
                    scene_number=(i // 2) + 1,
                    width=640,
                    height=480,
                    quality_metrics=create_test_quality_metrics(),
                )
                mock_frames.append(frame)

            mock_scene_analyses = [
                SceneFrameAnalysis(
                    scene_number=1,
                    start_time=0.0,
                    end_time=5.0,
                    duration=5.0,
                    frames=mock_frames[:2],
                    total_frames_extracted=2,
                    best_frame=mock_frames[0],
                    average_quality_score=0.75,
                    quality_distribution={
                        "good": 2,
                        "excellent": 0,
                        "fair": 0,
                        "poor": 0,
                    },
                    total_frames_processed=2,
                    frames_filtered_by_quality=0,
                    extraction_success_rate=1.0,
                ),
                SceneFrameAnalysis(
                    scene_number=2,
                    start_time=5.0,
                    end_time=10.0,
                    duration=5.0,
                    frames=mock_frames[2:],
                    total_frames_extracted=2,
                    best_frame=mock_frames[2],
                    average_quality_score=0.75,
                    quality_distribution={
                        "good": 2,
                        "excellent": 0,
                        "fair": 0,
                        "poor": 0,
                    },
                    total_frames_processed=2,
                    frames_filtered_by_quality=0,
                    extraction_success_rate=1.0,
                ),
            ]

            mock_visual_result = VisualAnalysisResult(
                total_scenes=2,
                total_frames_extracted=4,
                total_frames_processed=4,
                overall_success_rate=1.0,
                scene_analyses=mock_scene_analyses,
                overall_quality_distribution={
                    "good": 4,
                    "excellent": 0,
                    "fair": 0,
                    "poor": 0,
                },
                average_quality_score=0.75,
                best_frames_per_scene=[mock_frames[0], mock_frames[2]],
                video_duration=10.0,
                extraction_method="scene_based",
                processing_time=2.0,
            )

            mock_extract.return_value = mock_visual_result

            # Analyze video
            visual_result, metrics = pipeline.analyze_video_frames(
                video_path=test_video,
                scene_result=mock_scene_result,
            )

            assert visual_result == mock_visual_result
            assert isinstance(metrics, PipelineMetrics)
            assert metrics.total_frames_processed == 4
            assert metrics.successful_frames == 4
            assert metrics.average_quality_score == 0.75

    def test_analyze_video_frames_with_progress(
        self, pipeline, test_video, mock_scene_result
    ):
        """Test video analysis with progress callback."""
        progress_updates = []

        def progress_callback(progress, message):
            progress_updates.append((progress, message))

        with patch.object(
            pipeline.frame_extractor, "extract_frames_from_scenes"
        ) as mock_extract:
            # Create minimal mock result
            mock_visual_result = MagicMock()
            mock_visual_result.total_frames_extracted = 2
            mock_visual_result.total_frames_processed = 2
            mock_visual_result.overall_quality_distribution = {}
            mock_visual_result.average_quality_score = 0.7
            mock_visual_result.scene_analyses = []

            mock_extract.return_value = mock_visual_result

            # Analyze with progress
            visual_result, metrics = pipeline.analyze_video_frames(
                video_path=test_video,
                scene_result=mock_scene_result,
                progress_callback=progress_callback,
            )

            # Check progress updates were made
            assert len(progress_updates) > 0

    def test_analyze_video_frames_error_handling(
        self, pipeline, test_video, mock_scene_result
    ):
        """Test error handling in video analysis."""
        with patch.object(
            pipeline.frame_extractor, "extract_frames_from_scenes"
        ) as mock_extract:
            mock_extract.side_effect = Exception("Extraction failed")

            with pytest.raises(VideoProcessingError) as exc_info:
                pipeline.analyze_video_frames(
                    video_path=test_video,
                    scene_result=mock_scene_result,
                )

            assert "Frame analysis pipeline failed" in str(exc_info.value)

    def test_generate_analysis_summary(self, pipeline):
        """Test generating analysis summary."""
        # Create mock data
        from tests.analysis.test_visual_analyzer import create_test_quality_metrics

        mock_frame = ExtractedFrame(
            frame_number=0,
            timestamp=0.0,
            scene_number=1,
            width=640,
            height=480,
            quality_metrics=create_test_quality_metrics(),
        )

        mock_scene_analysis = SceneFrameAnalysis(
            scene_number=1,
            start_time=0.0,
            end_time=5.0,
            duration=5.0,
            frames=[mock_frame],
            total_frames_extracted=1,
            best_frame=mock_frame,
            average_quality_score=0.75,
            quality_distribution={"good": 1, "excellent": 0, "fair": 0, "poor": 0},
            total_frames_processed=1,
            frames_filtered_by_quality=0,
            extraction_success_rate=1.0,
        )

        visual_result = VisualAnalysisResult(
            total_scenes=1,
            total_frames_extracted=1,
            total_frames_processed=1,
            overall_success_rate=1.0,
            scene_analyses=[mock_scene_analysis],
            overall_quality_distribution={
                "good": 1,
                "excellent": 0,
                "fair": 0,
                "poor": 0,
            },
            average_quality_score=0.75,
            best_frames_per_scene=[mock_frame],
            video_duration=5.0,
            extraction_method="scene_based",
            processing_time=1.0,
        )

        metrics = PipelineMetrics(
            total_frames_processed=1,
            successful_frames=1,
            failed_frames=0,
            total_processing_time=1.0,
            average_frame_time=1.0,
            step_timings={},
            step_success_rates={},
            average_quality_score=0.75,
            quality_distribution={"good": 1},
            frames_with_captions=0,
            frames_with_ocr=0,
            frames_with_objects=0,
        )

        # Generate summary
        summary = pipeline.generate_analysis_summary(visual_result, metrics)

        assert "video_overview" in summary
        assert summary["video_overview"]["duration"] == 5.0
        assert summary["video_overview"]["total_scenes"] == 1

        assert "quality_summary" in summary
        assert summary["quality_summary"]["average_score"] == 0.75

        assert "content_analysis" in summary
        assert "analysis_coverage" in summary
        assert "key_insights" in summary
        assert "performance" in summary

    def test_determine_content_type(self, pipeline):
        """Test content type determination."""
        # Create mock visual result with slide layout
        from deep_brief.analysis.object_detector import ObjectDetectionResult

        mock_obj_result = ObjectDetectionResult(
            detected_objects=[],
            total_objects=0,
            processing_time=0.1,
            frame_width=640,
            frame_height=480,
            element_counts={},
            high_confidence_objects=0,
            average_confidence=0.0,
            layout_type="slide",
            has_presenter=False,
            dominant_element=None,
        )

        from tests.analysis.test_visual_analyzer import create_test_quality_metrics

        mock_frame = ExtractedFrame(
            frame_number=0,
            timestamp=0.0,
            scene_number=1,
            width=640,
            height=480,
            quality_metrics=create_test_quality_metrics(),
            object_detection_result=mock_obj_result,
        )

        mock_scene = SceneFrameAnalysis(
            scene_number=1,
            start_time=0.0,
            end_time=5.0,
            duration=5.0,
            frames=[mock_frame],
            total_frames_extracted=1,
            best_frame=mock_frame,
            average_quality_score=0.75,
            quality_distribution={},
            total_frames_processed=1,
            frames_filtered_by_quality=0,
            extraction_success_rate=1.0,
        )

        visual_result = VisualAnalysisResult(
            total_scenes=1,
            total_frames_extracted=1,
            total_frames_processed=1,
            overall_success_rate=1.0,
            scene_analyses=[mock_scene],
            overall_quality_distribution={},
            average_quality_score=0.75,
            best_frames_per_scene=[mock_frame],
            video_duration=5.0,
            extraction_method="scene_based",
            processing_time=1.0,
        )

        metrics = PipelineMetrics(
            total_frames_processed=1,
            successful_frames=1,
            failed_frames=0,
            total_processing_time=1.0,
            average_frame_time=1.0,
            step_timings={},
            step_success_rates={},
            average_quality_score=0.75,
            quality_distribution={},
            frames_with_captions=0,
            frames_with_ocr=0,
            frames_with_objects=1,
        )

        content_type = pipeline._determine_content_type(visual_result, metrics)
        assert content_type == "presentation"

    def test_cleanup(self, pipeline):
        """Test pipeline cleanup."""
        with patch.object(pipeline.frame_extractor, "cleanup") as mock_cleanup:
            pipeline.cleanup()
            mock_cleanup.assert_called_once()


class TestFrameAnalysisPipelineFactory:
    """Test factory function."""

    def test_create_frame_analysis_pipeline(self):
        """Test creating pipeline with factory."""
        pipeline = create_frame_analysis_pipeline()
        assert isinstance(pipeline, FrameAnalysisPipeline)
        assert pipeline.config is not None

    def test_create_frame_analysis_pipeline_with_config(self, mock_config):
        """Test creating pipeline with custom config."""
        pipeline = create_frame_analysis_pipeline(config=mock_config)
        assert isinstance(pipeline, FrameAnalysisPipeline)
        assert pipeline.config == mock_config


class TestPipelineIntegration:
    """Integration tests for the pipeline."""

    def test_full_pipeline_workflow(self, test_video):
        """Test complete pipeline workflow."""
        # Create pipeline with all analyses enabled
        config = DeepBriefConfig(
            visual_analysis=VisualAnalysisConfig(
                frames_per_scene=1,
                enable_captioning=False,  # Disable to avoid model loading
                enable_ocr=False,  # Disable to avoid dependencies
                enable_object_detection=True,  # Keep enabled as it's lightweight
            )
        )
        pipeline = FrameAnalysisPipeline(config=config)

        # Create simple scene result
        scene = Scene(
            scene_number=1,
            start_time=0.0,
            end_time=2.0,
            duration=2.0,
            start_frame=0,
            end_frame=60,
            confidence=0.95,
            transition_type="cut",
        )

        scene_result = SceneDetectionResult(
            scenes=[scene],
            total_scenes=1,
            detection_method="threshold",
            threshold_used=0.3,
            video_duration=2.0,
            average_scene_duration=2.0,
        )

        # Run pipeline
        visual_result, metrics = pipeline.analyze_video_frames(
            video_path=test_video,
            scene_result=scene_result,
        )

        # Verify results
        assert visual_result.total_scenes == 1
        assert visual_result.total_frames_extracted > 0
        assert metrics.total_frames_processed > 0
        assert metrics.average_quality_score >= 0

        # Generate summary
        summary = pipeline.generate_analysis_summary(visual_result, metrics)
        assert summary is not None
        assert "video_overview" in summary
        assert "quality_summary" in summary

        # Cleanup
        pipeline.cleanup()
        test_video.unlink()  # Clean up test file
