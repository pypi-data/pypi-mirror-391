{
  "id": "1805b8e4-0356-4384-adec-44f12a18f32e",
  "revision": 0,
  "last_node_id": 91,
  "last_link_id": 161,
  "nodes": [
    {
      "id": 58,
      "type": "ScaleROPE",
      "pos": [
        570,
        -150
      ],
      "size": [
        320,
        178
      ],
      "flags": {},
      "order": 15,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 116
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            117
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "ScaleROPE",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        1,
        0,
        1,
        0,
        7,
        0
      ]
    },
    {
      "id": 39,
      "type": "VAELoader",
      "pos": [
        -320,
        270
      ],
      "size": [
        390,
        58
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "VAE",
          "type": "VAE",
          "slot_index": 0,
          "links": [
            76,
            99
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "VAELoader",
        "cnr_id": "comfy-core",
        "ver": "0.3.67",
        "models": [
          {
            "name": "wan_2.1_vae.safetensors",
            "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors",
            "directory": "vae"
          }
        ]
      },
      "widgets_values": [
        "wan_2.1_vae.safetensors"
      ]
    },
    {
      "id": 49,
      "type": "CLIPVisionLoader",
      "pos": [
        -320,
        370
      ],
      "size": [
        390,
        58
      ],
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP_VISION",
          "type": "CLIP_VISION",
          "slot_index": 0,
          "links": [
            94
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPVisionLoader",
        "cnr_id": "comfy-core",
        "ver": "0.3.67",
        "models": [
          {
            "name": "clip_vision_h.safetensors",
            "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors",
            "directory": "clip_vision"
          }
        ]
      },
      "widgets_values": [
        "clip_vision_h.safetensors"
      ]
    },
    {
      "id": 37,
      "type": "UNETLoader",
      "pos": [
        -320,
        -210
      ],
      "size": [
        390,
        82
      ],
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "slot_index": 0,
          "links": [
            114
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "UNETLoader",
        "cnr_id": "comfy-core",
        "ver": "0.3.67",
        "models": [
          {
            "name": "chrono_edit_14B_fp16.safetensors",
            "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/chrono_edit_14B_fp16.safetensors",
            "directory": "diffusion_models"
          }
        ]
      },
      "widgets_values": [
        "chrono_edit_14B_fp16.safetensors",
        "fp8_e4m3fn"
      ]
    },
    {
      "id": 8,
      "type": "VAEDecode",
      "pos": [
        570,
        600
      ],
      "size": [
        320,
        46
      ],
      "flags": {},
      "order": 18,
      "mode": 0,
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 35
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 76
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "slot_index": 0,
          "links": [
            118
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "VAEDecode",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": []
    },
    {
      "id": 59,
      "type": "ImageFromBatch",
      "pos": [
        920,
        -250
      ],
      "size": [
        270,
        82
      ],
      "flags": {},
      "order": 19,
      "mode": 0,
      "inputs": [
        {
          "name": "image",
          "type": "IMAGE",
          "link": 118
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "links": [
            119
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "ImageFromBatch",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        4,
        1
      ]
    },
    {
      "id": 51,
      "type": "CLIPVisionEncode",
      "pos": [
        150,
        580
      ],
      "size": [
        340,
        78
      ],
      "flags": {},
      "order": 14,
      "mode": 0,
      "inputs": [
        {
          "name": "clip_vision",
          "type": "CLIP_VISION",
          "link": 94
        },
        {
          "name": "image",
          "type": "IMAGE",
          "link": 161
        }
      ],
      "outputs": [
        {
          "name": "CLIP_VISION_OUTPUT",
          "type": "CLIP_VISION_OUTPUT",
          "slot_index": 0,
          "links": [
            120
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPVisionEncode",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        "none"
      ]
    },
    {
      "id": 50,
      "type": "WanImageToVideo",
      "pos": [
        150,
        290
      ],
      "size": [
        342.5999755859375,
        210
      ],
      "flags": {},
      "order": 16,
      "mode": 0,
      "inputs": [
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 97
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 98
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 99
        },
        {
          "name": "clip_vision_output",
          "shape": 7,
          "type": "CLIP_VISION_OUTPUT",
          "link": 120
        },
        {
          "name": "start_image",
          "shape": 7,
          "type": "IMAGE",
          "link": 160
        }
      ],
      "outputs": [
        {
          "name": "positive",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            101
          ]
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "slot_index": 1,
          "links": [
            102
          ]
        },
        {
          "name": "latent",
          "type": "LATENT",
          "slot_index": 2,
          "links": [
            103
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "WanImageToVideo",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        1024,
        1024,
        5,
        1
      ]
    },
    {
      "id": 6,
      "type": "CLIPTextEncode",
      "pos": [
        110,
        -210
      ],
      "size": [
        422.84503173828125,
        164.31304931640625
      ],
      "flags": {},
      "order": 10,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 74
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            97
          ]
        }
      ],
      "title": "CLIP Text Encode (Positive Prompt)",
      "properties": {
        "Node name for S&R": "CLIPTextEncode",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        "A bottle of facial cleansing foam and bubble shampoo, surrounded by white, round, foamy bubbles. The bubbles are very fluffy, crystal clear, giving a sense of fluffiness and comfort. There are also several bubbles floating in the air around, and the bottle is floating in the air. The background is light pink. The high-resolution picture creates a professional advertising style with high-definition images and high-quality details."
      ],
      "color": "#232",
      "bgcolor": "#353"
    },
    {
      "id": 7,
      "type": "CLIPTextEncode",
      "pos": [
        110,
        10
      ],
      "size": [
        425.27801513671875,
        180.6060791015625
      ],
      "flags": {},
      "order": 11,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 75
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            98
          ]
        }
      ],
      "title": "CLIP Text Encode (Negative Prompt)",
      "properties": {
        "Node name for S&R": "CLIPTextEncode",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        "Ëâ≤Ë∞ÉËâ≥‰∏ΩÔºåËøáÊõùÔºåÈùôÊÄÅÔºåÁªÜËäÇÊ®°Á≥ä‰∏çÊ∏ÖÔºåÂ≠óÂπïÔºåÈ£éÊ†ºÔºå‰ΩúÂìÅÔºåÁîª‰ΩúÔºåÁîªÈù¢ÔºåÈùôÊ≠¢ÔºåÊï¥‰ΩìÂèëÁÅ∞ÔºåÊúÄÂ∑ÆË¥®ÈáèÔºå‰ΩéË¥®ÈáèÔºåJPEGÂéãÁº©ÊÆãÁïôÔºå‰∏ëÈôãÁöÑÔºåÊÆãÁº∫ÁöÑÔºåÂ§ö‰ΩôÁöÑÊâãÊåáÔºåÁîªÂæó‰∏çÂ•ΩÁöÑÊâãÈÉ®ÔºåÁîªÂæó‰∏çÂ•ΩÁöÑËÑ∏ÈÉ®ÔºåÁï∏ÂΩ¢ÁöÑÔºåÊØÅÂÆπÁöÑÔºåÂΩ¢ÊÄÅÁï∏ÂΩ¢ÁöÑËÇ¢‰ΩìÔºåÊâãÊåáËûçÂêàÔºåÈùôÊ≠¢‰∏çÂä®ÁöÑÁîªÈù¢ÔºåÊùÇ‰π±ÁöÑËÉåÊôØÔºå‰∏âÊù°ËÖøÔºåËÉåÊôØ‰∫∫ÂæàÂ§öÔºåÂÄíÁùÄËµ∞"
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 57,
      "type": "LoraLoaderModelOnly",
      "pos": [
        -310,
        -30
      ],
      "size": [
        360,
        82
      ],
      "flags": {},
      "order": 9,
      "mode": 4,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 114
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            115
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "LoraLoaderModelOnly",
        "cnr_id": "comfy-core",
        "ver": "0.3.67",
        "models": [
          {
            "name": "chronoedit_distill_lora.safetensors",
            "url": "https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers/resolve/main/lora/chronoedit_distill_lora.safetensors",
            "directory": "loras"
          }
        ]
      },
      "widgets_values": [
        "chronoedit_distill_lora.safetensors",
        1
      ]
    },
    {
      "id": 38,
      "type": "CLIPLoader",
      "pos": [
        -320,
        120
      ],
      "size": [
        390,
        106
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "slot_index": 0,
          "links": [
            74,
            75
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPLoader",
        "cnr_id": "comfy-core",
        "ver": "0.3.67",
        "models": [
          {
            "name": "umt5_xxl_fp8_e4m3fn_scaled.safetensors",
            "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors",
            "directory": "text_encoders"
          }
        ]
      },
      "widgets_values": [
        "umt5_xxl_fp8_e4m3fn_scaled.safetensors",
        "wan",
        "default"
      ]
    },
    {
      "id": 54,
      "type": "ModelSamplingSD3",
      "pos": [
        570,
        -250
      ],
      "size": [
        320,
        58
      ],
      "flags": {},
      "order": 13,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 115
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "slot_index": 0,
          "links": [
            116
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "ModelSamplingSD3",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        5
      ]
    },
    {
      "id": 60,
      "type": "SaveImage",
      "pos": [
        930,
        -120
      ],
      "size": [
        730,
        760
      ],
      "flags": {},
      "order": 20,
      "mode": 0,
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 119
        }
      ],
      "outputs": [],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        "Chrono_Edit_14B"
      ]
    },
    {
      "id": 3,
      "type": "KSampler",
      "pos": [
        570,
        70
      ],
      "size": [
        320,
        480
      ],
      "flags": {},
      "order": 17,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 117
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 101
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 102
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 103
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "slot_index": 0,
          "links": [
            35
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "KSampler",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        859991172109574,
        "randomize",
        20,
        4,
        "uni_pc",
        "simple",
        1
      ]
    },
    {
      "id": 87,
      "type": "MarkdownNote",
      "pos": [
        570,
        690
      ],
      "size": [
        340,
        200
      ],
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Settings",
      "properties": {},
      "widgets_values": [
        "The distilled LoRA can speed up this workflow, but it will also sacrifice some of the final output quality.\n\nIf you enable the distilled LoRA, don't forget to change the KSampler settings.\n\n| Parameters      | original | With distill LoRA enable |\n|----------------|---------------------|------------------------|\n| ModelSamplingSD3 - shift          | 5.00                | 2.0                    |\n| KSampler - steps          | 20                  | 8                      |\n| KSampler - cfg            | 4.0                 | 1.0                    |"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 64,
      "type": "MarkdownNote",
      "pos": [
        -850,
        -60
      ],
      "size": [
        490,
        540
      ],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Model links (for local users)",
      "properties": {},
      "widgets_values": [
        "\n## Model links\n\n**text_encoders**\n\n- [umt5_xxl_fp8_e4m3fn_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)\n\n**clip_vision**\n\n- [clip_vision_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\n**loras**\n\n- [chronoedit_distill_lora.safetensors](https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers/resolve/main/lora/chronoedit_distill_lora.safetensors)\n\n**diffusion_models**\n\n- [chrono_edit_14B_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/chrono_edit_14B_fp16.safetensors)\n\n**vae**\n\n- [wan_2.1_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)\n\n\nModel Storage Location\n\n```\nüìÇ ComfyUI/\n‚îú‚îÄ‚îÄ üìÇ models/\n‚îÇ   ‚îú‚îÄ‚îÄ üìÇ text_encoders/\n‚îÇ   ‚îÇ      ‚îî‚îÄ‚îÄ umt5_xxl_fp8_e4m3fn_scaled.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ üìÇ clip_vision/\n‚îÇ   ‚îÇ      ‚îî‚îÄ‚îÄ clip_vision_h.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ üìÇ loras/\n‚îÇ   ‚îÇ      ‚îî‚îÄ‚îÄ chronoedit_distill_lora.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ üìÇ diffusion_models/\n‚îÇ   ‚îÇ      ‚îî‚îÄ‚îÄ chrono_edit_14B_fp16.safetensors\n‚îÇ   ‚îî‚îÄ‚îÄ üìÇ vae/\n‚îÇ          ‚îî‚îÄ‚îÄ wan_2.1_vae.safetensors\n```\n\n## Report issue\n\nIf you have any problems while using this workflow, please report template-related issues via this link: [report the template issue here](https://github.com/Comfy-Org/workflow_templates/issues)."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 89,
      "type": "ImageScaleToMaxDimension",
      "pos": [
        -290,
        1080
      ],
      "size": [
        330,
        90
      ],
      "flags": {},
      "order": 12,
      "mode": 4,
      "inputs": [
        {
          "name": "image",
          "type": "IMAGE",
          "link": 159
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "links": [
            160,
            161
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "ImageScaleToMaxDimension",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        "area",
        1280
      ]
    },
    {
      "id": 91,
      "type": "MarkdownNote",
      "pos": [
        -310,
        1250
      ],
      "size": [
        390,
        130
      ],
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Note: Image size",
      "properties": {},
      "widgets_values": [
        "This model is fine - tuned from Wan2.1 - I2V - 14B 720P (1280x720). So, please don't upload images whose size is too large; that might take up a very large amount of VRAM or lead to bad results.\n\nYou can use `ImageScaleToMaxDimension` to scale it down."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 52,
      "type": "LoadImage",
      "pos": [
        -300,
        590
      ],
      "size": [
        360,
        440
      ],
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "slot_index": 0,
          "links": [
            159
          ]
        },
        {
          "name": "MASK",
          "type": "MASK",
          "slot_index": 1,
          "links": null
        }
      ],
      "properties": {
        "Node name for S&R": "LoadImage",
        "cnr_id": "comfy-core",
        "ver": "0.3.67"
      },
      "widgets_values": [
        "image_chrono_edit_input_image.png",
        "image"
      ]
    },
    {
      "id": 88,
      "type": "MarkdownNote",
      "pos": [
        -850,
        -250
      ],
      "size": [
        490,
        140
      ],
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "About ChronoEdit 14B",
      "properties": {},
      "widgets_values": [
        "[ChronoEdit-14B](https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers) is finetuned from the pretrain model of Wan2.1-I2V-14B 720P\n\n[ChronoEdit](https://research.nvidia.com/labs/toronto-ai/chronoedit/), a framework developed by teams from NVIDIA and the University of Toronto, reframes image editing as a two-frame video generation task. It leverages the temporal priors of pretrained video generative models and incorporates a temporal reasoning mechanism to achieve editing results with both visual fidelity and physical consistency. Additionally, it comes with the PBench-Edit benchmark for evaluating physical consistency, making it suitable for scenarios like world simulation that require strict adherence to physical laws."
      ],
      "color": "#432",
      "bgcolor": "#653"
    }
  ],
  "links": [
    [
      35,
      3,
      0,
      8,
      0,
      "LATENT"
    ],
    [
      74,
      38,
      0,
      6,
      0,
      "CLIP"
    ],
    [
      75,
      38,
      0,
      7,
      0,
      "CLIP"
    ],
    [
      76,
      39,
      0,
      8,
      1,
      "VAE"
    ],
    [
      94,
      49,
      0,
      51,
      0,
      "CLIP_VISION"
    ],
    [
      97,
      6,
      0,
      50,
      0,
      "CONDITIONING"
    ],
    [
      98,
      7,
      0,
      50,
      1,
      "CONDITIONING"
    ],
    [
      99,
      39,
      0,
      50,
      2,
      "VAE"
    ],
    [
      101,
      50,
      0,
      3,
      1,
      "CONDITIONING"
    ],
    [
      102,
      50,
      1,
      3,
      2,
      "CONDITIONING"
    ],
    [
      103,
      50,
      2,
      3,
      3,
      "LATENT"
    ],
    [
      114,
      37,
      0,
      57,
      0,
      "MODEL"
    ],
    [
      115,
      57,
      0,
      54,
      0,
      "MODEL"
    ],
    [
      116,
      54,
      0,
      58,
      0,
      "MODEL"
    ],
    [
      117,
      58,
      0,
      3,
      0,
      "MODEL"
    ],
    [
      118,
      8,
      0,
      59,
      0,
      "IMAGE"
    ],
    [
      119,
      59,
      0,
      60,
      0,
      "IMAGE"
    ],
    [
      120,
      51,
      0,
      50,
      3,
      "CLIP_VISION_OUTPUT"
    ],
    [
      159,
      52,
      0,
      89,
      0,
      "IMAGE"
    ],
    [
      160,
      89,
      0,
      50,
      4,
      "IMAGE"
    ],
    [
      161,
      89,
      0,
      51,
      1,
      "IMAGE"
    ]
  ],
  "groups": [
    {
      "id": 1,
      "title": "Step 4: Prompt",
      "bounding": [
        100,
        -280,
        445.27801513671875,
        484.2060791015625
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 2,
      "title": "Step 1 - Load models (for local users)",
      "bounding": [
        -330,
        -280,
        410,
        760
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 3,
      "title": "Step 2 - Upload image",
      "bounding": [
        -330,
        510,
        410,
        690
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 4,
      "title": "Step3: Image size",
      "bounding": [
        110,
        220,
        430,
        290
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 5,
      "title": "Ctrl-B to enable",
      "bounding": [
        -320,
        -100,
        390,
        170
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    }
  ],
  "config": {},
  "extra": {
    "ds": {
      "scale": 1.161837510447208,
      "offset": [
        1072.4315399787201,
        389.44746533291465
      ]
    },
    "frontendVersion": "1.32.1",
    "VHS_latentpreview": false,
    "VHS_latentpreviewrate": 0,
    "VHS_MetadataImage": true,
    "VHS_KeepIntermediate": true
  },
  "version": 0.4
}