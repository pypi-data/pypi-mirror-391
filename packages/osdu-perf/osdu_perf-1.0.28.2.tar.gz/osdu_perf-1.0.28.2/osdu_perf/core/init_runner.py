import os
from pathlib import Path
import getpass
from ..utils.logger import get_logger

class InitRunner:
    def __init__(self):
        self.initialized = False
        self.logger = get_logger('init_runner')

    def initialize(self):
        if not self.initialized:
            # Perform initialization tasks here
            self.logger.info("Initializing...")
            self.initialized = True
        else:
            self.logger.info("Already initialized.")

    def create_project_config(self, output_path: Path, service_name: str = None) -> None:
        """Creates a config.yaml file for the project."""
        username = getpass.getuser()
        test_name_prefix = f"{username}_{service_name}" if service_name else f"{username}_osdu_test"
        
        

        # Best practice: Load this template from a file in a 'templates' directory
        config_content = f"""# OSDU Performance Testing Configuration
# This file contains configuration settings for the OSDU performance testing framework

# OSDU Environment Configuration
osdu_environment:
  # OSDU instance details (required for run local command)
  host: "https://your-osdu-host.com"
  partition: "your-partition-id"
  app_id: "your-azure-app-id"
  
  # OSDU deployment details (optional - used for metrics collection)
  sku: "Standard"
  version: "25.2.35"
  
# Metrics Collection Configuration  
metrics_collector:
  # Kusto (Azure Data Explorer) Configuration
  kusto:
    cluster: ""
    database: ""
    ingest_uri: ""

# Test Configuration (Optional)
test_settings:
  # Where the azure load test resource and tests are located
  subscription_id: ""
  resource_group: "adme-performance-rg"
  location: "eastus"
  #Test specific configurations
  default_wait_time: 
    min: 1
    max: 3
  users: 10
  spawn_rate: 2
  run_time: "60s"
  engine_instances: 1
  test_name_prefix: "{test_name_prefix}"
  test_scenario: "{service_name}"
  test_run_id_description: "Test run for {service_name} api"
"""
        output_path.write_text(config_content, encoding='utf-8')
        self.logger.info(f"Created config.yaml at {output_path} generated prefix {test_name_prefix}")

    
    def create_service_test_file(self, output_path: Path, service_name: str):
        self.logger.info(f"Creating test file for service: {service_name} path {output_path}")
        service_name_clean = service_name.title()
        service_name_lower = service_name.lower()
        formatted_template = f'''import os
"""
Performance tests for {service_name_clean} Service
Generated by OSDU Performance Testing Framework
"""

from osdu_perf.core.base_service import BaseService


class {service_name_clean}PerformanceTest(BaseService):
    """
    Performance test class for {service_name_clean} Service
    
    This class will be automatically discovered and executed by the framework.
    """
    
    def __init__(self, client=None):
        super().__init__(client)
        self.name = "{service_name_lower}"
    
    def execute(self, headers=None, partition=None, base_url=None):
        """
        Execute {service_name_lower} performance tests
        
        Args:
            headers: HTTP headers including authentication
            partition: Data partition ID  
            base_url: Base URL for the service
        """
        print(f"üî• Executing {{self.name}} performance tests...")
        
        # Example 1: Health check endpoint
        try:
            self._test_health_check(headers, base_url)
        except Exception as e:
            print(f"‚ùå Health check failed: {{e}}")
        
        # Example 2: Service-specific API calls
        try:
            self._test_service_apis(headers, partition, base_url)
        except Exception as e:
            print(f"‚ùå Service API tests failed: {{e}}")
        
        print(f"‚úÖ Completed {{self.name}} performance tests")
    
    def provide_explicit_token(self) -> str:
        """
        Provide an explicit token for service execution.
        
        Returns the bearer token from environment variable set by localdev.py
        
        Returns:
            str: Authentication token for API requests
        """
        token = os.environ.get('ADME_BEARER_TOKEN', '')
        return token
  
    
    def prehook(self, headers=None, partition=None, base_url=None):
        """
        Pre-hook tasks before service execution.
        
        Use this method to set up test data, configurations, or prerequisites.
        
        Args:
            headers: HTTP headers including authentication
            partition: Data partition ID  
            base_url: Base URL for the service
        """
        print(f"üîß Setting up prerequisites for {{self.name}} tests...")
        # TODO: Implement setup logic (e.g., create test data, configure environment)
        # Example: Create test records, validate partition access, etc.
        pass
    
    def posthook(self, headers=None, partition=None, base_url=None):
        """
        Post-hook tasks after service execution.
        
        Use this method for cleanup, reporting, or post-test validations.
        
        Args:
            headers: HTTP headers including authentication
            partition: Data partition ID  
            base_url: Base URL for the service
        """
        print(f"üßπ Cleaning up after {{self.name}} tests...")
        # TODO: Implement cleanup logic (e.g., delete test data, reset state)
        # Example: Remove test records, generate reports, validate cleanup
        pass
    
    def _test_health_check(self, headers, base_url):
        """Test health check endpoint"""
        try:
            response = self.client.get(
                f"{{base_url}}/api/{service_name_lower}/v1/health",
                headers=headers,
                name="{service_name_lower}_health_check"
            )
            print(f"Health check status: {{response.status_code}}")
        except Exception as e:
            print(f"Health check failed: {{e}}")
    
    def _test_service_apis(self, headers, partition, base_url):
        """
        Implement your service-specific test scenarios here
        
        Examples:
        - GET /api/{service_name_lower}/v1/records
        - POST /api/{service_name_lower}/v1/records
        - PUT /api/{service_name_lower}/v1/records/{{id}}
        - DELETE /api/{service_name_lower}/v1/records/{{id}}
        """
        
        # TODO: Replace with actual {service_name_lower} API endpoints
        
        # Example GET request
        try:
            response = self.client.get(
                f"{{base_url}}/api/{service_name_lower}/v1/info",
                headers=headers,
                name="{service_name_lower}_get_info"
            )
            print(f"Get info status: {{response.status_code}}")
        except Exception as e:
            print(f"Get info failed: {{e}}")

# Additional test methods can be added here
# Each method should follow the pattern: def test_scenario_name(self, headers, partition, base_url):
'''
        output_path.write_text(formatted_template, encoding='utf-8')
        self.logger.info(f"‚úÖ Created {output_path.name}")

    def create_requirements_file(self, output_path: Path):
        
        output_path.write_text(f"osdu_perf\n", encoding='utf-8')
        self.logger.info(f"Created {output_path.name}")

    def create_project_readme(self, output_path: Path, service_name: str):

        readme_content = f'''# {service_name.title()} Service Performance Tests

This project contains performance tests for the OSDU {service_name.title()} Service using the OSDU Performance Testing Framework v1.0.24.

## üìÅ Project Structure

```
perf_tests/
‚îú‚îÄ‚îÄ config.yaml               # Framework configuration (OSDU connection, metrics, test settings)
‚îú‚îÄ‚îÄ locustfile.py              # Main test file with API calls and @task methods
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies (osdu_perf package)
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üöÄ Quick Start

### 1. Configure Your Environment
Edit `config.yaml` and update:
- OSDU environment details (host, partition, app_id)
- Kusto metrics collection settings (optional)
- Test defaults (users, spawn rate, wait time)

### 2. Customize Your Tests
Edit `locustfile.py` and add @task methods with:
- API endpoints for {service_name} service
- Test scenarios using self.get(), self.post(), etc.
- Custom load patterns and user behavior

### 3. Run Performance Tests

#### Local Testing (Development)
```bash
# Basic run using config.yaml
osdu_perf run local --config config.yaml

# Override specific settings for testing
osdu_perf run local --config config.yaml --users 5 --run-time 30s

```

#### Azure Load Testing (Production Scale)
```bash
# Deploy and run on Azure Load Testing service
osdu_perf run azure_load_test --config config.yaml 
  
```

## üìù Writing Performance Tests

### Simple API-Based Approach

Your main test file `locustfile.py` uses this simple pattern:

```python
import os
from locust import events, task
from osdu_perf import PerformanceUser

@events.init_command_line_parser.add_listener
def add_custom_args(parser):
    \"\"\"Add OSDU-specific command line arguments\"\"\"
    parser.add_argument("--partition", type=str, default=os.getenv("PARTITION"), help="OSDU Data Partition ID")
    parser.add_argument("--appid", type=str, default=os.getenv("APPID"), help="Azure AD Application ID")

class OSDUUser(PerformanceUser):
    \"\"\"
    OSDU Performance Test User
    
    This class automatically handles:
    - Azure authentication and token management
    - HTTP headers and request setup
    - Locust user simulation and load testing
    \"\"\"
    
    def on_start(self):
        \"\"\"Called when a user starts\"\"\"
        super().on_start()
        print(f"üöÄ Started performance testing user")
    
    @task(3)  # Higher weight = more frequent execution
    def test_{service_name}_health(self):
        # Simple API call - framework handles headers, tokens automatically
        self.get("/api/{service_name}/v1/health")
    
    @task(2) 
    def test_{service_name}_info(self):
        self.get("/api/{service_name}/v1/info")
    
    @task(1)
    def test_{service_name}_operations(self):
        # POST request with JSON data
        self.post("/api/{service_name}/v1/records", json={{
            "kind": f"osdu:wks:{{partition}}:{service_name}:1.0.0",
            "data": {{"test": "data"}}
        }})
```

### Key Implementation Points:

1. **Class Name**: Must inherit from `PerformanceUser` - handles all authentication automatically
2. **File Name**: Use `locustfile.py` - standard Locust convention
3. **@task Methods**: Mark test methods with `@task(weight)` decorator
4. **Simple HTTP calls**: Use `self.get()`, `self.post()`, `self.put()`, `self.delete()`
5. **No manual auth**: Framework automatically handles Azure AD tokens and HTTP headers

### Available HTTP Methods

```python
# GET request
self.get("/api/{service_name}/v1/records/12345")

# POST request with JSON data
self.post("/api/{service_name}/v1/records", json={{
    "kind": "osdu:wks:partition:{service_name}:1.0.0",
    "data": {{"test": "data"}}
}})

# PUT request
self.put("/api/{service_name}/v1/records/12345", json=updated_data)

# DELETE request  
self.delete("/api/{service_name}/v1/records/12345")

# Custom headers (if needed - auth headers added automatically)
self.get("/api/{service_name}/v1/info", headers={{"Custom-Header": "value"}})
```

## üîß Configuration

### Framework Configuration (config.yaml)

The `config.yaml` file contains framework-wide settings:

```yaml
# OSDU Environment Configuration
osdu_environment:
  host: "https://your-osdu-host.com"
  partition: "your-partition-id"
  app_id: "your-azure-app-id"
  sku: "Standard"
  version: "25.2.35"

# Metrics Collection Configuration  
metrics_collector:
  kusto:
    cluster: "https://your-kusto-cluster.eastus.kusto.windows.net"
    database: "your-database"
    ingest_uri: "https://ingest-your-kusto.eastus.kusto.windows.net"

# Test Configuration
test_settings:
  subscription_id: "your-azure-subscription-id"
  resource_group: "your-resource-group"
  location: "eastus"
  default_wait_time: 
    min: 1
    max: 3
  users: 10
  spawn_rate: 2
  run_time: "60s"
  engine_instances: 1
```

### Configuration Hierarchy

Settings are resolved in this order (highest priority first):
1. **CLI arguments** - Direct command-line overrides
2. **Environment variables** - Runtime values
3. **config.yaml** - Project-specific settings  
4. **Framework defaults** - Built-in defaults

### Environment Variables

**Local Development:**
- `OSDU_HOST`: OSDU instance URL
- `OSDU_PARTITION`: Data partition ID
- `OSDU_APP_ID`: Azure AD Application ID
- `ADME_BEARER_TOKEN`: Manual token override

**Azure Load Testing (auto-set):**
- `AZURE_LOAD_TEST=true`: Indicates Azure environment
- `PARTITION`, `LOCUST_HOST`, `APPID`: Set by Azure Load Testing

### Authentication

The framework automatically handles Azure authentication:

**Local Development:**
- Azure CLI credentials (`az login`)
- Service Principal (environment variables)
- DefaultAzureCredential chain

**Azure Environments:**
- Managed Identity (preferred)
- Automatic credential detection and fallback

## üìä Monitoring and Results

### Local Testing
- **Web UI**: Open http://localhost:8089 during test execution
- **Real-time metrics**: Request rates, response times, error rates
- **Results export**: Download CSV reports for analysis

### Azure Load Testing
- **Azure Portal**: Monitor in Azure Portal under "Load Testing"
- **Comprehensive dashboards**: Detailed performance metrics and trends
- **Automated retention**: Results stored automatically
- **Integration**: Works with Azure Monitor and Application Insights

### Key Metrics
- **Requests per second (RPS)**
- **Response time percentiles** (50th, 90th, 95th, 99th)
- **Error rate and failure counts**
- **Throughput and content transfer rates**

## üêõ Troubleshooting

### Common Issues

1. **Authentication Errors**
   ```bash
   # Ensure Azure CLI is logged in for local testing
   az login
   
   # Or verify environment variables
   echo $ADME_BEARER_TOKEN
   ```

2. **Test File Issues**
   ```bash
   # Ensure locustfile.py exists and inherits from PerformanceUser
   ls locustfile.py
   
   # Check class inheritance
   grep "PerformanceUser" locustfile.py
   ```

3. **Configuration Issues**
   ```bash
   # Validate config.yaml syntax
   python -c "import yaml; yaml.safe_load(open('config.yaml'))"
   ```

4. **Missing Dependencies**
   ```bash
   # Install framework and dependencies
   pip install osdu_perf
   ```

### Debugging Tips

- Use `--verbose` flag for detailed logging
- Check Azure CLI authentication: `az account show`
- Verify OSDU connectivity manually before running tests
- Start with small user counts (1-5) for initial testing

## üìö Additional Resources

- [OSDU Performance Framework Documentation](https://github.com/janraj/osdu_perf)
- [Locust Documentation](https://docs.locust.io/)
- [Azure Load Testing](https://docs.microsoft.com/en-us/azure/load-testing/)
- [Azure Authentication](https://docs.microsoft.com/en-us/azure/developer/python/azure-sdk-authenticate)

## ü§ù Contributing

1. Follow the existing code patterns and framework conventions
2. Add comprehensive test scenarios for the {service_name} service
3. Update this README with service-specific information
4. Test thoroughly in both local and Azure environments

---

**Generated by OSDU Performance Testing Framework v1.0.24**
'''
        
        output_path.write_text(readme_content, encoding='utf-8')
        self.logger.info(f"Created {output_path.name}")

    def create_locustfile_template(self, output_path: Path, service_name:str):
        service_list = service_name or ["example"]
        services_comment = f"# Simple API-based performance testing for {service_list[0]} service"

        template = f'''"""
OSDU Performance Tests - Locust Configuration
Generated by OSDU Performance Testing Framework

{services_comment}
"""

import os
from locust import events, task, tag
from osdu_perf import PerformanceUser


# STEP 1: Register custom CLI args with Locust
# Please dont remove this code as it is required for OSDU parameters.
@events.init_command_line_parser.add_listener
def add_custom_args(parser):
    """Add OSDU-specific command line arguments"""
    parser.add_argument("--partition", type=str, default=os.getenv("PARTITION"), help="OSDU Data Partition ID")
    # Note: --host is provided by Locust built-in, no need to add it here
    # Note: --token is not exposed as CLI arg for security, only via environment variable
    parser.add_argument("--appid", type=str, default=os.getenv("APPID"), help="Azure AD Application ID")


class OSDUUser(PerformanceUser):
    """
    OSDU Performance Test User
    
    This class automatically:
    - Handles Azure authentication and token management
    - Manages HTTP headers and request setup
    - Provides simple API methods (get, post, put, delete)
    - Manages Locust user simulation and load testing
    
    Usage:
        locust -f locustfile.py --host https://your-api.com --partition your-partition --appid your-app-id
    """
    
    # Optional: Customize user behavior
    # Default `wait_time` is provided by `PerformanceUser` (between(1, 3)).
    # To override in the generated file, uncomment and import `between` from locust:
    # from locust import between
    # wait_time = between(1, 3)  # realistic pacing (recommended)
    # wait_time = between(0, 0)  # no wait (maximum load)
    
    def on_start(self):
        """Called when a user starts - performs setup"""
        super().on_start()

        # Access OSDU parameters from Locust parsed options or environment variables.
        # The available APIs are detailed in the README and library documentation.
        # This code generation accelerates development by removing the need to manually refer to the library docs.
        # locust client is available as self.client. Also have self.get, self.put, self.post wrapper API to use
        # One time setup can be performed here.
        # each other load function must have task decorator.
        # token is supplied via environment variable which is not passed to azure load test. 

        self.partition = self.get_partition()
        self.host = self.get_host()
        self.headers = self.get_headers()
        self.appid = self.get_appid()
        self.logger = self.get_logger()

        self.logger.info(f"üöÄ Started performance testing user")
        self.logger.info(f"   üìç Partition: {{self.partition}}")
        self.logger.info(f"   üåê Host: {{self.host}}")
        self.logger.info(f"   üÜî App ID: {{self.appid or 'Not provided'}}")

    def on_stop(self):
        """Called when a user stops - performs cleanup"""
        self.logger.info("üõë Stopped performance testing user")

    
    @tag("{service_name}")
    @task(1)
    def check_service_health(self):
        # *** Simple and clean consumer call ***
        self.get("/api/{service_name}/v2/liveness_check")

'''
        output_path.write_text(template, encoding='utf-8')
        print(f"[create_locustfile_template] Created {output_path.name}")

    # required 
    
    def _should_create_file(self, filepath: str, choice: str) -> bool:
        """
        Determine if a file should be created based on user choice and file existence.
        
        Args:
            filepath: Path to the file
            choice: User choice ('o', 's', 'b')
            
        Returns:
            True if file should be created, False otherwise
        """
        if choice == 'o':  # Overwrite
            return True
        elif choice == 's':  # Skip existing
            return not os.path.exists(filepath)
        elif choice == 'b':  # Backup (already done, now create new)
            return True
        return False

    def _create_file_if_needed(self, path: Path, creation_func, choice: str, *args) -> None:
        """A wrapper to create a file or skip it based on user choice."""
        if self._should_create_file(path, choice):
            # Unpack the list of args if it's passed as a single list
            creation_func(path, *args[0] if isinstance(args[0], list) else args)
        else:
            print(f"‚è≠Ô∏è  Skipped existing: {path.name}")

    def init_project(self, service_name: str, force: bool = False) -> None:
        """
        Initialize a new performance testing project for a specific service.
        
        Args:
            service_name: Name of the service to test (e.g., 'storage', 'search', 'wellbore')
            force: If True, overwrite existing files without prompting
        """
        project_name = f"perf_tests"
        test_filename = f"perf_{service_name}_test.py"
        project_path = Path.cwd() / project_name
        
        self.logger.info(f"[init_project] Initializing OSDU Performance Testing project for: {service_name}")
        
        # Check if project already exists
        if os.path.exists(project_name):
            self.logger.info(f"[init_project]  Directory '{project_name}' already exists!")

            # Check if specific service test file exists
            test_file_path = os.path.join(project_name, test_filename)
            if os.path.exists(test_file_path):
                self.logger.info(f"[init_project]  Test file '{test_filename}' already exists!")

                if force:
                    choice = 'o'  # Force overwrite
                    self.logger.info("[init_project] Force mode: Overwriting existing files...")
                else:
                    # Ask user what to do
                    while True:
                        choice = input(f"Do you want to:\n"
                                    f"  [o] Overwrite existing files\n"
                                    f"  [s] Skip existing files and create missing ones\n" 
                                    f"  [b] Backup existing files and create new ones\n"
                                    f"  [c] Cancel initialization\n"
                                    f"Enter your choice [o/s/b/c]: ").lower().strip()
                        
                        if choice in ['o', 'overwrite']:
                            self.logger.info("üîÑ Overwriting existing files...")
                            break
                        elif choice in ['s', 'skip']:
                            self.logger.info("‚è≠Ô∏è  Skipping existing files, creating missing ones...")
                            break
                        elif choice in ['b', 'backup']:
                            self.logger.info("üíæ Creating backup of existing files...")
                            #_backup_existing_files(project_name, service_name)
                            break
                        elif choice in ['c', 'cancel']:
                            self.logger.info("‚ùå Initialization cancelled.")
                            return
                        else:
                            self.logger.info("‚ùå Invalid choice. Please enter 'o', 's', 'b', or 'c'.")
            else:
                # Directory exists but no service test file
                choice = 's' if not force else 'o'  # Skip mode or force
        else:
            choice = 'o'  # New project
            self.logger.info(f"[init_project] Creating new project directory: {project_name}")
            # Create project directory
            os.makedirs(project_name, exist_ok=True)
        
        files_to_create = [
            #{"name": test_filename, "creator": self.create_service_test_file, "args": [service_name]},
            {"name": "requirements.txt", "creator": self.create_requirements_file, "args": []},
            {"name": "README.md", "creator": self.create_project_readme, "args": [service_name]},
            {"name": "locustfile.py", "creator": self.create_locustfile_template, "args": [service_name]},
            {"name": "config.yaml", "creator": self.create_project_config, "args": [service_name]},
        ]
        
        for file_meta in files_to_create:
            file_path = project_path / file_meta["name"]
            self._create_file_if_needed(file_path, file_meta["creator"], choice, file_meta["args"])


        self.logger.info(f"Project {'updated' if choice == 's' else 'initialized'} successfully in {project_name}/")
        self.logger.info(f"Next steps:")
        self.logger.info(f"         1. cd {project_name}")
        self.logger.info(f"         2. pip install -r requirements.txt")
        self.logger.info(f"         3. Edit config.yaml to set your OSDU environment details")
        self.logger.info(f"         4. Edit locustfile.py to implement your test scenarios")
        self.logger.info(f"         5. Run local tests: osdu-perf run local --config config.yaml")
        self.logger.info(f"         6. Run Azure Load Tests: osdu-perf run azure_load_test --config config.yaml ")
        self.logger.info(f"         7. Optional: Override config values with CLI arguments")