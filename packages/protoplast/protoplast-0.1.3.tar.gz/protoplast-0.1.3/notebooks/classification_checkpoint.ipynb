{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66267ca4-71ee-44a2-b428-723c23e02526",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Showcasing Protoplast Checkpointing in Cell-line Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook showcases the checkpointing feature in PROTOplast, which enables resuming model training after finishing one dataset & switching to another. It demonstrates how to save and load training checkpoints, making it easy to continue model development without starting from scratch. This is particularly useful for long training sessions, experimentation with various datasets, or training across multiple sessions or environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vblA",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import protoplast as pt\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "from anndata.experimental import AnnCollection\n",
    "from protoplast.scrna.anndata.lightning_models import LinearClassifier\n",
    "from protoplast.scrna.anndata.trainer import RayTrainRunner\n",
    "from protoplast.scrna.anndata.torch_dataloader import DistributedAnnDataset\n",
    "from protoplast.scrna.anndata.torch_dataloader import cell_line_metadata_cb, DistributedCellLineAnnDataset\n",
    "\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.lightning import RayDDPStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 2. Dataset pre-processing\n",
    "\n",
    "We begin by reading the two datasets used to train the cell-line classification model in this notebook. To ensure compatibility, the model requires that both datasets have the same output dimensions\n",
    "\n",
    "In the following section, we create a unified view by performing an **inner join** on the two datasets based on shared features. During this step, we:\n",
    "\n",
    "- Identify and record the **number of output classes** (cell-lines),\n",
    "- Extract the list of **cell-line** of both dataset.\n",
    "\n",
    "This alignment is essential to ensure the model receives a consistent input/output structure regardless of the dataset source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_PATHS = [\"/mnt/hdd2/tan/tahoe100m/plate1_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad\",\n",
    "           \"/mnt/hdd2/tan/tahoe100m/plate2_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad\"]\n",
    "adatas = [anndata.io.read_h5ad(p, backed = \"r\") for p in DS_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view of all dataset\n",
    "collection = AnnCollection(adatas, join_vars = \"inner\")\n",
    "\n",
    "# Record the cell-lines (output classes) in both datasets\n",
    "cell_lines = collection.obs.cell_line.unique().tolist()\n",
    "cell_lines_count = collection.obs.cell_line.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 3. Configure training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_per_worker = 12\n",
    "test_size = 0.0 # We don't have the test step in the model, so we can set this to 0\n",
    "val_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 4. Train on `plate1_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate1_adata = adatas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Hstk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>gene_count</th>\n",
       "      <th>tscp_count</th>\n",
       "      <th>mread_count</th>\n",
       "      <th>drugname_drugconc</th>\n",
       "      <th>drug</th>\n",
       "      <th>cell_line</th>\n",
       "      <th>sublibrary</th>\n",
       "      <th>BARCODE</th>\n",
       "      <th>pcnt_mito</th>\n",
       "      <th>S_score</th>\n",
       "      <th>G2M_score</th>\n",
       "      <th>phase</th>\n",
       "      <th>pass_filter</th>\n",
       "      <th>cell_name</th>\n",
       "      <th>plate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BARCODE_SUB_LIB_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01_001_025-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1676</td>\n",
       "      <td>2441</td>\n",
       "      <td>2892</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0131</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_025</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>-0.095055</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>A-172</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_026-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1657</td>\n",
       "      <td>2454</td>\n",
       "      <td>2925</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0480</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_026</td>\n",
       "      <td>0.042787</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.650549</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>PANC-1</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_048-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1749</td>\n",
       "      <td>2521</td>\n",
       "      <td>2963</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0293</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_048</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.308791</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>HEC-1-A</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_076-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>834</td>\n",
       "      <td>1038</td>\n",
       "      <td>1258</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0397</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_076</td>\n",
       "      <td>0.066474</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.245788</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>LS 180</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_088-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1275</td>\n",
       "      <td>1710</td>\n",
       "      <td>2006</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_1097</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_088</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.085348</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>C32</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sample  gene_count  tscp_count  mread_count  \\\n",
       "BARCODE_SUB_LIB_ID                                                  \n",
       "01_001_025-lib_841  smp_1495        1676        2441         2892   \n",
       "01_001_026-lib_841  smp_1495        1657        2454         2925   \n",
       "01_001_048-lib_841  smp_1495        1749        2521         2963   \n",
       "01_001_076-lib_841  smp_1495         834        1038         1258   \n",
       "01_001_088-lib_841  smp_1495        1275        1710         2006   \n",
       "\n",
       "                                 drugname_drugconc          drug  cell_line  \\\n",
       "BARCODE_SUB_LIB_ID                                                            \n",
       "01_001_025-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0131   \n",
       "01_001_026-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0480   \n",
       "01_001_048-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0293   \n",
       "01_001_076-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0397   \n",
       "01_001_088-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_1097   \n",
       "\n",
       "                   sublibrary     BARCODE  pcnt_mito   S_score  G2M_score  \\\n",
       "BARCODE_SUB_LIB_ID                                                          \n",
       "01_001_025-lib_841    lib_841  01_001_025   0.025399 -0.066667  -0.095055   \n",
       "01_001_026-lib_841    lib_841  01_001_026   0.042787  0.128571   0.650549   \n",
       "01_001_048-lib_841    lib_841  01_001_048   0.056724  0.242857   0.308791   \n",
       "01_001_076-lib_841    lib_841  01_001_076   0.066474  0.009524   0.245788   \n",
       "01_001_088-lib_841    lib_841  01_001_088   0.028655 -0.100000  -0.085348   \n",
       "\n",
       "                   phase pass_filter cell_name   plate  \n",
       "BARCODE_SUB_LIB_ID                                      \n",
       "01_001_025-lib_841    G1        full     A-172  plate1  \n",
       "01_001_026-lib_841   G2M        full    PANC-1  plate1  \n",
       "01_001_048-lib_841   G2M        full   HEC-1-A  plate1  \n",
       "01_001_076-lib_841   G2M        full    LS 180  plate1  \n",
       "01_001_088-lib_841    G1        full       C32  plate1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plate1_adata.obs.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ROlb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 05:25:12,316\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "\u001b[36m(pid=2230296)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "\u001b[36m(pid=2230296)\u001b[0m   import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=2230296)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainTrainable pid=2230296)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m   import pynvml  # type: ignore[import]\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=2230296)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=2230296)\u001b[0m - (node_id=cab90e436ba3ce93a799aef847d73ea335c284b7b547548a2d7d6c49, ip=192.168.1.226, pid=2230450) world_rank=0, local_rank=0, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m =========Starting the training on 0 with num threads: 12=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.11 /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.1 ...\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m   | Name    | Type             | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 0 | model   | Linear           | 3.1 M  | train\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 3.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 3.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 12.542    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 2         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m   warnings.warn(  # warn only once\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m /home/nam/protoplast/src/protoplast/scrna/anndata/torch_dataloader.py:115: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m /home/nam/protoplast/src/protoplast/scrna/anndata/torch_dataloader.py:115: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m   return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/4224 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 1/4224 [00:13<16:10:55,  0.07it/s, v_num=0, train_loss=4.180]\n",
      "Epoch 0:   0%|          | 2/4224 [00:13<8:05:48,  0.14it/s, v_num=0, train_loss=3.650] \n",
      "Epoch 0:   0%|          | 11/4224 [00:13<1:28:52,  0.79it/s, v_num=0, train_loss=1.320]\n",
      "Epoch 0:   0%|          | 20/4224 [00:14<49:09,  1.43it/s, v_num=0, train_loss=0.550]  \n",
      "...\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4210/4224 [01:27<00:00, 48.29it/s, v_num=0, train_loss=0.0645]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4211/4224 [01:27<00:00, 48.29it/s, v_num=0, train_loss=0.115] \n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4219/4224 [01:27<00:00, 48.33it/s, v_num=0, train_loss=0.0825]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4220/4224 [01:27<00:00, 48.33it/s, v_num=0, train_loss=0.0746]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4224/4224 [01:27<00:00, 48.34it/s, v_num=0, train_loss=0.0711]\n",
      "\u001b[36m(RayTrainWorker pid=2230450)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/960 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 1/960 [00:00<00:04, 237.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 2/960 [00:00<07:08,  2.24it/s] \u001b[A\n",
      "Validation DataLoader 0:   0%|          | 3/960 [00:00<05:14,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 4/960 [00:01<04:01,  3.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|          | 5/960 [00:01<03:17,  4.85it/s]\u001b[A\n",
      "...\n",
      "Validation DataLoader 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 955/960 [00:18<00:00, 51.51it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 956/960 [00:18<00:00, 51.52it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 957/960 [00:18<00:00, 51.54it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 958/960 [00:18<00:00, 51.55it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 959/960 [00:18<00:00, 51.57it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:18<00:00, 51.58it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Set up training\n",
    "trainer = RayTrainRunner(\n",
    "    LinearClassifier,\n",
    "    DistributedCellLineAnnDataset,\n",
    "    model_keys = [\"num_genes\",\n",
    "                  \"num_classes\"],\n",
    "    metadata_cb = cell_line_metadata_cb,\n",
    "    sparse_key = \"X\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qnkX",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 workers with {'CPU': 12} each\n",
      "=========Length of val_split 66 length of test_split 0 length of train_split 268\n",
      "=========Warning: 0.09090909090909091 of data is dropped\n",
      "=========Length of after dropping remainder val_split 60 length of test_split 0 length of train_split 264\n",
      "Data splitting time: 10.48 seconds\n",
      "Spawning Ray worker and initiating distributed training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 05:25:26,474\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-28 05:25:26 (running for 00:00:00.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/96 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-25-08_135543_2223671/artifacts/2025-09-28_05-25-26/TorchTrainer_2025-09-28_05-25-26/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-28 05:25:31 (running for 00:00:05.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-25-08_135543_2223671/artifacts/2025-09-28_05-25-26/TorchTrainer_2025-09-28_05-25-26/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-28 05:25:36 (running for 00:00:10.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-25-08_135543_2223671/artifacts/2025-09-28_05-25-26/TorchTrainer_2025-09-28_05-25-26/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-28 05:25:41 (running for 00:00:15.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-25-08_135543_2223671/artifacts/2025-09-28_05-25-26/TorchTrainer_2025-09-28_05-25-26/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 05:28:00,593\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/nam/protoplast_results/TorchTrainer_2025-09-28_05-25-26' in 0.0101s.\n",
      "2025-09-28 05:28:00,597\tINFO tune.py:1041 -- Total run time: 154.12 seconds (154.09 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-28 05:28:00 (running for 00:02:34.10)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-25-08_135543_2223671/artifacts/2025-09-28_05-25-26/TorchTrainer_2025-09-28_05-25-26/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train([DS_PATHS[0]],\n",
    "                       batch_size = 1024,\n",
    "                       test_size = test_size, \n",
    "                       val_size = val_size,\n",
    "                       num_workers = 1,\n",
    "                       thread_per_worker = thread_per_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8838590-b2f2-47b1-b3df-17fc54312704",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 5. Train on `plate2_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab` dataset\n",
    "\n",
    "We now have a checkpoint saved after training the classification model using the first dataset. We need to pass into `train()` the path to the checkpoint file. This path can be retrieved from the result trainer in previous `train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate2_adata = adatas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "DnEU",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>gene_count</th>\n",
       "      <th>tscp_count</th>\n",
       "      <th>mread_count</th>\n",
       "      <th>drugname_drugconc</th>\n",
       "      <th>drug</th>\n",
       "      <th>cell_line</th>\n",
       "      <th>sublibrary</th>\n",
       "      <th>BARCODE</th>\n",
       "      <th>pcnt_mito</th>\n",
       "      <th>S_score</th>\n",
       "      <th>G2M_score</th>\n",
       "      <th>phase</th>\n",
       "      <th>pass_filter</th>\n",
       "      <th>cell_name</th>\n",
       "      <th>plate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BARCODE_SUB_LIB_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01_001_053-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>2671</td>\n",
       "      <td>5629</td>\n",
       "      <td>6830</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_1119</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_053</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>-0.265873</td>\n",
       "      <td>-0.313553</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>CFPAC-1</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_082-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>2148</td>\n",
       "      <td>3173</td>\n",
       "      <td>3826</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0292</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_082</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.400794</td>\n",
       "      <td>0.520879</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>HCT15</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_145-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>683</td>\n",
       "      <td>886</td>\n",
       "      <td>1073</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_1098</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_145</td>\n",
       "      <td>0.029345</td>\n",
       "      <td>-0.019841</td>\n",
       "      <td>-0.032967</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>HepG2/C3A</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_175-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>1845</td>\n",
       "      <td>2786</td>\n",
       "      <td>3368</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0131</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_175</td>\n",
       "      <td>0.031587</td>\n",
       "      <td>-0.123016</td>\n",
       "      <td>-0.118498</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>A-172</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_181-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>1228</td>\n",
       "      <td>1849</td>\n",
       "      <td>2226</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0399</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_181</td>\n",
       "      <td>0.015143</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.008791</td>\n",
       "      <td>S</td>\n",
       "      <td>full</td>\n",
       "      <td>LoVo</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sample  gene_count  tscp_count  mread_count  \\\n",
       "BARCODE_SUB_LIB_ID                                                   \n",
       "01_001_053-lib_1000  smp_1591        2671        5629         6830   \n",
       "01_001_082-lib_1000  smp_1591        2148        3173         3826   \n",
       "01_001_145-lib_1000  smp_1591         683         886         1073   \n",
       "01_001_175-lib_1000  smp_1591        1845        2786         3368   \n",
       "01_001_181-lib_1000  smp_1591        1228        1849         2226   \n",
       "\n",
       "                                 drugname_drugconc          drug  cell_line  \\\n",
       "BARCODE_SUB_LIB_ID                                                            \n",
       "01_001_053-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_1119   \n",
       "01_001_082-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_0292   \n",
       "01_001_145-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_1098   \n",
       "01_001_175-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_0131   \n",
       "01_001_181-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_0399   \n",
       "\n",
       "                    sublibrary     BARCODE  pcnt_mito   S_score  G2M_score  \\\n",
       "BARCODE_SUB_LIB_ID                                                           \n",
       "01_001_053-lib_1000   lib_1000  01_001_053   0.016522 -0.265873  -0.313553   \n",
       "01_001_082-lib_1000   lib_1000  01_001_082   0.025843  0.400794   0.520879   \n",
       "01_001_145-lib_1000   lib_1000  01_001_145   0.029345 -0.019841  -0.032967   \n",
       "01_001_175-lib_1000   lib_1000  01_001_175   0.031587 -0.123016  -0.118498   \n",
       "01_001_181-lib_1000   lib_1000  01_001_181   0.015143  0.023810  -0.008791   \n",
       "\n",
       "                    phase pass_filter  cell_name   plate  \n",
       "BARCODE_SUB_LIB_ID                                        \n",
       "01_001_053-lib_1000    G1        full    CFPAC-1  plate2  \n",
       "01_001_082-lib_1000   G2M        full      HCT15  plate2  \n",
       "01_001_145-lib_1000    G1        full  HepG2/C3A  plate2  \n",
       "01_001_175-lib_1000    G1        full      A-172  plate2  \n",
       "01_001_181-lib_1000     S        full       LoVo  plate2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plate2_adata.obs.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 05:28:05,535\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "\u001b[36m(pid=2237276)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "\u001b[36m(pid=2237276)\u001b[0m   import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=2237276)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainTrainable pid=2237276)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m   import pynvml  # type: ignore[import]\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=2237276)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=2237276)\u001b[0m - (node_id=cd4f57706d40551030cd12e2560459c8f5a49ed17252f3a2a73a5da8, ip=192.168.1.226, pid=2237407) world_rank=0, local_rank=0, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m =========Starting the training on 0 with num threads: 12=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m Restoring states from the checkpoint path at /home/nam/protoplast_results/TorchTrainer_2025-09-28_05-25-26/TorchTrainer_8a35e_00000_0_2025-09-28_05-25-26/checkpoint_000000/checkpoint.ckpt\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:445: The dirpath has changed from '/tmp/ray/session_2025-09-28_05-25-08_135543_2223671/artifacts/2025-09-28_05-25-26/TorchTrainer_2025-09-28_05-25-26/working_dirs/TorchTrainer_8a35e_00000_0_2025-09-28_05-25-26/lightning_logs/version_0/checkpoints' to '/tmp/ray/session_2025-09-28_05-28-02_402805_2223671/artifacts/2025-09-28_05-29-04/TorchTrainer_2025-09-28_05-29-04/working_dirs/TorchTrainer_0c0aa_00000_0_2025-09-28_05-29-04/lightning_logs/version_0/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m   | Name    | Type             | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 0 | model   | Linear           | 3.1 M  | train\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 3.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 3.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 12.542    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 2         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m Restored all states from the checkpoint at /home/nam/protoplast_results/TorchTrainer_2025-09-28_05-25-26/TorchTrainer_8a35e_00000_0_2025-09-28_05-25-26/checkpoint_000000/checkpoint.ckpt\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m   warnings.warn(  # warn only once\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m /home/nam/protoplast/src/protoplast/scrna/anndata/torch_dataloader.py:115: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m   return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m /mnt/hdd2/nam/miniconda3/envs/test/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/6144 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|          | 1/6144 [00:17<30:10:10,  0.06it/s, v_num=0, train_loss=0.0611]\n",
      "Epoch 1:   0%|          | 3/6144 [00:18<10:23:16,  0.16it/s, v_num=0, train_loss=0.192] \n",
      "Epoch 1:   0%|          | 6/6144 [00:18<5:13:55,  0.33it/s, v_num=0, train_loss=0.340] \n",
      "Epoch 1:   0%|          | 9/6144 [00:19<3:35:58,  0.47it/s, v_num=0, train_loss=0.0756]\n",
      "...\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 6126/6144 [02:16<00:00, 44.86it/s, v_num=0, train_loss=0.0227]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 6133/6144 [02:16<00:00, 44.88it/s, v_num=0, train_loss=0.140] \n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 6134/6144 [02:16<00:00, 44.88it/s, v_num=0, train_loss=0.025]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 6142/6144 [02:16<00:00, 44.90it/s, v_num=0, train_loss=0.00833]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6144/6144 [02:16<00:00, 44.91it/s, v_num=0, train_loss=0.0725] \n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1536 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 1/1536 [00:00<00:35, 42.90it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 2/1536 [00:00<00:34, 44.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 3/1536 [00:00<00:30, 50.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 4/1536 [00:00<00:29, 52.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 5/1536 [00:00<00:27, 55.32it/s]\u001b[A\n",
      "...\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1531/1536 [00:28<00:00, 54.07it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1532/1536 [00:28<00:00, 54.08it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1533/1536 [00:28<00:00, 54.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1534/1536 [00:28<00:00, 54.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1535/1536 [00:28<00:00, 54.12it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1536/1536 [00:28<00:00, 54.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/nam/protoplast_results/TorchTrainer_2025-09-28_05-29-04/TorchTrainer_0c0aa_00000_0_2025-09-28_05-29-04/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=2237407)\u001b[0m `Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "# Set up training\n",
    "trainer = RayTrainRunner(\n",
    "    LinearClassifier,\n",
    "    DistributedCellLineAnnDataset,\n",
    "    model_keys = [\"num_genes\",\n",
    "                  \"num_classes\"],\n",
    "    metadata_cb = cell_line_metadata_cb,\n",
    "    sparse_key = \"X\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 workers with {'CPU': 12} each\n",
      "=========Length of val_split 98 length of test_split 0 length of train_split 394\n",
      "=========Length of after dropping remainder val_split 96 length of test_split 0 length of train_split 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 05:29:04,296\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting time: 15.06 seconds\n",
      "Spawning Ray worker and initiating distributed training\n",
      "== Status ==\n",
      "Current time: 2025-09-28 05:29:04 (running for 00:00:00.12)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/96 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-28-02_402805_2223671/artifacts/2025-09-28_05-29-04/TorchTrainer_2025-09-28_05-29-04/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-28 05:29:09 (running for 00:00:05.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-28-02_402805_2223671/artifacts/2025-09-28_05-29-04/TorchTrainer_2025-09-28_05-29-04/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-28 05:29:14 (running for 00:00:10.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-28-02_402805_2223671/artifacts/2025-09-28_05-29-04/TorchTrainer_2025-09-28_05-29-04/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-28 05:29:19 (running for 00:00:15.30)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-28-02_402805_2223671/artifacts/2025-09-28_05-29-04/TorchTrainer_2025-09-28_05-29-04/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 05:32:51,999\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/nam/protoplast_results/TorchTrainer_2025-09-28_05-29-04' in 0.0112s.\n",
      "2025-09-28 05:32:52,003\tINFO tune.py:1041 -- Total run time: 227.71 seconds (227.68 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-28 05:32:52 (running for 00:03:47.69)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 13.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-28_05-28-02_402805_2223671/artifacts/2025-09-28_05-29-04/TorchTrainer_2025-09-28_05-29-04/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'train_loss': 0.07253149151802063, 'val_acc': 0.9816296696662903, 'epoch': 1, 'step': 10368},\n",
       "  path='/home/nam/protoplast_results/TorchTrainer_2025-09-28_05-29-04/TorchTrainer_0c0aa_00000_0_2025-09-28_05-29-04',\n",
       "  filesystem='local',\n",
       "  checkpoint=Checkpoint(filesystem=local, path=/home/nam/protoplast_results/TorchTrainer_2025-09-28_05-29-04/TorchTrainer_0c0aa_00000_0_2025-09-28_05-29-04/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(result.checkpoint.path, \"checkpoint.ckpt\")\n",
    "\n",
    "trainer.train([DS_PATHS[1]],\n",
    "              max_epochs = 2,\n",
    "              batch_size = 1024,\n",
    "              test_size = test_size, \n",
    "              val_size = val_size,\n",
    "              num_workers = 1,\n",
    "              thread_per_worker = thread_per_worker,\n",
    "              ckpt_path = ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b219b13-18c6-4dbe-9cd8-fa2a69939502",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f424dd0-f4f3-40cb-bbdc-dcd9f7651852",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This brings us to the end of the tutorial notebook.\n",
    "\n",
    "This workflow highlights using checkpointing in **PROTOplast**, enabling efficient model development across diverse datasets.\n",
    "\n",
    "Feel free to explore and extend this notebook to suit your own data and use cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
