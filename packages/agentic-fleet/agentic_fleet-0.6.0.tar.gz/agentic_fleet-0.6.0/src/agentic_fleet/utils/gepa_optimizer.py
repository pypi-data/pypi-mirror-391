"""
Utilities for integrating dspy.GEPA with the agent framework.

Provides helper functions for:
  ‚Ä¢ Loading/splitting routing examples into DSPy datasets
  ‚Ä¢ Building feedback-rich metrics for GEPA optimization
  ‚Ä¢ Running the GEPA optimizer with sensible defaults
  ‚Ä¢ Harvesting additional training examples from execution history
"""

from __future__ import annotations

from dataclasses import dataclass
import json
import logging
from pathlib import Path
import random
from typing import Any, Dict, Iterable, List, Literal, Optional, Sequence, Tuple

import dspy
from dspy.teleprompt.gepa.gepa import GEPAFeedbackMetric
from dspy.teleprompt.gepa.gepa_utils import ScoreWithFeedback

from .dspy_manager import get_reflection_lm
from .history_manager import HistoryManager
from .self_improvement import SelfImprovementEngine


logger = logging.getLogger(__name__)


@dataclass
class RoutingDecision:
    """Represents routing decisions for comparison and analysis."""

    agents: List[str]
    mode: str
    tools: List[str]


def load_example_dicts(examples_path: str) -> List[Dict[str, Any]]:
    """
    Load supervisor training examples from JSON file.

    Args:
        examples_path: Path to JSON list of training records.

    Returns:
        List of example dictionaries (possibly empty).
    """
    path = Path(examples_path)
    if not path.exists():
        logger.warning("Training examples file not found: %s", examples_path)
        return []

    try:
        with open(path, "r") as f:
            data = json.load(f)

        if not isinstance(data, list):
            logger.warning("Unexpected training data format at %s (expected list)", examples_path)
            return []

        return [record for record in data if isinstance(record, dict)]
    except Exception as exc:
        logger.error("Failed to load training examples from %s: %s", examples_path, exc)
        return []


def harvest_history_examples(
    *,
    min_quality: float = 8.0,
    limit: int = 200,
) -> List[Dict[str, Any]]:
    """
    Convert recent high-quality executions into routing examples.

    Args:
        min_quality: Minimum quality score (0-10) required.
        limit: Max number of history entries to scan.

    Returns:
        List of example dictionaries derived from history.
    """
    history_manager = HistoryManager()
    executions = history_manager.load_history(limit=limit)
    if not executions:
        return []

    engine = SelfImprovementEngine(min_quality_score=min_quality, history_lookback=limit)
    harvested: List[Dict[str, Any]] = []

    for execution in executions:
        quality = execution.get("quality", {})
        if quality.get("score", 0) < min_quality:
            continue

        example = engine.execution_to_example(execution)
        if example:
            harvested.append(example)

    return harvested


def dedupe_examples(records: Sequence[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate routing examples based on task + assignment + mode."""
    seen = set()
    unique: List[Dict[str, Any]] = []

    for record in records:
        fingerprint = "|".join(
            [
                record.get("task", "").strip().lower(),
                record.get("assigned_to", ""),
                record.get("mode", record.get("execution_mode", "")),
            ]
        )
        if fingerprint in seen:
            continue
        seen.add(fingerprint)
        unique.append(record)

    return unique


def convert_to_dspy_examples(records: Sequence[Dict[str, Any]]) -> List[dspy.Example]:
    """Convert raw dictionaries into DSPy Example objects."""
    examples: List[dspy.Example] = []
    for record in records:
        try:
            example = dspy.Example(
                task=record.get("task", ""),
                team_capabilities=record.get("team", record.get("team_capabilities", "")),
                available_tools=record.get("available_tools", "No tools available"),
                current_context=record.get("context", ""),
                assigned_to=record.get("assigned_to", ""),
                execution_mode=record.get("mode", record.get("execution_mode", "")),
                tool_requirements=record.get("tool_requirements", []),
            ).with_inputs("task", "team_capabilities", "available_tools", "current_context")
            examples.append(example)
        except Exception as exc:
            logger.warning(
                "Skipping invalid training record (%s): %s",
                record.get("task", "unknown"),
                exc,
            )
            continue
    return examples


def prepare_gepa_datasets(
    *,
    base_examples_path: str,
    base_records: Optional[Sequence[Dict[str, Any]]] = None,
    extra_examples: Optional[Iterable[Dict[str, Any]]] = None,
    val_split: float = 0.2,
    seed: int = 13,
) -> Tuple[List[dspy.Example], List[dspy.Example]]:
    """
    Load, merge, dedupe, and split routing examples for GEPA.

    Args:
        base_examples_path: Path to core training JSON.
        extra_examples: Optional iterable (e.g., harvested history) to append.
        val_split: Fraction of records reserved for validation.
        seed: RNG seed for deterministic shuffles.

    Returns:
        (trainset, valset) of DSPy Example objects.
    """
    records: List[Dict[str, Any]]
    if base_records is not None:
        records = list(base_records)
    else:
        records = load_example_dicts(base_examples_path)
    if extra_examples:
        records.extend(extra_examples)

    records = dedupe_examples(records)
    if not records:
        return [], []

    rng = random.Random(seed)
    rng.shuffle(records)

    val_size = int(len(records) * val_split) if val_split > 0 else 0
    if val_size == 0 and val_split > 0 and len(records) > 4:
        val_size = 1  # keep at least one validation example when we have data

    val_records = records[:val_size] if val_size else []
    train_records = records[val_size:] if val_size else records

    return convert_to_dspy_examples(train_records), convert_to_dspy_examples(val_records)


def _normalize_agents(value: Any) -> List[str]:
    if not value:
        return []
    if isinstance(value, str):
        parts = value.split(",")
    elif isinstance(value, (list, tuple, set)):
        parts = list(value)
    else:
        parts = [str(value)]
    return [part.strip() for part in parts if part and str(part).strip()]


def _normalize_mode(value: Any) -> str:
    if not value:
        return ""
    return str(value).strip().lower()


def _normalize_tools(value: Any) -> List[str]:
    if not value:
        return []
    if isinstance(value, str):
        parts = value.replace("\n", ",").split(",")
    else:
        parts = list(value)
    return [part.strip().lower() for part in parts if part and str(part).strip()]


def _jaccard_similarity(expected: List[str], predicted: List[str]) -> float:
    if not expected and not predicted:
        return 1.0
    if not expected or not predicted:
        return 0.0
    exp_set = {item.lower() for item in expected}
    pred_set = {item.lower() for item in predicted}
    intersection = len(exp_set & pred_set)
    union = len(exp_set | pred_set)
    return intersection / union if union else 0.0


def _detect_edge_cases(
    task: str,
    expected: RoutingDecision,
    predicted: RoutingDecision,
) -> List[str]:
    """Detect edge cases in routing decisions."""
    edge_cases = []
    task_lower = task.lower()

    # Detect ambiguous tasks
    ambiguous_keywords = ["maybe", "possibly", "could", "might", "perhaps", "either", "or"]
    if any(kw in task_lower for kw in ambiguous_keywords):
        edge_cases.append(
            "This task involves ambiguity - consider clarifying requirements before routing."
        )

    # Detect tool conflicts
    if expected.tools and predicted.tools:
        missing_tools = set(expected.tools) - set(predicted.tools)
        extra_tools = set(predicted.tools) - set(expected.tools)
        if missing_tools and extra_tools:
            edge_cases.append(
                f"Tool conflict detected: missing {missing_tools} but included {extra_tools}."
            )

    # Detect mode edge cases
    if expected.mode != predicted.mode:
        if expected.mode == "parallel" and predicted.mode == "sequential":
            edge_cases.append(
                "Edge case: Tasks requiring parallel execution were routed sequentially. Parallel mode is needed when subtasks are independent."
            )
        elif expected.mode == "sequential" and predicted.mode == "parallel":
            edge_cases.append(
                "Edge case: Tasks requiring sequential execution were routed in parallel. Sequential mode is needed when subtasks have dependencies."
            )

    # Detect agent mismatch patterns
    if expected.agents != predicted.agents:
        if len(expected.agents) > len(predicted.agents):
            edge_cases.append(
                "Edge case: Task requires multiple agents but was assigned to fewer. Consider task complexity and required capabilities."
            )
        elif len(expected.agents) < len(predicted.agents):
            edge_cases.append(
                "Edge case: Task was over-assigned. Consider if a single agent can handle this task."
            )

    # Detect time-sensitive queries
    time_keywords = ["latest", "current", "recent", "today", "now", "2025", "2026", "future"]
    if any(kw in task_lower for kw in time_keywords) and "tavilysearchtool" not in [
        t.lower() for t in predicted.tools
    ]:
        edge_cases.append(
            "Edge case: Time-sensitive query detected but web search tool not assigned. Tasks about current events, latest data, or future dates require TavilySearchTool."
        )

    return edge_cases


def _get_clarifying_examples(
    task: str,
    expected_agents: List[str],
    expected_mode: str,
    expected_tools: List[str],
    assignment_score: float,
    mode_score: float,
    tool_score: float,
) -> List[str]:
    """Generate clarifying examples for similar tasks."""
    examples = []
    task_lower = task.lower()

    # Agent selection examples
    if assignment_score < 1.0:
        if "research" in task_lower or "find" in task_lower or "search" in task_lower:
            examples.append(
                "For research tasks, assign to Researcher agent. Example: 'Research AI trends' ‚Üí Researcher"
            )
        if "analyze" in task_lower or "calculate" in task_lower or "data" in task_lower:
            examples.append(
                "For analysis tasks, assign to Analyst agent. Example: 'Analyze sales data' ‚Üí Analyst"
            )
        if "write" in task_lower or "create" in task_lower or "draft" in task_lower:
            examples.append(
                "For writing tasks, assign to Writer agent. Example: 'Write a blog post' ‚Üí Writer"
            )
        if "review" in task_lower or "check" in task_lower or "validate" in task_lower:
            examples.append(
                "For review tasks, assign to Reviewer agent. Example: 'Review this document' ‚Üí Reviewer"
            )

    # Mode selection examples
    if mode_score < 1.0:
        if expected_mode == "parallel":
            examples.append(
                "Use parallel mode when subtasks are independent. Example: 'Research X, analyze Y, write Z' ‚Üí parallel (all can run simultaneously)"
            )
        elif expected_mode == "sequential":
            examples.append(
                "Use sequential mode when subtasks have dependencies. Example: 'Research X, then analyze results, then write report' ‚Üí sequential (each depends on previous)"
            )
        elif expected_mode == "delegated":
            examples.append(
                "Use delegated mode for simple, single-agent tasks. Example: 'What is the capital of France?' ‚Üí delegated (one agent, one answer)"
            )

    # Tool selection examples
    if tool_score < 1.0:
        if "tavilysearchtool" in [t.lower() for t in expected_tools]:
            examples.append(
                "Tasks requiring current information need TavilySearchTool. Example: 'What is today's weather?' ‚Üí requires TavilySearchTool"
            )
        if "hostedcodeinterpretertool" in [t.lower() for t in expected_tools]:
            examples.append(
                "Tasks requiring calculations or data processing need HostedCodeInterpreterTool. Example: 'Calculate the average of these numbers' ‚Üí requires HostedCodeInterpreterTool"
            )

    return examples


# type: ignore[type-arg]
def build_routing_feedback_metric(perfect_score: float = 1.0) -> GEPAFeedbackMetric:
    """
    Create a GEPA metric that scores routing quality and emits actionable feedback.

    Enhanced with edge-case detection, clarifying examples, and step-by-step guidance
    following DSPy tutorial patterns for iterative prompt learning.
    """

    def metric(
        gold: Any, pred: Any, trace=None, pred_name=None, pred_trace=None
    ) -> ScoreWithFeedback:
        # Extract task for edge-case detection
        task = getattr(gold, "task", getattr(pred, "task", ""))  # type: ignore[attr-defined]

        expected_agents = _normalize_agents(getattr(gold, "assigned_to", ""))  # type: ignore[attr-defined]
        predicted_agents = _normalize_agents(getattr(pred, "assigned_to", ""))  # type: ignore[attr-defined]

        assignment_score = _jaccard_similarity(expected_agents, predicted_agents)

        expected_mode = _normalize_mode(getattr(gold, "execution_mode", getattr(gold, "mode", "")))  # type: ignore[attr-defined]
        predicted_mode = _normalize_mode(getattr(pred, "execution_mode", getattr(pred, "mode", "")))  # type: ignore[attr-defined]
        mode_score = 1.0 if expected_mode and expected_mode == predicted_mode else 0.0

        expected_tools = _normalize_tools(getattr(gold, "tool_requirements", []))  # type: ignore[attr-defined]
        predicted_tools = _normalize_tools(getattr(pred, "tool_requirements", []))  # type: ignore[attr-defined]
        tool_score = (
            len(set(expected_tools) & set(predicted_tools)) / len(set(expected_tools))
            if expected_tools
            else 1.0
        )

        weighted_score = (assignment_score * 0.6) + (mode_score * 0.3) + (tool_score * 0.1)
        final_score = max(0.0, min(perfect_score, weighted_score * perfect_score))

        # Build comprehensive feedback following DSPy tutorial patterns
        feedback_parts = []

        # Step 1: Overall assessment
        if final_score >= 0.9:
            feedback_parts.append("‚úÖ Routing decision is correct.")
        elif final_score >= 0.7:
            feedback_parts.append("‚ö†Ô∏è Routing decision is mostly correct but has minor issues.")
        else:
            feedback_parts.append("‚ùå Routing decision needs significant improvement.")

        # Step 2: Edge-case detection
        expected_decision = RoutingDecision(
            agents=expected_agents, mode=expected_mode, tools=expected_tools
        )
        predicted_decision = RoutingDecision(
            agents=predicted_agents, mode=predicted_mode, tools=predicted_tools
        )
        edge_cases = _detect_edge_cases(task, expected_decision, predicted_decision)
        if edge_cases:
            feedback_parts.append("\nüîç Edge Cases Detected:")
            for edge_case in edge_cases:
                feedback_parts.append(f"  ‚Ä¢ {edge_case}")

        # Step 3: Detailed component analysis
        feedback_parts.append("\nüìä Component Analysis:")

        # Agent assignment feedback
        if assignment_score == 1.0:
            feedback_parts.append("  ‚úÖ Agent selection matches ground truth.")
        else:
            feedback_parts.append(
                "  ‚ùå Agent mismatch: Assigned "
                f"{predicted_agents or ['none']} but expected "
                f"{expected_agents or ['none']}."
            )
            # Provide step-by-step guidance
            if expected_agents:
                feedback_parts.append(
                    "  üìù Step-by-step: First, analyze task requirements. Then, match capabilities:"
                )
                for agent in expected_agents:
                    feedback_parts.append(f"    - {agent} is needed for this task")

        # Mode selection feedback
        if mode_score == 1.0:
            feedback_parts.append(
                f"  ‚úÖ Execution mode '{expected_mode or 'unspecified'}' is correct."
            )
        else:
            feedback_parts.append(
                f"  ‚ùå Mode mismatch: Used '{predicted_mode or 'delegated'}' but should use '{
                    expected_mode or 'delegated'
                }'."
            )
            # Provide decision criteria
            if expected_mode == "parallel":
                feedback_parts.append(
                    "  üìù Decision criteria: Use parallel mode when subtasks are independent and can run simultaneously."
                )
            elif expected_mode == "sequential":
                feedback_parts.append(
                    "  üìù Decision criteria: Use sequential mode when subtasks have dependencies (output of one feeds into next)."
                )
            elif expected_mode == "delegated":
                feedback_parts.append(
                    "  üìù Decision criteria: Use delegated mode for simple, single-agent tasks that don't need coordination."
                )

        # Tool selection feedback
        if expected_tools:
            if tool_score == 1.0:
                feedback_parts.append("  ‚úÖ Tool selection matches requirements.")
            else:
                missing = sorted(set(expected_tools) - set(predicted_tools))
                extra = sorted(set(predicted_tools) - set(expected_tools))
                if missing:
                    feedback_parts.append(f"  ‚ùå Missing required tools: {', '.join(missing)}.")
                if extra:
                    feedback_parts.append(f"  ‚ö†Ô∏è Unnecessary tools assigned: {', '.join(extra)}.")
                # Provide tool selection guidance
                feedback_parts.append("  üìù Tool selection process:")
                feedback_parts.append(
                    "    1. Analyze task for information needs (current data ‚Üí TavilySearchTool)"
                )
                feedback_parts.append(
                    "    2. Check for computation needs (calculations ‚Üí HostedCodeInterpreterTool)"
                )
                feedback_parts.append("    3. Match tools to assigned agents' capabilities")
        else:
            if predicted_tools:
                feedback_parts.append("  ‚ö†Ô∏è Tools assigned but none required for this task.")
            else:
                feedback_parts.append("  ‚úÖ No tools required (correct).")

        # Step 4: Clarifying examples for similar tasks
        if final_score < 0.9:
            examples = _get_clarifying_examples(
                task,
                expected_agents,
                expected_mode,
                expected_tools,
                assignment_score,
                mode_score,
                tool_score,
            )
            if examples:
                feedback_parts.append("\nüí° Clarifying Examples for Similar Tasks:")
                for example in examples:
                    feedback_parts.append(f"  ‚Ä¢ {example}")

        # Step 5: Task-specific patterns
        task_lower = task.lower()
        if final_score < 0.9:
            feedback_parts.append("\nüéØ Task-Specific Patterns:")
            if any(kw in task_lower for kw in ["research", "find", "search", "latest", "current"]):
                feedback_parts.append(
                    "  ‚Ä¢ Research tasks typically require: Researcher agent + TavilySearchTool + delegated/sequential mode"
                )
            if any(kw in task_lower for kw in ["analyze", "calculate", "data", "compute"]):
                feedback_parts.append(
                    "  ‚Ä¢ Analysis tasks typically require: Analyst agent + HostedCodeInterpreterTool + delegated/sequential mode"
                )
            if any(kw in task_lower for kw in ["write", "create", "draft", "compose"]):
                feedback_parts.append(
                    "  ‚Ä¢ Writing tasks typically require: Writer agent + (no tools) + delegated mode"
                )
            if any(kw in task_lower for kw in ["review", "check", "validate", "verify"]):
                feedback_parts.append(
                    "  ‚Ä¢ Review tasks typically require: Reviewer agent + (no tools) + delegated mode"
                )
            if any(kw in task_lower for kw in ["and", "also", "then", "multiple"]):
                feedback_parts.append(
                    "  ‚Ä¢ Multi-step tasks typically require: Multiple agents + sequential/parallel mode"
                )

        # Combine all feedback parts
        feedback = "\n".join(feedback_parts)
        if not feedback.strip():
            feedback = "No actionable feedback available."

        return ScoreWithFeedback(score=final_score, feedback=feedback)

    return metric


def optimize_with_gepa(
    module: Any,
    trainset: Sequence[dspy.Example],
    valset: Optional[Sequence[dspy.Example]] = None,
    *,
    auto: Literal["light", "medium", "heavy"] | None = "light",
    max_full_evals: Optional[int] = 50,
    max_metric_calls: Optional[int] = 150,
    reflection_model: Optional[str] = None,
    perfect_score: float = 1.0,
    log_dir: str = "logs/gepa",
    metric: Optional[GEPAFeedbackMetric] = None,  # type: ignore[type-arg]
    **gepa_kwargs: Any,
) -> Any:
    """
    Compile the DSPy module using dspy.GEPA with routing-aware feedback.
    """

    if not trainset:
        logger.warning("No training data supplied for GEPA; returning original module.")
        return module

    Path(log_dir).mkdir(parents=True, exist_ok=True)
    metric = metric or build_routing_feedback_metric(perfect_score=perfect_score)

    # Use centralized DSPy manager for reflection LM (reuses shared instance)
    reflection_lm = get_reflection_lm(reflection_model)

    optimizer = dspy.GEPA(  # type: ignore[attr-defined]
        metric=metric,
        auto=auto,
        max_full_evals=max_full_evals,
        max_metric_calls=max_metric_calls,
        reflection_minibatch_size=gepa_kwargs.pop("reflection_minibatch_size", 3),
        reflection_lm=reflection_lm,
        perfect_score=perfect_score,
        log_dir=log_dir,
        track_stats=gepa_kwargs.pop("track_stats", True),
        warn_on_score_mismatch=gepa_kwargs.pop("warn_on_score_mismatch", True),
        **gepa_kwargs,
    )

    compiled = optimizer.compile(  # type: ignore[attr-defined]
        module,
        trainset=list(trainset),
        valset=list(valset) if valset else None,
    )

    logger.info(
        "GEPA optimization complete (train=%d, val=%d, log_dir=%s)",
        len(trainset),
        len(valset or []),
        log_dir,
    )
    return compiled
